1
00:00:33,520 --> 00:00:34,460
你有一场表演要来了，

2
00:00:34,460 --> 00:00:38,470
我们稍后再讨论这个。

3
00:00:38,470 --> 00:00:38,870
对于来到这节课的人，

4
00:00:39,310 --> 00:00:41,390
作业一截止这周五，15 号，

5
00:00:41,650 --> 00:00:45,050
项目一已经发布，截止 10 月 1 日，

6
00:00:46,270 --> 00:00:47,175
尽管我们还没有讨论

7
00:00:47,175 --> 00:00:49,460
什么是缓冲池，什么是缓冲池管理器，

8
00:00:49,750 --> 00:00:51,020
如果你想要开始，你可以开始，

9
00:00:51,880 --> 00:00:54,620
在哪里分配内存，以便将其写出磁盘，

10
00:00:56,020 --> 00:00:57,620
即将到来的活动，

11
00:00:57,880 --> 00:01:04,280
下周一， Dana Van Aken 将给我们一个演讲，

12
00:01:04,540 --> 00:01:07,550
在我们的系列讲座上进行演讲，

13
00:01:14,410 --> 00:01:16,280
演讲系列讲座在 Zoom 上 在星期一，

14
00:01:16,510 --> 00:01:23,265
在那之前， DJ2PL 在本周六晚上 9 有一场演唱会，

15
00:01:23,265 --> 00:01:28,140
在校园里，在 CUC 的 Rangos 。

16
00:01:28,940 --> 00:01:30,420
他们是不是还让你签名？

17
00:01:34,130 --> 00:01:36,310
是的， CMU 有点奇怪，

18
00:01:36,310 --> 00:01:37,000
他们不会让他，

19
00:01:37,000 --> 00:01:38,155
这是一场盛大的演出，

20
00:01:38,155 --> 00:01:39,570
之后也不会让他签名，

21
00:01:40,700 --> 00:01:41,550
有点愚蠢，是的。

22
00:01:43,130 --> 00:01:47,250
所以上一节课，跳转到那里。

23
00:01:48,160 --> 00:01:51,950
上一节课，我们讨论了其他方法，

24
00:01:51,970 --> 00:01:57,620
对于我们上周提出的面向 tuple 的插槽页面存储方案，

25
00:01:57,970 --> 00:02:02,600
特别是我们花了很多时间讨论日志结构存储方法，

26
00:02:03,040 --> 00:02:04,890
不是存储实际的 tuple ，

27
00:02:04,890 --> 00:02:10,010
而是存储对 tuple 所做更改的日志条目，

28
00:02:10,810 --> 00:02:14,480
我说过这在写入密集型的现代系统中很流行。

29
00:02:15,430 --> 00:02:18,045
所以，我们谈到的三种方法，

30
00:02:18,045 --> 00:02:22,250
面向 tuple 的插槽页面，日志结构存储，索引组织存储，

31
00:02:22,780 --> 00:02:28,700
这些存储方法非常适合写入工作负载很重的情况，

32
00:02:29,460 --> 00:02:32,705
这意味着如果你执行大量的插入、更新或删除，

33
00:02:32,705 --> 00:02:34,700
日志结构显然更适合这个，

34
00:02:34,700 --> 00:02:36,580
因为你在追加日志，

35
00:02:38,130 --> 00:02:40,340
对于许多应用程序或大多数应用程序，

36
00:02:40,340 --> 00:02:45,700
当你开始时，你可能是更多写入工作负载，

37
00:02:46,500 --> 00:02:49,955
但将会有一些应用程序、某些环境或一些工作负载，

38
00:02:49,955 --> 00:02:53,290
你可能并不关心获得最好的写入性能，

39
00:02:54,160 --> 00:02:57,000
你真正想要做的是获得最好的读取性能，

40
00:02:57,960 --> 00:03:03,440
所以，这些方法可能不是接近它的最好方式。

41
00:03:04,280 --> 00:03:08,580
所以，我将花一点时间讨论一下大类别或数据库应用程序是什么样子的，

42
00:03:08,990 --> 00:03:12,720
然后这将是我们为什么要考虑替代存储方案的动机，

43
00:03:13,100 --> 00:03:16,200
我们可能不想存储所有东西在行中，

44
00:03:16,200 --> 00:03:18,750
tuple 将所有属性放在一起。

45
00:03:20,660 --> 00:03:22,260
这是一个粗略的分类，

46
00:03:23,630 --> 00:03:24,720
但行业是，

47
00:03:25,040 --> 00:03:28,890
如果你说这三个方法，

48
00:03:29,660 --> 00:03:31,380
人们大致知道你的意思。

49
00:03:32,360 --> 00:03:36,630
第一类应用程序将称为 OLTP 或在线事务处理，

50
00:03:37,340 --> 00:03:42,210
这些应用程序中，你可以从外部世界获取新数据，

51
00:03:42,500 --> 00:03:46,230
你同时为许多用户提供服务，

52
00:03:46,730 --> 00:03:49,950
我一直喜欢使用的示例应用程序是 Amazon ，

53
00:03:50,690 --> 00:03:52,020
当你去亚马逊网站，

54
00:03:52,280 --> 00:03:55,165
你看产品，然后你点击东西，

55
00:03:55,165 --> 00:03:57,210
你把它们添加到你的购物车，然后你购买它们，

56
00:03:57,380 --> 00:03:59,050
也许你打开你的账户信息，

57
00:03:59,050 --> 00:04:02,730
然后更新你的邮寄地址或付款信息，

58
00:04:03,350 --> 00:04:06,820
这些都被认为是 OLTP 类型工作负载，

59
00:04:06,820 --> 00:04:11,310
因为你对数据库的一小部分进行更改，

60
00:04:11,330 --> 00:04:15,980
比如，你要更新你的购物车，更新你的支付信息。

61
00:04:16,810 --> 00:04:19,700
想想在 Reddit 或 Hacker News 上发布一些东西，

62
00:04:19,990 --> 00:04:21,290
它们进行小的更改，

63
00:04:21,310 --> 00:04:22,740
这可能是一个大的数据库，

64
00:04:22,740 --> 00:04:26,460
但是每个操作进行的查询的更改量是小的，

65
00:04:26,460 --> 00:04:28,520
它们读取的很多数据是小的，

66
00:04:29,020 --> 00:04:30,800
读取单个实体。

67
00:04:31,980 --> 00:04:35,480
将这与在线分析处理 OLAP 工作负载进行对比，

68
00:04:35,480 --> 00:04:37,960
这就是我想要使用，

69
00:04:38,820 --> 00:04:45,100
我将运行将在整个数据集中提取或推断新信息的查询，

70
00:04:45,740 --> 00:04:47,525
所以，这就像是亚马逊运行一个查询，

71
00:04:47,525 --> 00:04:49,630
说为我找到销售排名第一的产品，

72
00:04:50,010 --> 00:04:52,570
在宾夕法尼亚州，在星期六，

73
00:04:52,650 --> 00:04:56,350
而且当时气温超过 80 度，

74
00:04:56,350 --> 00:05:00,175
它不是着眼于单个人或单个实体，

75
00:05:00,175 --> 00:05:01,590
而是着眼于整个表，

76
00:05:02,530 --> 00:05:07,700
可以与额外的信息做了很多 JOIN ，

77
00:05:08,020 --> 00:05:11,360
类似于你们在家庭作业一中做过的事情。

78
00:05:13,080 --> 00:05:14,570
所以，在这些 OLAP 工作负载中，

79
00:05:14,570 --> 00:05:18,065
它们将主要是重读或只读，

80
00:05:18,065 --> 00:05:19,085
我不做单个更新，

81
00:05:19,085 --> 00:05:22,600
我要在大表上做大的扫描和 JOIN 。

82
00:05:23,890 --> 00:05:30,380
最后一个是行业分析师或 Gartner 的流行语，叫做 HTAP ，

83
00:05:30,700 --> 00:05:32,430
这基本上就是，在某些应用程序中，

84
00:05:32,430 --> 00:05:36,440
你可能希望同时执行 OLTP 工作负载和 OLAP 工作负载，

85
00:05:37,210 --> 00:05:38,330
可能在同一个系统中，

86
00:05:39,100 --> 00:05:43,280
所以，不是让我把我所有的交易数据放进一个单独的数据仓库，

87
00:05:44,050 --> 00:05:45,350
然后在那里做我的分析，

88
00:05:46,240 --> 00:05:48,980
也许我可以在数据进来的时候直接做一些分析，

89
00:05:50,500 --> 00:05:52,965
我们会在整个学期讨论这个问题，

90
00:05:52,965 --> 00:05:56,390
但你们要关注的主要两个是 OLTP 和 OLAP 。

91
00:05:57,940 --> 00:06:00,290
另一种考虑它们之间区别的方法是，

92
00:06:01,390 --> 00:06:03,015
这样一个简单的网格，

93
00:06:03,015 --> 00:06:08,750
在 x 轴上表示工作负载是重写入的还是重读取，

94
00:06:08,980 --> 00:06:12,500
然后 y 轴表示查询的复杂程度，

95
00:06:12,550 --> 00:06:13,815
所以你可以像这样划分它，

96
00:06:13,815 --> 00:06:15,350
OLTP 会在这个角落，

97
00:06:15,670 --> 00:06:17,630
因为我们可能会进行大量的更新，

98
00:06:18,460 --> 00:06:20,510
但我们要执行的查询将非常简单，

99
00:06:21,140 --> 00:06:22,420
比如 SELECT 单个，

100
00:06:22,530 --> 00:06:26,230
SELECT * FROM account WHERE id = 'Andy',

101
00:06:26,850 --> 00:06:29,560
它会获取单个东西。

102
00:06:30,680 --> 00:06:32,850
OLAP 会在范围的另一端，

103
00:06:33,710 --> 00:06:37,800
在这里，我们主要进行读取，

104
00:06:38,210 --> 00:06:41,400
读取， SELECT 查询将执行的，

105
00:06:42,680 --> 00:06:45,250
比我们在 OLTP 的世界做的 要复杂得多，

106
00:06:45,960 --> 00:06:48,520
想想作业一中的 Q9 Q10 。

107
00:06:49,880 --> 00:06:53,790
所以， OLTP 术语可以追溯到 80 年代，

108
00:06:54,020 --> 00:06:56,305
OLAP 诞生于 90 年代，

109
00:06:56,305 --> 00:06:59,125
这个叫 Jim Gray 的人是著名的数据库研究员，

110
00:06:59,125 --> 00:07:01,410
他发明了很多东西。当我们谈到这个学期时，

111
00:07:02,120 --> 00:07:04,300
他写了一篇文章说，

112
00:07:04,300 --> 00:07:07,240
90 年代初出现了一种新的工作负载类别，称为 OLAP ，

113
00:07:07,240 --> 00:07:08,400
我们应该注意它，

114
00:07:09,800 --> 00:07:11,845
事实证明，他从一家公司获得报酬，

115
00:07:11,845 --> 00:07:14,575
这家公司在 90 年代初试图销售一种 OLAP 数据库系统产品，

116
00:07:14,575 --> 00:07:17,370
论文被撤回，但这个名字仍然存在，

117
00:07:17,980 --> 00:07:21,480
然后 Jim Gray 在 96 年获得了关于数据库的图灵奖，

118
00:07:22,250 --> 00:07:23,640
他是一位非常著名的数据研究员，

119
00:07:25,240 --> 00:07:26,360
有人听说过这个故事吗，

120
00:07:26,710 --> 00:07:27,920
之前有人听说过 Jim Gray 吗，

121
00:07:30,490 --> 00:07:31,430
一个，一个人，

122
00:07:31,540 --> 00:07:36,210
他于 2006 年在旧金山湾的海上失踪是出了名的，

123
00:07:36,210 --> 00:07:37,335
他独自一人出海航行，

124
00:07:37,335 --> 00:07:38,030
这不是一个玩笑，

125
00:07:38,350 --> 00:07:41,000
他独自出海航行，他的船不见了，

126
00:07:42,100 --> 00:07:44,880
这是早期的[]搜寻的例子之一，

127
00:07:44,880 --> 00:07:48,675
因为他们移动了卫星来拍摄旧金山湾的照片，

128
00:07:48,675 --> 00:07:51,290
人们查看图像，试图找到那艘船，

129
00:07:51,740 --> 00:07:52,600
但他们一直没找到他，

130
00:07:55,290 --> 00:07:57,850
好的，这太奇怪了，

131
00:07:59,520 --> 00:08:01,090
我从来没见过他，但是很多，

132
00:08:01,650 --> 00:08:04,895
我们谈到的去冥王星，在你面前读书，

133
00:08:04,895 --> 00:08:06,530
这是 Jim Gray 的比喻，

134
00:08:06,530 --> 00:08:07,990
他有很多这样的有趣的事情。

135
00:08:08,990 --> 00:08:10,260
所以 HTAP 应该在中间，

136
00:08:10,730 --> 00:08:13,830
所以，今天我想花点时间谈谈，

137
00:08:16,070 --> 00:08:19,470
为什么到目前为止，我们在前两节课中谈到的东西，

138
00:08:19,610 --> 00:08:21,990
它们将有利于 OLTP 而不是 OLAP ，

139
00:08:22,100 --> 00:08:26,160
然后我们将设计一个更适合 OLAP 的存储方案。

140
00:08:27,980 --> 00:08:28,585
为了做到这一点，

141
00:08:28,585 --> 00:08:32,220
我们将使用一个真实的数据库来做一个非常简单的示例，

142
00:08:32,600 --> 00:08:38,040
所以这大概就是维基百科数据库的样子，

143
00:08:38,180 --> 00:08:39,690
它运行着一个名为 MediaWiki 的软件，

144
00:08:39,710 --> 00:08:42,480
它运行在 MySQL 和 PHP 上，

145
00:08:42,560 --> 00:08:44,100
它是开源的，你可以去看看它，

146
00:08:44,210 --> 00:08:48,750
[模式]大概是这样的，

147
00:08:48,750 --> 00:08:51,140
有 useracct ，那些进行修改的人，

148
00:08:51,670 --> 00:08:53,930
有 pages ，就像维基百科上的文章，

149
00:08:54,160 --> 00:08:56,390
然后有 revisions ，对于那些文章，

150
00:08:56,830 --> 00:08:58,760
这里 revision 有一个外键引用，

151
00:08:59,020 --> 00:09:00,470
你有创建更改的用户，

152
00:09:00,520 --> 00:09:06,080
然后是页面本身的 ID ，

153
00:09:06,730 --> 00:09:09,200
但所有的文本本身都将在 revision 部分，

154
00:09:11,430 --> 00:09:14,960
这里有一个外键从 page 回到 revision ，

155
00:09:14,960 --> 00:09:16,840
所以，你可以找到最新的版本。

156
00:09:18,970 --> 00:09:20,750
我在之前已经说过了，

157
00:09:21,160 --> 00:09:25,280
关系模型并没有定义或指定，

158
00:09:26,580 --> 00:09:30,065
关于我们应该如何将数据存储在表中，

159
00:09:30,065 --> 00:09:31,660
所以在我目前为止展示的所有例子中，

160
00:09:31,770 --> 00:09:32,830
我们只是展示 tuple ，

161
00:09:33,000 --> 00:09:34,990
每个 tuple ，所有一个接一个的属性，

162
00:09:35,130 --> 00:09:37,540
是的，我们说过大型属性有溢出页面，

163
00:09:37,800 --> 00:09:43,990
但是，通常所有较小的属性都存储在一起，

164
00:09:44,310 --> 00:09:47,060
但是，关系模型并没有说你必须这样做，

165
00:09:47,060 --> 00:09:49,280
只是我们作为人类最先想到的东西，

166
00:09:49,280 --> 00:09:50,950
我们很容易从概念上思考。

167
00:09:51,870 --> 00:09:53,450
但是，对于 OLAP 工作负载而言，

168
00:09:53,450 --> 00:09:54,970
这可能不是最好的选择。

169
00:09:55,870 --> 00:09:58,790
那么让我们来看看它是如何为 OLTP 工作的，

170
00:09:58,960 --> 00:10:01,460
对于 OLTP ，它将是一堆小查询，

171
00:10:02,410 --> 00:10:05,300
这将是非常简单的大量查询，

172
00:10:05,620 --> 00:10:08,450
它们将读取或写入少量数据，

173
00:10:08,530 --> 00:10:11,745
相对于数据库中的所有数据。

174
00:10:11,745 --> 00:10:14,090
这里的第一个查询将是获取，

175
00:10:14,530 --> 00:10:16,460
对于给定页面，给出它的 pageID ，

176
00:10:16,810 --> 00:10:18,980
为我获取它的最新版本，

177
00:10:19,090 --> 00:10:20,840
所以，对 revision 表做 JOIN ，

178
00:10:21,010 --> 00:10:22,940
但它只有一页和一个版本，

179
00:10:23,530 --> 00:10:24,410
它获取的。

180
00:10:25,100 --> 00:10:26,585
下一个是更新查询，

181
00:10:26,585 --> 00:10:27,580
有人在登录吗，

182
00:10:27,840 --> 00:10:30,370
你有一个 userID ，假设他们已经过身份验证，

183
00:10:30,630 --> 00:10:32,315
你更新用户账户，

184
00:10:32,315 --> 00:10:34,655
使用他们最后一次登录的时间戳，

185
00:10:34,655 --> 00:10:36,460
和他们登录的主机名。

186
00:10:36,930 --> 00:10:38,525
或者，如果我在 revision 表中执行 INSERT ，

187
00:10:38,525 --> 00:10:41,320
它会插入一行。

188
00:10:42,310 --> 00:10:44,085
这通常是人们得到的结果，

189
00:10:44,085 --> 00:10:45,260
当他们在构建一个全新的应用程序时，

190
00:10:45,490 --> 00:10:47,960
比如，如果你要创建一家初创公司，

191
00:10:48,040 --> 00:10:49,820
你开始构建一些在线服务，

192
00:10:50,230 --> 00:10:52,290
你通常会得到这样的结果，

193
00:10:52,290 --> 00:10:54,320
因为你一开始没有任何数据，

194
00:10:54,640 --> 00:10:55,500
你需要得到它，

195
00:10:55,500 --> 00:10:56,630
你做了一个网站，

196
00:10:56,680 --> 00:10:59,030
然后你的网站会运行这些查询。

197
00:11:01,180 --> 00:11:03,140
而对于 OLAP ，我们将做更复杂的事情，

198
00:11:03,640 --> 00:11:07,460
需要我们查看表的更大部分，

199
00:11:08,110 --> 00:11:11,780
这是真实查询的粗略近似值，

200
00:11:11,800 --> 00:11:14,210
当人们运行，

201
00:11:15,010 --> 00:11:17,630
你看一下维基百科的用户账号，

202
00:11:17,800 --> 00:11:22,610
你会发现所有来自用户的登录尝试，

203
00:11:22,720 --> 00:11:27,300
有 IP 地址或主机名以 .gov 结尾，

204
00:11:27,300 --> 00:11:30,735
因为，有一起丑闻发生在 2000 年代末，2010 年代初，

205
00:11:30,735 --> 00:11:35,180
国会里的人让他们的工作人员更新维基百科，

206
00:11:35,320 --> 00:11:38,510
对国会议员或国会女议员说更多恭维的话，

207
00:11:39,100 --> 00:11:41,300
比如 Pence 做了这个， Joe Biden 做了这个，

208
00:11:42,010 --> 00:11:44,085
所以，这个查询可以找到执行这个操作的所有人，

209
00:11:44,085 --> 00:11:47,640
所以，基本上是付钱让政府雇员更新维基百科，

210
00:11:48,170 --> 00:11:48,900
他们不应该这么做。

211
00:11:49,720 --> 00:11:52,110
这个我们要在数据上执行的查询，

212
00:11:52,110 --> 00:11:56,360
在应用程序的 OLTP 部分收集数据后。

213
00:11:59,270 --> 00:12:01,320
所以我们现在要谈的是，

214
00:12:02,000 --> 00:12:03,900
我所说的存储模型，

215
00:12:04,520 --> 00:12:06,060
这将是，

216
00:12:06,320 --> 00:12:10,620
数据库系统如何在物理上组织 tuple 在磁盘和内存中，

217
00:12:11,300 --> 00:12:14,610
相对于它们各自的属性中的其他 tuple 。

218
00:12:15,280 --> 00:12:17,250
到目前为止，

219
00:12:17,420 --> 00:12:20,640
我假设所有的属性对于 tuple 来说都是连续的，

220
00:12:21,050 --> 00:12:23,520
这叫做行存储，

221
00:12:24,800 --> 00:12:28,020
但是，对于 OLAP ，这可能不是最好的事情，

222
00:12:28,280 --> 00:12:29,910
我们很快就会知道为什么。

223
00:12:30,730 --> 00:12:33,330
我们之所以要讨论系统的这一部分，

224
00:12:33,330 --> 00:12:39,050
因为在数据库世界现在的市场中，存在明显的区别，

225
00:12:39,400 --> 00:12:41,690
在行存储系统和列存储系统之间，

226
00:12:41,980 --> 00:12:44,600
对于行存储系统，你想要将其用于 OLTP ，

227
00:12:44,950 --> 00:12:47,810
对于列存储系统，你想要将其用于 OLAP ，

228
00:12:47,950 --> 00:12:49,245
如果有人试图说，

229
00:12:49,245 --> 00:12:52,640
嘿，我有一个快速的行存储，你可以用来进行分析，

230
00:12:53,500 --> 00:12:55,040
你应该非常怀疑。

231
00:12:56,130 --> 00:12:57,695
好的，三个选择是，

232
00:12:57,695 --> 00:13:01,300
N-ary 存储模型或 NSM ，这是行存储，

233
00:13:01,500 --> 00:13:04,510
分解存储模型， DSM ，这是列存储，

234
00:13:04,680 --> 00:13:07,955
然后是混合方法，这是列存储最常见的方法，

235
00:13:07,955 --> 00:13:09,070
我们一会儿就会知道原因，

236
00:13:09,510 --> 00:13:12,010
它被称为 PAX 或分区属性访问，

237
00:13:13,260 --> 00:13:15,130
大多数时候，人们说他们有一个列存储，

238
00:13:15,150 --> 00:13:16,870
他们有的是 PAX ，

239
00:13:16,920 --> 00:13:21,880
但这不是很大的区别。

240
00:13:24,570 --> 00:13:26,740
让我们从第一个开始， NSM 行存储，

241
00:13:27,270 --> 00:13:29,740
这是我们本学期到目前为止已经说过的内容，

242
00:13:30,090 --> 00:13:32,945
我们假设给定 tuple 的几乎所有属性

243
00:13:32,945 --> 00:13:34,690
都连续存储在一个页面中，

244
00:13:35,040 --> 00:13:37,120
一个接一个，

245
00:13:37,720 --> 00:13:38,275
我们的想法是，

246
00:13:38,275 --> 00:13:39,670
你要在页面上，

247
00:13:39,670 --> 00:13:42,030
放置给定 tuple 的所有数据，

248
00:13:42,080 --> 00:13:43,300
你不会开始任何东西，

249
00:13:43,300 --> 00:13:45,610
你不会为下一个 tuple 放下任何比特，

250
00:13:45,610 --> 00:13:47,550
直到你完成当前的 tuple 。

251
00:13:48,730 --> 00:13:51,105
这对于 OLTP 系统来说会更好的原因，

252
00:13:51,105 --> 00:13:52,490
正如我之前已经说过的，

253
00:13:52,630 --> 00:13:53,900
OLTP 应用程序是，

254
00:13:54,970 --> 00:13:59,560
大多数查询将访问单个条目或单个 tuple ，

255
00:13:59,850 --> 00:14:01,925
现在，我可以转到一个页面，

256
00:14:01,925 --> 00:14:04,330
获取单一属性所需的所有数据，

257
00:14:04,380 --> 00:14:07,540
这就是我满足该查询所需的全部数据。

258
00:14:08,930 --> 00:14:10,060
我们已经谈到了页面大小，

259
00:14:10,060 --> 00:14:12,900
但是，它总是硬件页面的倍数。

260
00:14:14,280 --> 00:14:17,110
所以这基本上是我们之前看到的相同的布局，

261
00:14:17,310 --> 00:14:18,700
我们有一些数据库页面，

262
00:14:18,960 --> 00:14:21,460
我们在前面有一个 header ，接着 slot array ，

263
00:14:21,810 --> 00:14:25,960
然后，当我们开始扫描我们的表，

264
00:14:26,100 --> 00:14:27,700
开始插入数据的应用程序，

265
00:14:28,020 --> 00:14:30,850
它只会将条目追加到末尾，

266
00:14:31,950 --> 00:14:34,210
并不断添加越来越多，

267
00:14:34,920 --> 00:14:37,060
然后认为它已经填满了，

268
00:14:37,740 --> 00:14:39,230
现在，如果出现任何查询，

269
00:14:39,230 --> 00:14:43,930
比如 SELECT * FROM 从这个表中， id 等于某个东西，

270
00:14:44,540 --> 00:14:45,670
我们可以获取这一页，

271
00:14:45,900 --> 00:14:48,220
跳到 slot array 中定义的偏移量，

272
00:14:48,450 --> 00:14:49,870
我们就可以获得所需的所有数据。

273
00:14:51,830 --> 00:14:54,070
让我们看看这在我们的维基百科示例是如何工作的，

274
00:14:55,350 --> 00:14:56,210
假设你在这里有一个查询，

275
00:14:56,210 --> 00:14:57,280
其中有人想要登录，

276
00:14:57,930 --> 00:14:59,800
他们传入用户名和密码，

277
00:15:00,390 --> 00:15:01,900
我们检查一下是否匹配，

278
00:15:01,920 --> 00:15:05,500
这大致就是登录到基于数据库应用程序的方式，

279
00:15:06,000 --> 00:15:06,400
使用，

280
00:15:06,420 --> 00:15:08,410
如果你对数据库进行身份验证，它大致就是这样。

281
00:15:09,430 --> 00:15:14,000
我们可以忽略如何为一个给定的用户找到我们想要的数据，

282
00:15:14,260 --> 00:15:15,680
但假设有某种索引，

283
00:15:16,030 --> 00:15:17,960
哈希表，还是 B+ 树，并不重要，

284
00:15:18,250 --> 00:15:19,485
我们将在第 8 课中讨论这个，

285
00:15:19,485 --> 00:15:20,420
但有一种方法可以说明，

286
00:15:20,680 --> 00:15:24,680
对于这个用户帐户，这是记录 ID 和偏移量。

287
00:15:25,620 --> 00:15:27,190
现在，我们进入页面目录，

288
00:15:27,270 --> 00:15:29,830
找到包含我们要查找的数据的页面，

289
00:15:30,210 --> 00:15:31,330
我们查看 slot array ，

290
00:15:31,410 --> 00:15:32,620
跳到某个偏移量，

291
00:15:32,850 --> 00:15:36,100
现在我们有了查询所需的所有数据，

292
00:15:36,270 --> 00:15:37,840
我们可以生成结果。

293
00:15:38,490 --> 00:15:39,950
这是 OLTP 的理想选择，

294
00:15:39,950 --> 00:15:41,590
因为所有数据都是连续的。

295
00:15:42,820 --> 00:15:44,060
同样的事情，我们想要做插入，

296
00:15:44,230 --> 00:15:46,545
我们所需要做的就是查看页面目录，

297
00:15:46,545 --> 00:15:48,350
找到一个有空闲空间的页面，

298
00:15:48,850 --> 00:15:50,480
给它一个内存，假设就是这个，

299
00:15:51,550 --> 00:15:54,600
然后，把它追加到末尾。

300
00:15:55,650 --> 00:15:56,030
那很好。

301
00:15:57,200 --> 00:15:58,465
但是现在，如果我尝试运行这个查询，

302
00:15:58,465 --> 00:16:00,840
其中我想找到所有，

303
00:16:01,340 --> 00:16:03,180
获得人们每月登录的次数，

304
00:16:03,380 --> 00:16:05,910
如果它们的主机名以 .gov 结尾，

305
00:16:07,250 --> 00:16:08,520
现在，在本例中你可以看到，

306
00:16:08,570 --> 00:16:11,010
我必须扫描表中的所有页面，

307
00:16:11,510 --> 00:16:14,610
因为我需要查看所有内容，所有用户帐户，

308
00:16:15,350 --> 00:16:16,950
然后当我带入一个页面，

309
00:16:18,600 --> 00:16:21,065
我们将粗略地执行查询的方式，

310
00:16:21,065 --> 00:16:22,445
我们还没有了解如何执行查询，

311
00:16:22,445 --> 00:16:23,680
但大致的想法是，

312
00:16:23,910 --> 00:16:26,650
我们有这个 WHERE 子句，这是关于主机名的，

313
00:16:26,970 --> 00:16:30,460
我们需要在一个页面中找到 tuple ，

314
00:16:30,480 --> 00:16:33,910
主机名上的谓词是满足的。

315
00:16:34,710 --> 00:16:38,710
所以，我们真正需要查看的唯一数据就是这里的 hostname ，

316
00:16:40,910 --> 00:16:45,900
然后我们必须对 lastLogin 进行聚合，对于 GROUP BY ，

317
00:16:47,180 --> 00:16:49,090
这意味着我们需要查看的唯一数据，

318
00:16:49,090 --> 00:16:52,200
对于那个查询部分来说，就是这些属性。

319
00:16:54,205 --> 00:16:54,960
那么最明显的问题是什么？

320
00:16:59,820 --> 00:17:00,785
你必须遍历所有的行，

321
00:17:00,785 --> 00:17:02,620
你带来了一堆数据，而你实际上并不需要，

322
00:17:03,690 --> 00:17:06,010
所以，为了获得我需要的属性，

323
00:17:06,270 --> 00:17:07,880
我必须引入整个页面，

324
00:17:07,880 --> 00:17:10,180
但是整个页面带来了一些我不需要的属性，

325
00:17:10,410 --> 00:17:13,930
userID, userName, userPass ，我在这个查询中不需要的，

326
00:17:14,310 --> 00:17:16,220
所以我在做无用的 IO ，

327
00:17:16,220 --> 00:17:18,520
我从磁盘中获取数据，

328
00:17:18,690 --> 00:17:22,170
但我根本不需要它，

329
00:17:22,170 --> 00:17:23,445
这不仅很慢，

330
00:17:23,445 --> 00:17:26,540
而且在某些系统中，你为每个磁盘 IOPS 付费，

331
00:17:27,490 --> 00:17:29,210
在亚马逊的 Aurora 上，你付费，

332
00:17:29,260 --> 00:17:30,770
如果你从磁盘上读取某些东西，

333
00:17:30,850 --> 00:17:34,785
你按查询的 IO 操作次数付费，

334
00:17:34,785 --> 00:17:35,640
所以在这种情况下，

335
00:17:35,640 --> 00:17:38,570
我是在为我实际上并不需要的数据付费。

336
00:17:41,830 --> 00:17:43,485
这就是 NSM 的明显问题所在，

337
00:17:43,485 --> 00:17:45,165
它非常适用于插入、更新和删除，

338
00:17:45,165 --> 00:17:49,730
也适用于需要获取数据库中单个实体的全部数据的查询，

339
00:17:50,470 --> 00:17:53,060
但是，如果你想要扫描表的大部分，

340
00:17:53,710 --> 00:17:55,725
并且只需要一个子集的属性，

341
00:17:55,725 --> 00:17:59,715
大多数 OLAP 查询所需要的，

342
00:17:59,715 --> 00:18:03,510
在一个很大的表上调用 SELECT * 是非常罕见的，

343
00:18:03,510 --> 00:18:05,060
因为你基本上是在获取整个东西，

344
00:18:05,680 --> 00:18:08,630
还有其他实用程序可以让它运行得更快，除了 SELECT * 。

345
00:18:09,270 --> 00:18:10,490
所以，这可能对 OLAP 不利，

346
00:18:10,490 --> 00:18:12,760
因为我们带来了不需要的数据。

347
00:18:14,620 --> 00:18:17,295
这是一种低级别的详细信息，

348
00:18:17,295 --> 00:18:19,610
但回到这里的这一部分，

349
00:18:19,900 --> 00:18:26,200
考虑如何执行查询，运行这个谓词，

350
00:18:26,520 --> 00:18:29,800
我跳转到内存中的不同位置，

351
00:18:30,360 --> 00:18:32,045
以执行我的扫描，

352
00:18:32,045 --> 00:18:35,050
我必须读取第一个 tuple 的 header ，

353
00:18:35,370 --> 00:18:38,740
找出我需要跳过多远才能获得 hostname ，

354
00:18:39,120 --> 00:18:40,865
然后我也许可以查看 lastLogin ，

355
00:18:40,865 --> 00:18:42,490
我一直在计算聚合，

356
00:18:42,660 --> 00:18:44,560
但我跳到下一个 tuple ，

357
00:18:44,760 --> 00:18:48,910
然后跳过，以获得它的 hostname 属性。

358
00:18:49,290 --> 00:18:51,395
在现代超大规模的 CPU 中，

359
00:18:51,395 --> 00:18:52,030
这是很糟糕的，

360
00:18:52,140 --> 00:18:55,300
因为有一堆这样的非顺序操作，

361
00:18:55,380 --> 00:18:57,010
也可能变得不确定，

362
00:18:57,090 --> 00:19:02,200
我访问的内存位置是，

363
00:19:02,850 --> 00:19:03,635
它不是随机的，

364
00:19:03,635 --> 00:19:05,410
因为你总是以递增的顺序进入，

365
00:19:05,520 --> 00:19:08,105
但这不会像我只是读取[步长的]内存，

366
00:19:08,105 --> 00:19:09,640
并非常快地处理它。

367
00:19:11,690 --> 00:19:12,775
这是一个低层次的细节，

368
00:19:12,775 --> 00:19:14,170
我们在本学期没有真正涉及到，

369
00:19:14,170 --> 00:19:17,190
但它至少值得讨论。

370
00:19:18,240 --> 00:19:19,210
然后我们会看到这个，

371
00:19:19,650 --> 00:19:21,590
我们将在这节课的后面部分讨论压缩，

372
00:19:21,590 --> 00:19:23,180
但在这里，

373
00:19:23,180 --> 00:19:25,235
我们将无法获得良好的压缩率，

374
00:19:25,235 --> 00:19:28,660
如果你想要减少单个页面中的数据量或在单个页面中填充更多数据，

375
00:19:28,710 --> 00:19:33,290
因为给定表的所有属性都被放在该页面中，

376
00:19:33,290 --> 00:19:36,815
重复性的机会会更少，

377
00:19:36,815 --> 00:19:38,855
识别的机会也会更少，

378
00:19:38,855 --> 00:19:40,390
嘿，这些值是一样的，

379
00:19:40,560 --> 00:19:43,215
我可以把它们压缩得很好，

380
00:19:43,215 --> 00:19:43,970
回来这里，

381
00:19:44,380 --> 00:19:46,490
我们有 userID ，是一个整数，

382
00:19:46,990 --> 00:19:48,330
userName 是随机字符串，

383
00:19:48,330 --> 00:19:49,430
userPass 是随机字符串，

384
00:19:49,750 --> 00:19:52,610
这将是所有不同的值域，

385
00:19:52,960 --> 00:19:54,710
这对于压缩来说并不是理想的。

386
00:19:58,450 --> 00:20:03,140
另一种方法是 DSM ，列存，分解存储模型，

387
00:20:03,670 --> 00:20:05,180
这里的想法是，

388
00:20:05,500 --> 00:20:10,010
不是在一个页面中存储单个 tuple 的所有属性，

389
00:20:10,690 --> 00:20:13,460
我们将存储所有 tuple 的所有属性，

390
00:20:14,510 --> 00:20:19,260
抱歉，对于所有 tuple ，在一个页面中存储单个属性。

391
00:20:19,260 --> 00:20:21,110
我有 lastLogin 字段，

392
00:20:21,370 --> 00:20:24,900
不是将所有属性与单个页面中其他属性混合在一起，

393
00:20:24,900 --> 00:20:28,700
我只有 lastLogin 属性。

394
00:20:29,780 --> 00:20:32,125
这对于 OLAP 查询来说将是理想的，

395
00:20:32,125 --> 00:20:34,350
因为它们将扫描整个表，

396
00:20:34,730 --> 00:20:37,290
只扫描属性的一部分，

397
00:20:37,610 --> 00:20:39,840
现在，当我从磁盘获取页面时，

398
00:20:39,860 --> 00:20:41,020
我只获取数据，

399
00:20:41,020 --> 00:20:43,500
我知道我只获取实际需要的属性的数据，

400
00:20:44,410 --> 00:20:47,660
而不是其他[顺带]的东西。

401
00:20:49,340 --> 00:20:50,190
所以，

402
00:20:51,240 --> 00:20:53,530
像 SQL 这样的声明语言的好处是，

403
00:20:53,640 --> 00:20:55,355
你不必知道，不必关心，

404
00:20:55,355 --> 00:20:58,120
自己是在行存储系统上运行，还是在列存储系统上运行，

405
00:20:58,260 --> 00:21:00,970
你相同的 SQL 查询，可以很好地工作，（结果）是一样的，

406
00:21:02,100 --> 00:21:04,030
但现在这是数据库系统的责任，

407
00:21:04,500 --> 00:21:06,190
意味着我们实际构建它的人，

408
00:21:06,390 --> 00:21:07,715
这是我们的责任，

409
00:21:07,715 --> 00:21:11,740
获取数据，将其拆分成单独的列，分隔属性，

410
00:21:11,910 --> 00:21:15,700
然后在需要产生结果时将其缝合在一起。

411
00:21:19,640 --> 00:21:22,560
好的，这只是我之前分享的另一个相同的图表，

412
00:21:23,120 --> 00:21:24,385
同样，考虑这一点的方法是，

413
00:21:24,385 --> 00:21:26,850
对于这里的第一列，第一个属性列 A ，

414
00:21:27,140 --> 00:21:28,950
我们有一个单独的文件，其中有一系列页面，

415
00:21:29,720 --> 00:21:30,840
现在，它会有一个 header ，

416
00:21:30,860 --> 00:21:33,300
告诉我们页面里面有什么，

417
00:21:33,920 --> 00:21:36,120
然后我们就有了这个 null bitmap ，

418
00:21:36,800 --> 00:21:42,390
对于所有列，抱歉，是该列中的所有值，

419
00:21:43,220 --> 00:21:47,670
接下来是表中所有 tuple 的连续值，

420
00:21:49,490 --> 00:21:51,390
我们在下一个和下一个也是这样做的。

421
00:21:52,590 --> 00:21:56,600
好的，所以这些仍然是，

422
00:21:56,600 --> 00:21:59,590
这些文件仍然会像我们之前谈到的那样被分解成数据库页，

423
00:21:59,820 --> 00:22:03,010
无论是 4 千字节还是 8 千字节，无论系统支持什么，

424
00:22:03,690 --> 00:22:09,910
但是，文件本身将包含，仅仅是单个属性的数据，

425
00:22:10,380 --> 00:22:13,550
现在这些不同文件的元数据开销

426
00:22:13,550 --> 00:22:16,985
实际上比行存储中的要少得多，

427
00:22:16,985 --> 00:22:20,570
因为我不必像跟踪所有其他的，

428
00:22:20,570 --> 00:22:25,510
比如每一列，它是否为空，

429
00:22:26,280 --> 00:22:29,980
关于偏移量或在哪里找到东西的不同信息，

430
00:22:30,000 --> 00:22:32,720
这些都将是存储中的元数据，

431
00:22:32,720 --> 00:22:35,950
因为它们都是相同的值域，相同的属性类型，

432
00:22:36,180 --> 00:22:38,050
它比行存储小得多。

433
00:22:41,090 --> 00:22:43,720
好的，所以，这里的想法是，

434
00:22:43,720 --> 00:22:45,090
所以我们回到维基百科的例子，

435
00:22:45,440 --> 00:22:50,310
我们只获取表中的每一列，

436
00:22:50,840 --> 00:22:54,640
然后将其存储为单独的页面，

437
00:22:54,640 --> 00:22:57,360
如果你回到主机名的示例，

438
00:22:57,530 --> 00:23:01,200
在单个页面中，我们只存储主机名列的值，

439
00:23:04,255 --> 00:23:06,420
我们的所有属性都将有单独的页面。

440
00:23:07,920 --> 00:23:12,245
所以，如果我们回到这里的 OLAP 查询，

441
00:23:12,245 --> 00:23:16,660
然后我们进行查找，政府地址每月登录的数量，

442
00:23:17,870 --> 00:23:20,400
执行查询的第一部分是获得 hostname ，

443
00:23:20,810 --> 00:23:23,820
好的，这是假设每个属性都有一个页面，

444
00:23:24,020 --> 00:23:25,620
它将获取单个页面，

445
00:23:26,150 --> 00:23:28,800
然后执行扫描，只需遍历列，

446
00:23:29,300 --> 00:23:32,880
并识别与该 hostname 匹配的所有内容。

447
00:23:33,630 --> 00:23:37,835
我已经完全利用了我引入的所有数据，

448
00:23:37,835 --> 00:23:40,600
因为我只引入了这个查询所需的数据，

449
00:23:40,710 --> 00:23:42,730
我不会引入我不关心的属性。

450
00:23:44,130 --> 00:23:45,815
然后我们将在后面讨论这个，

451
00:23:45,815 --> 00:23:46,390
我们如何做，

452
00:23:47,070 --> 00:23:49,750
我们讨论查询执行，如何进行匹配，

453
00:23:49,770 --> 00:23:51,455
但是假设我跟踪一个列表，

454
00:23:51,455 --> 00:23:55,450
这是该列中与我的谓词匹配的 tuple 的偏移量。

455
00:23:56,320 --> 00:23:59,120
然后我现在转到 lastLogin 页面，

456
00:23:59,470 --> 00:24:02,030
去取那个只有我们需要的数据，

457
00:24:02,350 --> 00:24:07,095
然后，现在我知道如何跳到匹配 hostname 的不同偏移量，

458
00:24:07,095 --> 00:24:10,760
以找到正确的 login 偏移量，即登录时间戳，

459
00:24:11,340 --> 00:24:12,920
然后计算出查询所需的内容。

460
00:24:16,270 --> 00:24:16,880
这里清楚了吗？

461
00:24:18,770 --> 00:24:20,520
今天在座的各位，有谁之前听说列存储？

462
00:24:22,320 --> 00:24:23,410
少于 10% ，好的。

463
00:24:24,220 --> 00:24:25,340
所以，这是一种，

464
00:24:26,500 --> 00:24:27,800
这似乎是显而易见的，

465
00:24:28,000 --> 00:24:29,580
这显然是你想要做的方式，

466
00:24:29,580 --> 00:24:34,130
但是，在 15 年前，在 20 年前，

467
00:24:34,480 --> 00:24:36,675
任何数据库系统都不是这样构建的，

468
00:24:36,675 --> 00:24:37,640
这是非常罕见的。

469
00:24:50,680 --> 00:24:51,770
抱歉，你的问题是，

470
00:24:51,940 --> 00:24:53,720
如果我回到行存储的例子，

471
00:24:57,610 --> 00:24:58,250
这里的这个，

472
00:24:58,480 --> 00:24:59,340
你的问题是，

473
00:24:59,340 --> 00:25:01,640
即使在这里，我是否也必须。

474
00:25:07,780 --> 00:25:10,230
哦，比如，这个，

475
00:25:11,740 --> 00:25:14,180
从字面上看，是的，为什么。

476
00:25:15,400 --> 00:25:24,030
哦，好的，问题是，

477
00:25:24,530 --> 00:25:25,960
我说过有一些索引，

478
00:25:25,960 --> 00:25:26,755
我没说那是什么，

479
00:25:26,755 --> 00:25:28,080
有一种神奇的方式可以说，

480
00:25:28,520 --> 00:25:29,440
看看 WHERE 子句，

481
00:25:29,440 --> 00:25:30,840
它说 userName 等于某个东西，

482
00:25:31,850 --> 00:25:33,510
因为你会走到 userName 的索引，

483
00:25:33,530 --> 00:25:35,550
而我神奇地得到了单一的记录，

484
00:25:36,110 --> 00:25:38,280
record ID ， page ID 和偏移量，

485
00:25:38,630 --> 00:25:39,450
我怎么做的，

486
00:25:39,920 --> 00:25:40,795
这就是索引的作用，

487
00:25:40,795 --> 00:25:42,320
那是下个星期，

488
00:25:42,700 --> 00:25:43,760
这只是一个键值，

489
00:25:45,100 --> 00:25:47,480
你可以想象成键值映射或关联数组，

490
00:25:47,650 --> 00:25:54,410
对于给出的 userName ，给出匹配的 record ID 或 record IDs（如果不是唯一的），

491
00:25:54,910 --> 00:25:57,860
我的索引给我 record ID ，

492
00:25:58,060 --> 00:25:59,360
我查看我的页面目录，

493
00:25:59,470 --> 00:26:01,125
说，好的，我需要页面 1 2 3 ，

494
00:26:01,125 --> 00:26:01,730
它在哪里，

495
00:26:01,870 --> 00:26:03,720
它要么在内存中，要么在磁盘上，

496
00:26:03,720 --> 00:26:04,400
我去拿它，

497
00:26:04,690 --> 00:26:06,570
现在我有了 record ID 中的插槽编号，

498
00:26:06,570 --> 00:26:08,450
我查看那个页面，并跳到那个插槽，

499
00:26:08,770 --> 00:26:09,440
获得我需要的东西，

500
00:26:09,760 --> 00:26:12,325
所以，这使我可以准确地跳到我需要的页面，

501
00:26:12,325 --> 00:26:15,360
然后在该页面中找到我需要的记录，

502
00:26:16,240 --> 00:26:18,770
但是同样，如果我只需要一个，

503
00:26:19,810 --> 00:26:21,200
假设 userName 是唯一的，

504
00:26:21,460 --> 00:26:22,850
我只需要一个 userName ，

505
00:26:24,670 --> 00:26:26,900
但我不得不去取，我实际上并不需要的所有其他行。

506
00:26:29,290 --> 00:26:29,655
是的。

507
00:26:29,655 --> 00:26:42,370
你说的是什么意思，稍微更多地实现？

508
00:27:08,390 --> 00:27:10,410
我想说它们是一样难的，

509
00:27:12,770 --> 00:27:15,240
如果你不关心其他一堆保护措施，

510
00:27:16,130 --> 00:27:16,920
我们不是在讨论事务，

511
00:27:17,060 --> 00:27:19,850
但是，如果你不关心这些事情，

512
00:27:20,440 --> 00:27:23,660
那么，是的，我同意行存储将更容易实现，

513
00:27:26,110 --> 00:27:29,210
所有东西都在一起，那么是的，

514
00:27:29,380 --> 00:27:33,800
假设每条记录都可以放在一个页面中，忽略溢出，

515
00:27:34,090 --> 00:27:36,260
那么行存储可能会更容易，是的。

516
00:27:37,970 --> 00:27:38,370
是的。

517
00:27:38,510 --> 00:27:46,260
所以，他的问题是，

518
00:27:47,570 --> 00:27:52,900
数据库系统存储是行存储还是列存储，

519
00:27:52,900 --> 00:27:54,420
这是可以对表配置的东西，

520
00:27:54,980 --> 00:27:57,210
还是一种全有或全无？

521
00:27:58,980 --> 00:28:03,940
大多数系统将只使用行或列，

522
00:28:06,000 --> 00:28:09,910
HTAP 的东西，混合的东西，试图同时做到这两种，

523
00:28:11,820 --> 00:28:15,250
所以通常你会说，

524
00:28:15,570 --> 00:28:17,380
是的，所以在大多数系统中你会说，

525
00:28:18,510 --> 00:28:20,830
我知道我在用这个系统，它将是一个列存储，

526
00:28:20,910 --> 00:28:22,060
所以我将在那里存储所有内容，

527
00:28:24,030 --> 00:28:25,480
表将成为列，

528
00:28:25,950 --> 00:28:29,200
尽管我说过。

529
00:28:30,480 --> 00:28:33,010
尽管它是一个列存储，

530
00:28:33,600 --> 00:28:36,550
我们将针对只读查询进行优化，

531
00:28:36,750 --> 00:28:39,095
人们显然想要更新数据，

532
00:28:39,095 --> 00:28:42,070
所以，通常的解决方法是，

533
00:28:42,330 --> 00:28:46,390
这些系统会有一种行存储缓冲区，

534
00:28:47,070 --> 00:28:48,790
它通常是日志结构的，

535
00:28:49,050 --> 00:28:50,890
如果我有任何更新，

536
00:28:50,940 --> 00:28:53,510
我会将它们应用到行部分，

537
00:28:53,510 --> 00:28:56,440
然后定期将它们合并到列存储中，

538
00:28:57,420 --> 00:28:58,660
这是解决这一问题的一种方法，

539
00:29:00,300 --> 00:29:01,385
Oracle 采用了一种不同的方法，

540
00:29:01,385 --> 00:29:04,720
其中行存储被视为数据库的主存储位置，

541
00:29:05,070 --> 00:29:09,520
但随后他们将以列存储格式制作表的副本，

542
00:29:09,900 --> 00:29:12,460
并负责为你保持更新，

543
00:29:12,570 --> 00:29:13,805
所以不同的方法，不同的东西，

544
00:29:13,805 --> 00:29:17,690
但通常如果系统支持我想用行存储而不是列存储，

545
00:29:17,690 --> 00:29:19,630
你可以在每张表的基础上定义它，

546
00:29:19,950 --> 00:29:21,610
但大多数系统不这样做。

547
00:29:22,540 --> 00:29:22,940
是的。

548
00:29:37,080 --> 00:29:37,790
这个问题是，

549
00:29:37,790 --> 00:29:40,480
如果我有和列一样多的磁盘，

550
00:29:40,800 --> 00:29:43,000
假设我在列存储表中拆分，

551
00:29:43,620 --> 00:29:47,020
每个属性转到单独的磁盘，

552
00:29:47,250 --> 00:29:48,730
这会和行存储一样快吗。

553
00:29:50,460 --> 00:29:51,880
不，因为你必须要做，

554
00:29:52,110 --> 00:29:55,750
你还是要做那个分裂，然后写出来，

555
00:29:56,640 --> 00:29:58,990
然后你也会有更多的内存压力，

556
00:29:59,070 --> 00:30:03,240
因为，假设我有一千个属性，

557
00:30:03,860 --> 00:30:05,850
所以现在，如果我必须更新一千个页面，

558
00:30:06,320 --> 00:30:08,700
我的缓冲池中有一千个页面，

559
00:30:08,720 --> 00:30:11,130
进行更新，

560
00:30:12,230 --> 00:30:13,585
使用一个新的属性更新它们，

561
00:30:13,585 --> 00:30:15,735
然后将它们全部写出来，

562
00:30:15,735 --> 00:30:19,860
通常，在列存储系统中更新，

563
00:30:19,860 --> 00:30:21,770
在没有我刚才提到的这种缓冲区的情况下，

564
00:30:22,150 --> 00:30:23,300
总是很慢的。

565
00:30:25,590 --> 00:30:25,990
是的。

566
00:30:32,050 --> 00:30:32,835
她的问题是，

567
00:30:32,835 --> 00:30:37,410
在行存储中，什么是与 null bitmap 相对应的，

568
00:30:37,410 --> 00:30:38,630
是的，我们上节课讨论过了，

569
00:30:38,800 --> 00:30:43,575
它是一种表示哪个属性为空的方式，

570
00:30:43,575 --> 00:30:44,865
我没有在这里画出来，

571
00:30:44,865 --> 00:30:46,130
上一节课的 header ，图表，

572
00:30:46,390 --> 00:30:48,050
在每行的 header 中，

573
00:30:48,340 --> 00:30:53,060
都有一个 bitmap ，说明哪个属性是空的，

574
00:30:53,140 --> 00:30:55,130
这是一种你可以做到的常规的方法，

575
00:30:55,630 --> 00:30:56,360
你可以这样做，

576
00:30:56,470 --> 00:30:59,130
我们谈论特定值或每个属性，

577
00:30:59,130 --> 00:31:00,800
你可以在它前面放一个小标志，

578
00:31:01,060 --> 00:31:03,170
null bitmap 基本上表示属性，

579
00:31:03,190 --> 00:31:05,690
对于这个 tuple ，属性一是空，属性二是空，

580
00:31:06,010 --> 00:31:06,950
所以想一想，

581
00:31:07,000 --> 00:31:10,550
不是在每个 tuple 的 header 中有 bitmap ，

582
00:31:10,900 --> 00:31:13,970
在列存储中，对于整个列，这是 null bitmap 。

583
00:31:17,060 --> 00:31:17,460
是的。

584
00:31:18,340 --> 00:31:26,840
他的问题是，

585
00:31:26,890 --> 00:31:27,950
在列存储中，

586
00:31:29,930 --> 00:31:31,530
在列存储中，索引是做什么的，

587
00:31:31,790 --> 00:31:34,650
所以一些 OLAP 系统是列存储，

588
00:31:35,630 --> 00:31:36,550
你不会有任何索引，

589
00:31:37,400 --> 00:31:39,000
我不认为 Snowflake 给了你索引，

590
00:31:39,140 --> 00:31:40,270
你不能创建一个，

591
00:31:40,270 --> 00:31:43,200
它可能已经改变了[]，你不能有索引，

592
00:31:45,080 --> 00:31:49,590
因为，他们并不是试图进行点查询或单一事物查找，

593
00:31:50,180 --> 00:31:51,120
来进行完整的扫描。

594
00:31:51,930 --> 00:31:55,000
所以现在你说到点子上了，你是对的，

595
00:31:55,020 --> 00:31:58,690
你可以拥有范围索引，

596
00:31:58,740 --> 00:32:03,490
所以，如果你的 ID 是 0 和 100 ，可以在这里找到这个页面，

597
00:32:05,370 --> 00:32:06,460
有这样的东西，

598
00:32:06,920 --> 00:32:13,600
有倒排索引，比如在所有记录中查找关键字 Andy ，

599
00:32:13,980 --> 00:32:15,365
这看起来不像是树形结构，

600
00:32:15,365 --> 00:32:16,420
这通常是一个哈希表，

601
00:32:16,530 --> 00:32:17,810
有不同类型的索引，

602
00:32:17,810 --> 00:32:18,400
但你不会，

603
00:32:19,540 --> 00:32:22,130
也许你不会有执行点查询查找的索引。

604
00:32:27,470 --> 00:32:28,110
好的，太酷了。

605
00:32:30,370 --> 00:32:32,330
所以，让我们跳回去。

606
00:32:36,180 --> 00:32:39,280
好的，我试图解释这部分，

607
00:32:39,810 --> 00:32:40,750
我说，好的，

608
00:32:40,950 --> 00:32:43,720
让我去获取包含 hostname 的页面，

609
00:32:44,280 --> 00:32:46,330
运行我的 WHERE 子句，我会得到一堆匹配的，

610
00:32:46,740 --> 00:32:49,090
然后让我去获取 lastLogin 页面，

611
00:32:49,470 --> 00:32:52,960
然后我有了一种神奇的方法在那里找到匹配的东西，

612
00:32:53,860 --> 00:32:54,620
他们是怎么做到的。

613
00:32:55,460 --> 00:32:56,250
好的，有两种方法。

614
00:32:57,450 --> 00:33:00,370
最常见的一种是进行固定长度的偏移，

615
00:33:01,340 --> 00:33:03,840
这意味着，

616
00:33:04,610 --> 00:33:07,290
你不是通过插槽编号来标识行，

617
00:33:07,670 --> 00:33:09,630
而是通过唯一 tuple 来标识，

618
00:33:09,950 --> 00:33:13,200
这就是为什么我不想说行代替 tuple 记录，

619
00:33:13,760 --> 00:33:17,460
因为列存储中的行是什么样子的，

620
00:33:18,570 --> 00:33:19,510
tuple 是更好的术语，

621
00:33:19,620 --> 00:33:26,050
但是 tuple 的唯一标识符将是它在表中的偏移量，

622
00:33:27,060 --> 00:33:31,270
所以现在如果我在一个列中的偏移量 3 ，

623
00:33:31,740 --> 00:33:34,745
我就知道如何跳到偏移量 3 和另一列，

624
00:33:34,745 --> 00:33:36,850
然后我可以把 tuple 缝合在一起，

625
00:33:37,710 --> 00:33:41,110
但是，这仅在值是固定长度的情况下有效，

626
00:33:42,160 --> 00:33:43,700
当然，是什么打破了这一假设，

627
00:33:46,350 --> 00:33:48,850
可变长度 VARCHAR 字符串， BLOB,  TEXT ，

628
00:33:49,500 --> 00:33:51,070
我们稍后将讨论如何处理，

629
00:33:52,080 --> 00:33:54,680
所以这种方法，固定长度的列，

630
00:33:54,680 --> 00:33:56,080
这是最常见的。

631
00:33:57,480 --> 00:34:01,270
一种传统的方法是使用嵌入的 ID ，

632
00:34:01,560 --> 00:34:05,710
其中每个值都有一些唯一的 tuple 标识符，

633
00:34:06,030 --> 00:34:08,625
比如，有点像日志结构，

634
00:34:08,625 --> 00:34:11,000
比如一些计数器加 1 ，

635
00:34:11,920 --> 00:34:14,985
然后有一些索引结构，我在这里没有展示，

636
00:34:14,985 --> 00:34:17,360
对于给定的记录 ID ，给定的列，

637
00:34:18,130 --> 00:34:19,760
它告诉你跳到哪里。

638
00:34:20,500 --> 00:34:22,580
这很少见，

639
00:34:23,650 --> 00:34:24,690
你可能根本不应该提它，

640
00:34:24,690 --> 00:34:26,535
但是，这是一种方法，

641
00:34:26,535 --> 00:34:29,400
在过去，我忘记了，一些系统确实做了这个，

642
00:34:29,400 --> 00:34:31,700
因为它们有点像是[扭曲]行存储，使其成为列存储，

643
00:34:32,860 --> 00:34:34,650
但每个都用固定长度集合，

644
00:34:34,650 --> 00:34:36,590
当然，你现在要再次处理的问题是，

645
00:34:37,090 --> 00:34:40,810
如何将可变长度值转换为固定长度值，

646
00:34:41,790 --> 00:34:42,970
让我想想，你如何做到这个。

647
00:34:45,450 --> 00:34:45,850
是的。

648
00:34:47,170 --> 00:34:48,890
他说指针，指向什么？

649
00:34:56,030 --> 00:34:58,080
对于每个不连续的数据。

650
00:35:00,270 --> 00:35:03,815
是的，这可能会奏效，

651
00:35:03,815 --> 00:35:04,655
这个版本的问题是，

652
00:35:04,655 --> 00:35:07,760
你做就地更新，

653
00:35:07,760 --> 00:35:09,220
如果你只是把所有的数据打包，

654
00:35:09,870 --> 00:35:11,840
如果它是不变的，你就不会有这个问题，

655
00:35:11,840 --> 00:35:15,610
但如果它是可变的，那么你就有了碎片化。

656
00:35:17,800 --> 00:35:18,570
他说是 slot array ，

657
00:35:20,630 --> 00:35:22,140
但是它指向什么。

658
00:35:35,120 --> 00:35:36,330
是的，是的。

659
00:35:38,400 --> 00:35:40,660
这与他所说的有点类似，有可能奏效，

660
00:35:45,640 --> 00:35:48,310
压缩。

661
00:35:48,720 --> 00:35:52,925
所以，这个方法将会奏效，

662
00:35:52,925 --> 00:35:53,860
我不认为有人这么做，

663
00:35:54,990 --> 00:35:56,090
你可以把东西填满，

664
00:35:56,090 --> 00:35:57,430
但就像我们之前说的那样，那将是浪费的，

665
00:35:58,050 --> 00:36:00,520
但这基本上就是[字典压缩]的工作方式，

666
00:36:00,870 --> 00:36:07,060
字典压缩是用整型代码来替换一些可变长度值，

667
00:36:07,350 --> 00:36:09,850
它是固定长度的，通常是 32 位，

668
00:36:10,290 --> 00:36:17,240
我们可以在字典代码上做一些[谓词]，

669
00:36:17,440 --> 00:36:20,000
如果有必要，如果它匹配我们寻找的东西，

670
00:36:20,110 --> 00:36:22,760
去查找，找出实际的值是什么，

671
00:36:23,320 --> 00:36:24,230
这是一个打字错误，

672
00:36:24,250 --> 00:36:25,365
它不是更多在下周，

673
00:36:25,365 --> 00:36:31,065
它更多在这个小时，我们现在就会讨论。

674
00:36:31,065 --> 00:36:32,460
这就是我们如何能够解决这个问题，

675
00:36:32,460 --> 00:36:35,600
以及几乎每个人在现代系统中解决问题的方式。

676
00:36:38,130 --> 00:36:40,750
这种[]的想法并不是新的，

677
00:36:41,100 --> 00:36:43,360
根据文献记载，

678
00:36:43,590 --> 00:36:46,750
最早的版本可以追溯到 1970 年代，

679
00:36:47,070 --> 00:36:51,400
瑞典国防部有一个名为 Cantor 的项目，

680
00:36:51,540 --> 00:36:54,880
与其说它是一个数据库系统，不如说它是一个文件系统，

681
00:36:55,200 --> 00:37:00,070
但这被认为是第一个记录在案的关于列存储系统的类似提案，

682
00:37:00,300 --> 00:37:02,620
我不知道它今天是否存在，

683
00:37:03,630 --> 00:37:07,025
在 1980 年代，有一篇论文实际描绘了

684
00:37:07,025 --> 00:37:10,990
分解存储模型的理论性质，

685
00:37:12,090 --> 00:37:14,890
但是，它在很大程度上仍然只是在学术界，

686
00:37:16,060 --> 00:37:20,450
被认为是列存储系统的第一个商业实现

687
00:37:20,860 --> 00:37:22,610
是一个被称为 SybaseIQ 的东西，

688
00:37:22,990 --> 00:37:26,150
但它并不是一个真正成熟的数据库系统，

689
00:37:26,920 --> 00:37:29,090
它更像是一个查询加速器，

690
00:37:29,470 --> 00:37:32,175
与我之前提到的 Oracle 类似，

691
00:37:32,175 --> 00:37:35,210
它们将行存储的副本复制到内存中列存储中，

692
00:37:35,380 --> 00:37:37,850
这基本上就是 Sybase 在 1990 年代做的事情，

693
00:37:38,110 --> 00:37:39,380
所以，你的查询将会出现，

694
00:37:39,520 --> 00:37:40,830
然后 Sybase 会计算出，

695
00:37:40,830 --> 00:37:44,210
我应该去行存储，并可能在那里做一些事情，

696
00:37:44,290 --> 00:37:47,930
或者我应该只在内存中列存储上运行查询。

697
00:37:49,610 --> 00:37:52,720
在 2000 年，是列存储真正起飞的时候，

698
00:37:52,720 --> 00:37:54,700
这个领域的三个关键系统，

699
00:37:54,700 --> 00:38:00,750
由 [] 创建， Mike Stonebraker ，

700
00:38:01,610 --> 00:38:07,140
Vectorwise 是一个 [] 数据库，它是 CWI 的，

701
00:38:08,030 --> 00:38:12,270
MonetDB 也是 CWI 的一个重要学术项目，

702
00:38:13,540 --> 00:38:15,200
DuckDB 来自 CWI ，

703
00:38:15,520 --> 00:38:20,270
DuckDB 的第一个版本实际上被称为 MonetDBLite ，

704
00:38:20,830 --> 00:38:22,110
他们扔掉了所有的代码，

705
00:38:22,110 --> 00:38:25,160
然后从头开始 DuckDB ，在了解了 MonetDBLite 之后，

706
00:38:25,700 --> 00:38:28,950
Vectorwise 是由一些在 MonetDB 工作的人创办的，

707
00:38:30,290 --> 00:38:33,160
然后 Vectorwise 的两个主要人员，

708
00:38:33,160 --> 00:38:36,360
其中一个离开了，是 Snowflake 的联合创始人，

709
00:38:36,890 --> 00:38:39,420
所以， Vectorwise 提出的许多早期想法都在 Snowflake ，

710
00:38:39,620 --> 00:38:41,665
然后另一个人， Peter Boncz ，

711
00:38:41,665 --> 00:38:42,595
他回到了 CWI ，

712
00:38:42,595 --> 00:38:45,660
然后，他帮助建议 DuckDB 项目。

713
00:38:46,950 --> 00:38:48,890
所以当时有很多其他的系统，

714
00:38:48,890 --> 00:38:52,090
但我认为这三个主要的系统是这个领域的先驱者。

715
00:38:53,920 --> 00:38:55,910
实际上，这一切出来，

716
00:38:55,960 --> 00:38:59,480
Mike 说的，或者 Stonebraker 说的方法，

717
00:38:59,680 --> 00:39:03,590
他是沃尔玛实验室的顾问，在 2000 年代早期，

718
00:39:03,820 --> 00:39:07,280
他们努力扩大他们的 TB 数据库，

719
00:39:07,450 --> 00:39:08,720
当时是行存储，

720
00:39:09,010 --> 00:39:13,070
我认为沃尔玛是多个 PB ，

721
00:39:13,180 --> 00:39:14,625
它是每一笔交易的数据库，

722
00:39:14,625 --> 00:39:16,520
任何时候，当有人在商店购买东西，

723
00:39:16,630 --> 00:39:18,980
比如[]，它就在那个数据库中，

724
00:39:19,420 --> 00:39:21,530
他们努力让 TB 数据运行更快，

725
00:39:21,910 --> 00:39:22,590
然后迈克想，

726
00:39:22,590 --> 00:39:24,560
哦，我们应该把这个做成列存储，

727
00:39:24,730 --> 00:39:29,810
然后他创立了 C-store 项目，后来变成了 Vectica ，

728
00:39:29,890 --> 00:39:33,330
然后，这是一个非常著名的项目。

729
00:39:34,250 --> 00:39:35,580
现在几乎每个人都这么做了，

730
00:39:35,840 --> 00:39:42,480
这只是一些被认为是列存储的数据库系统中的一个示例，

731
00:39:43,460 --> 00:39:46,620
但是 2010 年还有两件很有趣的事情，

732
00:39:46,790 --> 00:39:50,520
那就是有这些开源的文件格式， Parquet 和 ORC ，

733
00:39:50,630 --> 00:39:54,870
Parquet 来自 Dremio 和另一个我忘记的人，，

734
00:39:55,190 --> 00:39:56,400
ORC 来自 Facebook ，

735
00:39:56,930 --> 00:40:01,020
这些都是基于列的开源文件格式，

736
00:40:01,970 --> 00:40:06,630
现在你可以构建读写 Parquet ORC 文件的数据库系统。

737
00:40:09,810 --> 00:40:14,890
好的，列或分解存储模型的优势在于，

738
00:40:15,150 --> 00:40:17,050
我们将极大地减少 I/O 浪费的数量，

739
00:40:17,130 --> 00:40:18,740
我们进行分析查询所需的，

740
00:40:18,740 --> 00:40:22,120
因为我们只读取所需的数据，

741
00:40:23,520 --> 00:40:25,395
我们将获得更好的缓存使用

742
00:40:25,395 --> 00:40:27,710
和更好的访问模式位置，

743
00:40:27,730 --> 00:40:31,310
因为，我们实际上只需要一个接一个地[撕裂]列，

744
00:40:31,540 --> 00:40:34,700
而不必在内存中跳跃，

745
00:40:34,990 --> 00:40:36,080
这对 CPU 更好，

746
00:40:36,490 --> 00:40:37,940
同样，我们将获得更好的压缩，

747
00:40:38,260 --> 00:40:39,200
这是我们要讲到的。

748
00:40:39,940 --> 00:40:41,010
这是因为缺点是，

749
00:40:41,010 --> 00:40:42,510
点查询的速度会很慢，

750
00:40:42,510 --> 00:40:43,695
插入，更新，删除的速度会很慢，

751
00:40:43,695 --> 00:40:45,500
因为我们将不得不拆分数据，

752
00:40:45,970 --> 00:40:48,075
并将多个数据写到多个位置，

753
00:40:48,075 --> 00:40:48,930
然后再带回它，

754
00:40:48,930 --> 00:40:50,150
如果我们想把它放在一起。

755
00:40:52,230 --> 00:40:52,630
是的。

756
00:40:56,520 --> 00:40:58,940
问题是，[]是否建立了自己的数据库，没有，

757
00:41:00,030 --> 00:41:01,750
它认为每个[]，

758
00:41:02,730 --> 00:41:06,190
但[]数据库是真实的，是一个真正的系统。

759
00:41:08,560 --> 00:41:08,960
好的。

760
00:41:10,870 --> 00:41:14,180
所以，有一点需要指出的是，

761
00:41:15,370 --> 00:41:16,725
在我前面的例子中，

762
00:41:16,725 --> 00:41:18,860
我展示了我运行一个查询，

763
00:41:19,150 --> 00:41:27,380
我在 hostname 列上进行了扫描，

764
00:41:27,970 --> 00:41:33,710
然后，我在 login 上运行了查询的扫描部分，

765
00:41:34,210 --> 00:41:35,655
你可以想象一下，

766
00:41:35,655 --> 00:41:38,060
就像我做了一个，然后它又移动到另一个，

767
00:41:38,140 --> 00:41:39,405
但是，在很多情况下的查询，

768
00:41:39,405 --> 00:41:43,195
你希望同时查看多个列，

769
00:41:43,195 --> 00:41:45,510
我的 WHERE 子句只引用了一个属性，

770
00:41:45,710 --> 00:41:48,150
但像你在为家庭作业一编写的查询中看到的那样，

771
00:41:48,410 --> 00:41:52,020
通常，你在 WHERE 子句中经常引用了多个列或多个属性，

772
00:41:52,750 --> 00:41:58,700
所以，现在维护会有点昂贵或繁琐，

773
00:42:00,100 --> 00:42:03,690
因为我沿着一列扫描，同时获取另一列，

774
00:42:03,690 --> 00:42:05,480
并试图将东西拼凑在一起，

775
00:42:06,160 --> 00:42:10,400
所以我们仍然希望有数据，

776
00:42:10,630 --> 00:42:11,685
我们想要一种方式，

777
00:42:11,685 --> 00:42:15,870
让在一起使用的属性，

778
00:42:16,340 --> 00:42:19,080
在我们的文件中彼此靠近的磁盘上，

779
00:42:19,610 --> 00:42:26,920
但仍然可以获得列存储布局的所有好处。

780
00:42:27,930 --> 00:42:29,950
这就是 PAX 模型，

781
00:42:31,560 --> 00:42:32,800
正如我所说的，

782
00:42:33,180 --> 00:42:35,320
在大多数系统中，他们说他们是一个列存储，

783
00:42:35,490 --> 00:42:36,730
他们是这么做的，

784
00:42:36,930 --> 00:42:39,100
Parquet 和 ORC 是这样做的。

785
00:42:39,820 --> 00:42:42,190
这个想法不是令人振奋的，

786
00:42:42,190 --> 00:42:43,290
它只是说，

787
00:42:43,790 --> 00:42:48,210
不是每个单独的列或属性都有一个单独的文件，

788
00:42:48,710 --> 00:42:53,940
我会把它们分成块，分成行组，

789
00:42:54,080 --> 00:42:57,960
让同一个 tuple 里的数据彼此接近，

790
00:42:59,680 --> 00:43:05,030
在同一个文件中，只是在不同的页面中隔开了一些。

791
00:43:05,410 --> 00:43:07,880
所以，如果我们回到我们这里的模拟示例，

792
00:43:08,080 --> 00:43:13,310
我们要做的就是将表水平分区到行组中，

793
00:43:13,630 --> 00:43:15,135
然后在行组中，

794
00:43:15,135 --> 00:43:18,570
我们将根据列对其进行分区，

795
00:43:18,570 --> 00:43:20,660
所以，你想想这里的前三行，

796
00:43:21,100 --> 00:43:26,270
我在我的巨大文件中有一些部分定义行组，

797
00:43:26,380 --> 00:43:27,770
我有那个行组的 header ，

798
00:43:27,970 --> 00:43:30,470
然后我把列 A 的所有属性都放在一起，

799
00:43:30,610 --> 00:43:33,300
然后是列 A 的所有值，

800
00:43:33,300 --> 00:43:34,200
列 B 的所有值，

801
00:43:34,200 --> 00:43:38,340
以及列 C 的所有值。

802
00:43:38,690 --> 00:43:40,555
所以现在如果我有一个 WHERE 子句，

803
00:43:40,555 --> 00:43:43,410
需要访问列 A 和列 C ，

804
00:43:43,850 --> 00:43:46,350
当我去获取这个行组的这些页面时，

805
00:43:46,760 --> 00:43:50,400
我有我需要的所有数据靠在一起，

806
00:43:50,540 --> 00:43:52,210
我也得到了[] I/O 的好处，

807
00:43:52,210 --> 00:43:56,260
因为这个行组将以几十兆字节的形式存在，

808
00:43:56,260 --> 00:44:02,700
而不是 4 千字节或 8 千字节的页面，

809
00:44:02,780 --> 00:44:04,200
下一个也是这样，诸如此类。

810
00:44:04,340 --> 00:44:07,920
所以，这大概就是 Parquet 的工作原理，

811
00:44:08,450 --> 00:44:13,770
有很多 Parquet 看起来是什么样子的图表或演示，

812
00:44:14,360 --> 00:44:18,150
他们基本上使用了我们在这里使用的所有相同的语言，

813
00:44:19,100 --> 00:44:22,240
这里他们说，页面的默认大小是 1 兆字节，

814
00:44:22,240 --> 00:44:24,145
因为他们想要把东西组合在一起，

815
00:44:24,145 --> 00:44:26,430
并尽可能多地[伸缩] I/O ，

816
00:44:26,630 --> 00:44:29,700
然后一个行组将是 128 兆字节。

817
00:44:32,190 --> 00:44:32,590
是的。

818
00:44:33,660 --> 00:44:39,390
问题是，

819
00:44:39,620 --> 00:44:42,510
这难道不是仍然有很多 I/O 的问题，

820
00:44:43,400 --> 00:44:44,640
如果你在做全表扫描时。

821
00:44:46,220 --> 00:44:49,030
header 告诉你东西在哪里，

822
00:44:49,470 --> 00:44:50,795
因为它是如此之大，

823
00:44:50,795 --> 00:44:52,870
我在这里带来入第一个 header ，

824
00:44:53,070 --> 00:44:55,250
我显示的是 header ，

825
00:44:55,250 --> 00:44:57,200
但是在真实系统或 Parquet Orc 中，

826
00:44:57,200 --> 00:44:58,180
它实际上位于 footer ，

827
00:44:58,350 --> 00:44:59,800
因为假设文件是不可变的，

828
00:44:59,820 --> 00:45:01,000
所以我不知道会是什么样子，

829
00:45:01,080 --> 00:45:03,910
我不知道一切会是什么样子，直到我写完它，

830
00:45:04,080 --> 00:45:05,290
所以它在 footer ，

831
00:45:05,340 --> 00:45:07,000
但这是一个次要问题，

832
00:45:07,230 --> 00:45:08,500
所以他的问题是，

833
00:45:08,670 --> 00:45:10,175
不会有和这里的行存储一样的问题，

834
00:45:10,175 --> 00:45:11,330
如果我做这个 PAX 的事情，

835
00:45:11,330 --> 00:45:14,830
因为现在如果我把整个行组都带入进来，

836
00:45:15,400 --> 00:45:16,740
我是不是要读一大堆我不需要的东西，

837
00:45:17,000 --> 00:45:19,255
所以你不会把整个行组都带进来，

838
00:45:19,255 --> 00:45:20,220
你把 header 带进来，

839
00:45:20,450 --> 00:45:23,640
说这是我的属性所在位置的偏移量，

840
00:45:24,080 --> 00:45:25,800
然后继续去取那些。

841
00:45:33,230 --> 00:45:34,290
实际上，你可以在这里看到，

842
00:45:36,660 --> 00:45:39,875
这里，你看到的是 footer ，而不是 header ，

843
00:45:39,875 --> 00:45:40,805
元数据在这里，

844
00:45:40,805 --> 00:45:43,840
文件和列元数据，偏移量在哪里，

845
00:45:44,010 --> 00:45:45,435
不是在 header ，而是在 footer ，

846
00:45:45,435 --> 00:45:47,740
对于 Parquet 和 Orc 也是一样的。

847
00:45:50,350 --> 00:45:53,840
好的，所以，正如你多次提到的，

848
00:45:54,070 --> 00:45:55,850
I/O 一直是我们的主要瓶颈，

849
00:45:56,770 --> 00:45:58,010
尤其是在分析查询时，

850
00:45:58,420 --> 00:46:01,995
如果我们假设数据未被压缩，

851
00:46:01,995 --> 00:46:03,560
那就意味着，

852
00:46:04,600 --> 00:46:08,820
无论表中 tuple 的确切大小是多少，

853
00:46:10,010 --> 00:46:13,110
每个页面都会带来确切的数据，

854
00:46:14,390 --> 00:46:16,470
所以，减少的最明显方法是，

855
00:46:17,870 --> 00:46:22,010
为了加快查询，你可以跳过数据，

856
00:46:22,390 --> 00:46:26,300
或者你可以让你提取的数据，把更多的东西放入内存。

857
00:46:26,890 --> 00:46:29,325
所以，跳过数据是列存储的帮助，

858
00:46:29,325 --> 00:46:32,210
因为你不必读取不需要的属性，

859
00:46:32,710 --> 00:46:33,675
压缩是另一种方法，

860
00:46:33,675 --> 00:46:35,180
说，好的，对于我获取的每个页面，

861
00:46:35,350 --> 00:46:38,090
我得到的比未压缩的页面更多的 tuple ，

862
00:46:38,980 --> 00:46:43,010
现在，这将是速度和压缩率之间的权衡，

863
00:46:43,690 --> 00:46:47,760
显然，磁盘可能会比 CPU 慢，

864
00:46:47,760 --> 00:46:48,920
尤其是在云环境中，

865
00:46:49,270 --> 00:46:53,170
我需要支付必须解压缩和压缩数据的额外成本，

866
00:46:53,860 --> 00:46:56,750
因为现在这又一次减少了我的 IOPS 数量，

867
00:46:56,980 --> 00:46:59,990
我在 IOPS 上浪费的取东西的时间。

868
00:47:01,970 --> 00:47:03,270
事情略微变得，

869
00:47:05,330 --> 00:47:11,550
磁盘和 CPU 速度差距变小，

870
00:47:11,550 --> 00:47:13,910
在某些情况下，磁盘最近变得如此之快，

871
00:47:14,200 --> 00:47:16,010
而你可能不希望东西被压缩。

872
00:47:16,820 --> 00:47:18,320
我们很快就会看到一些其他的好处，

873
00:47:18,320 --> 00:47:19,610
我们保持压缩，

874
00:47:19,610 --> 00:47:21,010
数据库系统可以运行得更快，

875
00:47:21,630 --> 00:47:23,260
当它在内存中处理东西时，

876
00:47:23,460 --> 00:47:25,895
我们将在几周内讨论这个，

877
00:47:25,895 --> 00:47:28,570
但总的来说，对于大多数系统来说，

878
00:47:28,740 --> 00:47:30,820
压缩磁盘上的数据总是一件好事。

879
00:47:33,230 --> 00:47:36,960
所以，我们需要使用的任何压缩方案都必须产生固定的长度值，

880
00:47:37,670 --> 00:47:38,425
正如我们前面所说的，

881
00:47:38,425 --> 00:47:40,200
因为我们希望将其存储在列存储中，

882
00:47:40,790 --> 00:47:43,320
我们希望确保所有集合都是固定长度。

883
00:47:45,500 --> 00:47:46,825
在某些情况下，

884
00:47:46,825 --> 00:47:50,790
我们希望延迟尽可能长的时间，当我们解压缩时，

885
00:47:50,990 --> 00:47:52,170
当我们执行查询时，

886
00:47:52,430 --> 00:47:53,350
我们会再次看到这个，

887
00:47:53,350 --> 00:47:55,080
我们将在讨论查询执行时更多地讨论这个，

888
00:47:55,370 --> 00:47:56,610
但这里的想法是，

889
00:47:56,810 --> 00:47:59,550
如果我有一堆 1 兆字节的字符串，

890
00:48:00,440 --> 00:48:01,990
在我的表里，

891
00:48:02,580 --> 00:48:04,930
但是我可以把它们转换成 32 位整数，

892
00:48:05,100 --> 00:48:08,620
我希望尽可能长地处理 32 位整数，

893
00:48:09,240 --> 00:48:11,225
因为我必须将数据从一个运算符复制到下一个，

894
00:48:11,225 --> 00:48:12,310
当我执行查询时，

895
00:48:12,690 --> 00:48:15,370
或者如果它是通过网络复制的分布式系统，

896
00:48:15,570 --> 00:48:17,300
我希望尽可能长地保持压缩的内容，

897
00:48:17,300 --> 00:48:20,200
只有在我必须显示的时候才将其解压缩，

898
00:48:20,580 --> 00:48:22,300
某些东西需要它被解压，

899
00:48:22,530 --> 00:48:23,950
或者用户需要输出，

900
00:48:24,530 --> 00:48:25,980
JOIN 使这个变得更加困难，

901
00:48:26,390 --> 00:48:27,870
我们稍后将会讨论。

902
00:48:28,720 --> 00:48:32,130
然后，对于我们的数据系统中的任何压缩方案最重要的是，

903
00:48:32,130 --> 00:48:34,250
我们需要确保我们使用的是无损方案，

904
00:48:35,650 --> 00:48:36,560
有人知道这是什么意思吗，

905
00:48:37,610 --> 00:48:38,700
有损对无损。

906
00:48:38,700 --> 00:48:39,200
是的。

907
00:48:41,060 --> 00:48:44,620
正确，当你压缩或解压时，信息不会丢失，

908
00:48:45,570 --> 00:48:49,900
一个有损方案就像是 MP3 MP4 JPEG ，

909
00:48:50,430 --> 00:48:55,300
他们在做一些关于人类如何感知音频数据或视频数据的技巧，

910
00:48:55,590 --> 00:48:57,970
将东西压缩到更小的尺寸，

911
00:48:58,720 --> 00:49:02,070
这意味着如果你有你拍摄的原始图像和原始声音文件，

912
00:49:02,150 --> 00:49:02,905
当你压缩它的时候，

913
00:49:02,905 --> 00:49:06,000
你不会得到相同的值，

914
00:49:06,080 --> 00:49:08,040
当你解压缩它的时候。

915
00:49:09,095 --> 00:49:10,420
我们不想在数据库系统中这样做，

916
00:49:10,470 --> 00:49:15,080
因为，就像你之前说的，人们不会丢失数据，

917
00:49:15,280 --> 00:49:17,925
如果你的银行账户中有 100 美元，

918
00:49:17,925 --> 00:49:19,050
然后他们压缩数据，

919
00:49:19,050 --> 00:49:21,350
当数据被解压缩时，现在你有 90 美元，

920
00:49:21,910 --> 00:49:23,660
你会注意到，你会抱怨，

921
00:49:24,220 --> 00:49:29,190
所以，通常情况下，大多数系统不会使用有损方案，

922
00:49:29,190 --> 00:49:32,390
因为它会有问题。

923
00:49:33,340 --> 00:49:36,805
你自己可以使用有损方案，

924
00:49:36,805 --> 00:49:40,285
所以想想，我的意思是，应用程序可以做这个，

925
00:49:40,285 --> 00:49:45,985
比如如果我每秒都在记录这个房间的温度，

926
00:49:45,985 --> 00:49:47,610
我这样做了十年，

927
00:49:48,140 --> 00:49:50,800
我真的需要知道当时的确切温度吗，

928
00:49:50,800 --> 00:49:53,850
在一年后的某一秒时间，

929
00:49:54,440 --> 00:49:56,095
不，我可以把它压缩到，

930
00:49:56,095 --> 00:49:58,730
这是每分钟的平均温度。

931
00:49:58,840 --> 00:50:00,470
我无法恢复原始数据，

932
00:50:00,880 --> 00:50:03,170
因为它已被压缩或聚合，

933
00:50:03,640 --> 00:50:04,730
这可能没有问题。

934
00:50:05,260 --> 00:50:07,680
但是，这是作为应用程序中的用户，

935
00:50:07,680 --> 00:50:09,530
人必须知道事情是否可以做，

936
00:50:09,640 --> 00:50:10,980
数据库系统做不到，

937
00:50:10,980 --> 00:50:13,430
所以，数据库系统总是使用无损方案。

938
00:50:15,490 --> 00:50:16,400
所以现在的问题是，

939
00:50:17,170 --> 00:50:18,380
我们想要压缩什么，

940
00:50:19,030 --> 00:50:20,630
这里有几个不同的选择。

941
00:50:21,430 --> 00:50:24,110
一是我们可以压缩单个页面或一块数据，

942
00:50:24,880 --> 00:50:27,300
这是一个表上的所有 tuple 。

943
00:50:28,010 --> 00:50:29,965
我们可以单独压缩一个 tuple ，

944
00:50:29,965 --> 00:50:31,590
如果它是行存储系统。

945
00:50:33,290 --> 00:50:34,720
我们甚至可以做更细的粒度，

946
00:50:34,720 --> 00:50:36,970
我们可以说我在一个 tuple 中压缩，

947
00:50:36,970 --> 00:50:39,540
压缩一个属性，

948
00:50:40,360 --> 00:50:43,470
想象我们之前说过的溢出表，

949
00:50:43,850 --> 00:50:47,850
如果你在存储巨大的文本属性，

950
00:50:48,020 --> 00:50:53,040
或在维基百科中，修订可能是大量的文本，

951
00:50:54,530 --> 00:50:56,635
我忘记了维基百科上最大的文章是什么，

952
00:50:56,635 --> 00:50:58,230
它是星球大战的东西，

953
00:50:58,640 --> 00:51:01,800
所以，可能是千字节的文本数据，

954
00:51:02,000 --> 00:51:05,400
我可以为对它进行压缩，那个条目，

955
00:51:05,860 --> 00:51:08,700
Postgres 这样做的，还有一堆其他系统这样做。

956
00:51:09,410 --> 00:51:12,630
或者，我也可以压缩单个列，

957
00:51:13,490 --> 00:51:14,700
如果它是列存储系统。

958
00:51:16,280 --> 00:51:18,330
所以让我们讨论一下如何在块级别上做到这个，

959
00:51:19,820 --> 00:51:22,255
然后我们将花大部分时间来讨论如何在列级别上做到这个，

960
00:51:22,255 --> 00:51:25,440
因为这对于列存储系统来说是最重要的。

961
00:51:27,590 --> 00:51:28,780
所以，要在数据块级别上做到这个，

962
00:51:28,780 --> 00:51:31,170
我们需要使用简单的压缩方案，

963
00:51:31,580 --> 00:51:34,230
我说的简单，我的意思是，

964
00:51:34,730 --> 00:51:40,080
它是数据库系统调用类似于 Gzip 的第三方库，

965
00:51:40,610 --> 00:51:41,935
你不会想要使用它，因为它很慢，

966
00:51:41,935 --> 00:51:43,470
但它是一个第三方库，

967
00:51:43,550 --> 00:51:47,580
它将获取页面，然后将其压缩成某种二进制形式，

968
00:51:47,930 --> 00:51:50,490
数据库系统无法解释，

969
00:51:50,570 --> 00:51:56,220
或自省到块的压缩版本，

970
00:51:56,220 --> 00:52:01,310
所以在一个文件上调用 Gzip ，

971
00:52:01,750 --> 00:52:05,205
数据库系统不知道如何读取压缩文件中的内容，

972
00:52:05,205 --> 00:52:06,120
它必须对其进行解压缩，

973
00:52:06,120 --> 00:52:09,260
才能恢复其原始版本。

974
00:52:10,150 --> 00:52:11,685
你不会想要使用 Gzip ，

975
00:52:11,685 --> 00:52:13,100
有一堆这样更快的替代方案，

976
00:52:14,620 --> 00:52:16,760
它们都来自 LZO ，

977
00:52:17,020 --> 00:52:20,480
是 1990 年代一个巨大的突破，

978
00:52:20,980 --> 00:52:23,900
Zstd 被认为是最先进的压缩方案，

979
00:52:24,460 --> 00:52:25,010
来自 Facebook ，

980
00:52:25,450 --> 00:52:27,740
他们正在开发一个新版本，它还没有公开，

981
00:52:29,630 --> 00:52:32,190
它更快更好，

982
00:52:32,360 --> 00:52:33,210
但这还没有出来，

983
00:52:34,250 --> 00:52:36,390
但是 Zstd 是你要使用的。

984
00:52:37,610 --> 00:52:38,830
所以，让我们来看看 MySQL 是如何做的这个，

985
00:52:40,110 --> 00:52:43,120
所以 MySQL 操作，你可以支持表压缩，

986
00:52:43,410 --> 00:52:45,070
你在每个表的基础上声明它，

987
00:52:45,420 --> 00:52:46,540
我不认为它是默认的，

988
00:52:46,950 --> 00:52:48,040
它的工作方式是，

989
00:52:48,690 --> 00:52:50,780
当你的所有页面被写入磁盘时，

990
00:52:50,780 --> 00:52:56,660
它们将被压缩成页面大小，

991
00:52:56,660 --> 00:53:03,170
是 4 或 2 ，最多 8 千字节，

992
00:53:04,330 --> 00:53:05,295
在每个页面中，

993
00:53:05,295 --> 00:53:07,340
他们将有一个 header 部分，称为 mod log ，

994
00:53:07,780 --> 00:53:10,950
这有点像我之前提到的行存储，

995
00:53:10,950 --> 00:53:14,480
我可以在那里进行一系列写入和对页面进行更改，

996
00:53:14,860 --> 00:53:17,660
而不必先解压缩它，

997
00:53:18,630 --> 00:53:20,000
所以这就像是开始有一点额外的空间。

998
00:53:23,290 --> 00:53:24,200
我也会说，

999
00:53:24,310 --> 00:53:28,610
比如，如果你的页面在压缩后是 6 千字节，

1000
00:53:28,630 --> 00:53:32,750
他们会将其填充到 1，2，4，8 中的下一个最高值，

1001
00:53:33,100 --> 00:53:38,030
这确保你在磁盘上的布局中没有任何碎片，

1002
00:53:39,180 --> 00:53:40,480
当你把东西带入内存时。

1003
00:53:40,980 --> 00:53:43,295
假设我运行了一个查询，

1004
00:53:43,295 --> 00:53:45,550
想要读取页 0 中的内容，

1005
00:53:46,140 --> 00:53:47,650
如果我执行一个盲写操作，

1006
00:53:48,150 --> 00:53:51,710
比如插入、删除甚至更新，

1007
00:53:51,710 --> 00:53:52,630
假设我有这些值，

1008
00:53:53,580 --> 00:53:55,240
我就不需要解压缩页面，

1009
00:53:55,470 --> 00:53:57,520
我只是将更改写入 mod log ，

1010
00:53:57,960 --> 00:53:59,360
它是一个日志结构，

1011
00:53:59,360 --> 00:54:00,160
就像我们之前谈到的那样，

1012
00:54:00,210 --> 00:54:03,160
我说我们会在这学期剩下的时间里看到这个想法，

1013
00:54:03,870 --> 00:54:06,550
你可以认为 mod log 就是我们前面谈到的日志结构存储。

1014
00:54:08,120 --> 00:54:09,420
在某些情况下，

1015
00:54:09,470 --> 00:54:11,040
我也可以对 mod log 进行读取，

1016
00:54:11,480 --> 00:54:15,660
因为如果我需要的数据只是插入到 mod log 中，

1017
00:54:15,890 --> 00:54:18,180
我就不必解压缩页面的其余部分，

1018
00:54:20,170 --> 00:54:21,645
但如果我确实需要读取页面，

1019
00:54:21,645 --> 00:54:23,030
他们会将其解压缩，

1020
00:54:23,530 --> 00:54:28,140
将其作为常规的 16 千字节页面存储在内存中，缓冲池中，

1021
00:54:28,140 --> 00:54:30,050
因为这是 MySQL 的默认大小，

1022
00:54:30,400 --> 00:54:33,420
然后我可以对其执行任何读取操作，

1023
00:54:34,150 --> 00:54:36,110
但我仍然保留压缩版本，

1024
00:54:36,580 --> 00:54:37,620
我也认为，

1025
00:54:37,620 --> 00:54:38,535
当它被解压缩时，

1026
00:54:38,535 --> 00:54:42,600
他们会将对 mod log 的更改应用到那里的页面，

1027
00:54:45,980 --> 00:54:47,370
这是一个好主意，还是一个坏主意？

1028
00:54:50,140 --> 00:54:51,200
Postgres 没有这样做。

1029
00:54:56,770 --> 00:54:57,170
是的。

1030
00:55:00,530 --> 00:55:02,100
他说，读取不是很好，为什么，

1031
00:55:02,900 --> 00:55:09,830
不一定，

1032
00:55:09,830 --> 00:55:12,680
比如回到这里，

1033
00:55:14,480 --> 00:55:15,510
如果我做插入的话，

1034
00:55:16,280 --> 00:55:17,830
然后它落在 mod log 中，

1035
00:55:18,450 --> 00:55:19,540
我不需要解压缩它，

1036
00:55:19,860 --> 00:55:20,620
我的索引，

1037
00:55:21,150 --> 00:55:22,445
比如这里有[记录]表示，

1038
00:55:22,445 --> 00:55:24,275
好的，我现在更新了索引，

1039
00:55:24,275 --> 00:55:25,900
所以 record ID 指向这个页面，

1040
00:55:26,010 --> 00:55:27,455
然后你可以查看 mod log ，

1041
00:55:27,455 --> 00:55:29,810
对于那个插槽号，那个 record ID ，

1042
00:55:29,810 --> 00:55:31,300
它在 mod log 中，那个页面，

1043
00:55:31,770 --> 00:55:32,770
这样你就不必解压缩了。

1044
00:55:36,150 --> 00:55:37,490
我会说，我觉得这是个不错的主意，

1045
00:55:38,200 --> 00:55:39,880
而且，我说， Postgres 没有做这个，

1046
00:55:39,880 --> 00:55:40,470
不是因为，

1047
00:55:40,730 --> 00:55:43,770
哦， Postgres 不是福音，

1048
00:55:43,850 --> 00:55:45,250
Postgres 没有做某事，

1049
00:55:45,250 --> 00:55:47,250
并不意味着你不应该做这件事，

1050
00:55:47,690 --> 00:55:49,530
Postgres 有令人惊叹的前端，

1051
00:55:49,610 --> 00:55:52,190
后端实际上相当糟糕，

1052
00:55:53,320 --> 00:55:56,180
因为很多设计都是 1980 年代的残余物，

1053
00:55:56,440 --> 00:55:58,220
这不是你今天构建现代系统的方式，

1054
00:55:59,040 --> 00:56:03,845
所以，他们不支持对于常规数据页面的压缩，

1055
00:56:03,845 --> 00:56:05,650
只支持 TOAST 表，溢出页面，

1056
00:56:06,180 --> 00:56:09,370
所以，这实际上是一个不错的想法。

1057
00:56:09,660 --> 00:56:11,140
不过，它确实有一些挑战，

1058
00:56:14,760 --> 00:56:16,270
因为 MySQL 是行存储，

1059
00:56:16,740 --> 00:56:19,745
这就是为什么你必须使用简单的压缩方案，

1060
00:56:19,745 --> 00:56:20,980
因为你不能做任何奇妙的事情，

1061
00:56:21,090 --> 00:56:26,020
因为你存储在 tuple 本身或页面本身中的值，

1062
00:56:26,040 --> 00:56:27,155
来自所有不同的属性，

1063
00:56:27,155 --> 00:56:30,215
你不能进行所有的简单压缩，

1064
00:56:30,215 --> 00:56:31,990
我们一会儿就会看到。

1065
00:56:32,790 --> 00:56:37,660
因为我们只是使用，我想他们使用的 Snappy 或 Zstd ，

1066
00:56:38,010 --> 00:56:40,600
因为我们使用的是通用压缩算法，

1067
00:56:40,740 --> 00:56:42,580
数据库系统不知道如何解释

1068
00:56:42,990 --> 00:56:45,700
这些压缩版本、压缩字节的实际含义。

1069
00:56:46,710 --> 00:56:49,180
搅局的是我之前谈到的所有压缩算法，

1070
00:56:49,260 --> 00:56:53,260
它们基本上是字典压缩的一些变体，

1071
00:56:54,180 --> 00:56:57,610
它将为重复的字节序列构建自己的字典，

1072
00:56:58,050 --> 00:57:01,030
但是， MySQL 不知道如何读取那个字典，

1073
00:57:01,500 --> 00:57:02,980
所以它必须解压缩整个东西。

1074
00:57:04,950 --> 00:57:08,675
所以，对于一些工作负载，我认为这是一个好主意，

1075
00:57:08,675 --> 00:57:10,900
我也希望 Postgres 做一些压缩。

1076
00:57:12,920 --> 00:57:16,280
好的，所以，如果我们做 OLAP ，

1077
00:57:17,450 --> 00:57:21,330
理想情况下，我们希望能够直接在压缩数据中运行查询，

1078
00:57:21,650 --> 00:57:23,730
而不必先将其解压缩。

1079
00:57:23,730 --> 00:57:24,885
比如这样的东西，

1080
00:57:24,885 --> 00:57:27,230
有我的工资， DJ 2PL 的工资，

1081
00:57:27,820 --> 00:57:28,970
假设我有一些压缩算法，

1082
00:57:29,080 --> 00:57:30,080
我没有说它是什么，

1083
00:57:30,400 --> 00:57:33,860
然后我有了数据库的压缩形式，

1084
00:57:34,780 --> 00:57:36,090
好的，如果我的查询出现，

1085
00:57:36,230 --> 00:57:37,410
我想获得我的工资，

1086
00:57:37,700 --> 00:57:40,770
我使用了某种魔法转换查询，

1087
00:57:41,900 --> 00:57:45,780
将常量字符串 Andy 转换为压缩形式，

1088
00:57:46,160 --> 00:57:49,820
现在我可以在我的压缩表上进行直接查找，

1089
00:57:50,080 --> 00:57:51,200
使用我的压缩常量，

1090
00:57:51,640 --> 00:57:54,050
而不必在我前进的过程中解压缩每一页，

1091
00:57:56,490 --> 00:57:59,135
现在我要减少我必须做的 I/O 的数量，

1092
00:57:59,135 --> 00:58:01,030
因为我获取和压缩页面，

1093
00:58:01,350 --> 00:58:04,240
我不需要解压缩它们来查找它们。

1094
00:58:08,390 --> 00:58:09,870
这就是我们想要的想法，

1095
00:58:10,130 --> 00:58:13,290
实现这个的最简单方法是在列式系统中。

1096
00:58:16,300 --> 00:58:20,690
所以这只是你可能有的一系列不同压缩算法的简单概述。

1097
00:58:21,780 --> 00:58:24,920
这将是字典压缩，

1098
00:58:24,920 --> 00:58:27,310
字典编码是大多数系统的默认选择，

1099
00:58:28,290 --> 00:58:29,405
但你可以做的是，

1100
00:58:29,405 --> 00:58:35,645
你可能不想使用这些其他方案压缩单个列，

1101
00:58:35,645 --> 00:58:37,450
我们将看到一些它有意义的例子，

1102
00:58:37,650 --> 00:58:40,330
但是在你进行字典编码之后，

1103
00:58:40,680 --> 00:58:44,230
你可以将所有这些其他压缩方案应用于字典本身，

1104
00:58:44,310 --> 00:58:47,590
或者仍然你的字典编码值进行进一步的压缩。

1105
00:58:48,070 --> 00:58:49,500
所以你可以得到一种乘法效果，

1106
00:58:49,500 --> 00:58:50,870
你以一种方式进行压缩，

1107
00:58:51,160 --> 00:58:53,340
然后对压缩的数据运行另一种压缩算法，

1108
00:58:53,340 --> 00:58:54,350
得到更好的压缩，

1109
00:58:54,640 --> 00:58:58,130
而且它仍然是以数据库系统可以简单解释的方式完成，

1110
00:58:58,840 --> 00:59:00,780
这些字节在压缩形式中的实际含义，

1111
00:59:00,980 --> 00:59:02,370
而不必先将其解压缩。

1112
00:59:03,790 --> 00:59:06,540
这就是为什么你希望数据库系统做任何事情，

1113
00:59:06,540 --> 00:59:08,630
而不希望操作系统做任何事情或任何其他人做任何事情，

1114
00:59:09,570 --> 00:59:13,240
因为，因为我们可以进行简单压缩。

1115
00:59:16,270 --> 00:59:18,140
所以让我们在这里做一些快速的例子。

1116
00:59:19,060 --> 00:59:22,010
所以你做的一种方法叫做游程编码， RLE ，

1117
00:59:22,800 --> 00:59:24,555
这里的基本思想是，

1118
00:59:24,555 --> 00:59:31,640
如果我们有一系列连续的值是相同的，

1119
00:59:33,070 --> 00:59:33,980
从字面上看是相同的值，

1120
00:59:34,300 --> 00:59:37,400
不是为每个 tuple 一次又一次地存储那个值，

1121
00:59:37,840 --> 00:59:40,370
而是存储一个压缩的摘要，

1122
00:59:40,780 --> 00:59:43,760
对于这个值，在这个偏移量上，

1123
00:59:44,380 --> 00:59:47,500
这里是它有多少出现。

1124
00:59:48,500 --> 00:59:49,765
那么这种方法非常有效，

1125
00:59:49,765 --> 00:59:52,980
如果你的数据是根据你试图压缩的列进行排序的，

1126
00:59:53,480 --> 00:59:55,860
你不能总是这样做，

1127
00:59:56,300 --> 00:59:58,255
但是，如果你对东西进行排序，

1128
00:59:58,255 --> 01:00:02,640
那么你就可以最大化重复游程。

1129
01:00:03,440 --> 01:00:04,735
假设我有一个单独的表，

1130
01:00:04,735 --> 01:00:05,850
其中有一个 id 字段，

1131
01:00:06,080 --> 01:00:08,380
然后有一个列，说明某人是否死了，

1132
01:00:08,380 --> 01:00:12,990
是或不是，没有 null ，没有可能，

1133
01:00:14,180 --> 01:00:16,260
所以我们可以这样压缩，

1134
01:00:17,250 --> 01:00:18,610
所以一个压缩的形式，

1135
01:00:19,170 --> 01:00:22,330
基本上只需要扫描列，

1136
01:00:22,950 --> 01:00:27,610
找到具有连续相同值的属性或 tuple ，

1137
01:00:28,050 --> 01:00:29,570
然后将其转换为这个三元组，

1138
01:00:29,570 --> 01:00:33,340
它说，这是值，在这个偏移量，这是游程的大小。

1139
01:00:34,450 --> 01:00:35,000
好的。

1140
01:00:36,890 --> 01:00:38,610
现在，如果有一个问题出现，

1141
01:00:38,900 --> 01:00:43,920
比如计算死亡人数与未死亡人数，

1142
01:00:44,270 --> 01:00:47,640
我只需展开 isDead 列，

1143
01:00:49,010 --> 01:00:54,300
并通过对行程长度求和来计算我的聚合，

1144
01:00:55,100 --> 01:00:57,850
然后和那里的值一起。

1145
01:01:01,300 --> 01:01:03,950
事实上，我可以做得更好，

1146
01:01:05,590 --> 01:01:07,130
所以我这里有一个小的部分，

1147
01:01:07,330 --> 01:01:08,690
我有一些人没有死，

1148
01:01:08,770 --> 01:01:10,100
然后又有一些死了，有一些没有死，

1149
01:01:10,540 --> 01:01:15,650
所以我现在有三个三元组，游程大小是 1 ，

1150
01:01:16,660 --> 01:01:18,470
所以在这个例子中，我实际上做得更糟，

1151
01:01:18,490 --> 01:01:20,300
因为我存储的是一个三元组，

1152
01:01:20,770 --> 01:01:24,330
而我本来可以只存储一个值本身，

1153
01:01:25,700 --> 01:01:28,650
所以，如果我根据某人是否死亡来对数据进行排序，

1154
01:01:29,870 --> 01:01:33,750
现在我的压缩列只有两个条目，

1155
01:01:34,835 --> 01:01:36,490
这里是所有的死的人，这里是所有没有死的人，

1156
01:01:37,690 --> 01:01:41,810
这极大地减少了存储的数据量。

1157
01:01:42,570 --> 01:01:43,610
所以，情况可能是这样，

1158
01:01:43,610 --> 01:01:45,620
比如我有，总是认为它是极端的，

1159
01:01:45,620 --> 01:01:46,330
我的例子是，

1160
01:01:46,500 --> 01:01:47,500
我必须把它们放在我的幻灯片上，

1161
01:01:47,940 --> 01:01:50,380
我这里有八九个 tuple ，

1162
01:01:50,610 --> 01:01:52,510
如果我有十亿个 tuple 或十亿个人，

1163
01:01:53,680 --> 01:01:55,400
我现在可以压缩，

1164
01:01:55,450 --> 01:01:58,940
跟踪 10 亿人中谁死了，谁没有死，

1165
01:01:59,170 --> 01:02:01,670
到一小部分字节，

1166
01:02:02,360 --> 01:02:03,430
这将适合在一页上，

1167
01:02:07,310 --> 01:02:09,520
你需要三元组中的长度，

1168
01:02:09,520 --> 01:02:12,600
因为，假设我们总是有固定长度的偏移量，

1169
01:02:12,890 --> 01:02:14,125
这可以让你计算出，

1170
01:02:14,125 --> 01:02:17,440
好的，如果我需要找到单个 tuple ，单一条目，

1171
01:02:17,440 --> 01:02:18,510
他们是否死了，

1172
01:02:18,740 --> 01:02:21,760
允许你做数学还原它，

1173
01:02:21,760 --> 01:02:24,180
说，好的，如果我被解压缩，我会在这个偏移量上，

1174
01:02:25,380 --> 01:02:26,570
这只是一个简单的算术。

1175
01:02:30,350 --> 01:02:32,220
你可以做的另一个压缩方法叫做位压缩，

1176
01:02:32,940 --> 01:02:34,010
这里的想法是，

1177
01:02:35,470 --> 01:02:40,160
人们经常会声明某种类型的属性或列，

1178
01:02:40,870 --> 01:02:43,070
比实际需要的要大，

1179
01:02:44,390 --> 01:02:46,540
假设想法是，

1180
01:02:46,540 --> 01:02:48,690
我有一个列，跟踪某个数字，

1181
01:02:48,980 --> 01:02:51,420
我将它声明为一个整数类型，

1182
01:02:51,950 --> 01:02:54,810
在 SQL 中，这是一个 32 位的整数，

1183
01:02:55,610 --> 01:02:56,470
这意味着，

1184
01:02:56,470 --> 01:02:59,020
即使有一个很小的值，

1185
01:02:59,020 --> 01:03:01,140
我仍然会分配 32 位来存储，

1186
01:03:01,730 --> 01:03:02,760
所以对于这里的这些数字，

1187
01:03:02,930 --> 01:03:03,930
它们都不是很大，

1188
01:03:05,000 --> 01:03:06,625
但我总是会以固定位存储它，

1189
01:03:06,625 --> 01:03:07,560
所以在这种情况下，

1190
01:03:08,360 --> 01:03:11,455
为了存储这八九个数字，八个数字，

1191
01:03:11,455 --> 01:03:12,750
我存储了 256 位，

1192
01:03:13,400 --> 01:03:17,010
但是，唯一重要的是这里位的下部，

1193
01:03:17,870 --> 01:03:22,210
因为，这是我需要的实际数据，

1194
01:03:22,530 --> 01:03:23,530
所有这些其他的东西，

1195
01:03:24,030 --> 01:03:30,640
其他的 24 位都是浪费的空间，

1196
01:03:31,260 --> 01:03:33,040
所以我能做的是，

1197
01:03:34,330 --> 01:03:36,530
即使你将其声明为 32 位整数，

1198
01:03:37,030 --> 01:03:38,870
我也会将其存储为 8 位整数，

1199
01:03:40,170 --> 01:03:44,620
现在，这极大地缩小了四倍的大小，

1200
01:03:46,180 --> 01:03:49,340
所以我能够从 256 位到 64 位，

1201
01:03:50,380 --> 01:03:53,660
你可以用位移位运算符和[]来做一系列的技巧，

1202
01:03:53,950 --> 01:03:55,100
我们在本学期晚些时候讨论，

1203
01:03:55,420 --> 01:03:58,620
现在我进行扫描，

1204
01:03:58,620 --> 01:04:01,010
比如试图找到某个数字的匹配，

1205
01:04:01,630 --> 01:04:03,030
因为这些现在是 8 位整数，

1206
01:04:03,030 --> 01:04:06,590
我可以把它们放到一个单独的 32 位整数中，

1207
01:04:06,910 --> 01:04:08,430
我会在我的系统中跟踪，

1208
01:04:08,430 --> 01:04:11,270
哦，它是在这个偏移量，这些不同的值，

1209
01:04:11,650 --> 01:04:13,550
现在，使用一条 CPU 指令，

1210
01:04:13,870 --> 01:04:16,940
我可以一次操作四个值，

1211
01:04:21,010 --> 01:04:21,750
这有什么问题？

1212
01:04:21,750 --> 01:04:22,070
是的。

1213
01:04:22,630 --> 01:04:27,600
好的，太棒了，谢谢。

1214
01:04:28,190 --> 01:04:29,070
所以他的陈述是，

1215
01:04:29,390 --> 01:04:33,085
如果我有一个数字，不能存储在 8 位中，会发生什么，

1216
01:04:33,085 --> 01:04:34,620
我试图将它们打包成的 8 位中，

1217
01:04:35,270 --> 01:04:38,155
解决这个问题的方法是来自 Amazon Redshift 的技术，

1218
01:04:38,155 --> 01:04:39,300
它被称为主要是编码，

1219
01:04:39,680 --> 01:04:40,530
在那里你可以说，

1220
01:04:41,490 --> 01:04:42,770
这个想法基本上是说，

1221
01:04:42,770 --> 01:04:45,250
我的列中的大多数数据都足够小，

1222
01:04:45,690 --> 01:04:47,320
但在不够小的情况下，

1223
01:04:47,730 --> 01:04:49,010
他们会跟踪这些数据，

1224
01:04:49,010 --> 01:04:51,920
然后将其作为单独的数据存储在字典中，

1225
01:04:51,920 --> 01:04:53,680
再次，我有 32 位比特数字，

1226
01:04:53,970 --> 01:04:55,970
但是我有这个 99999999 ，

1227
01:04:55,970 --> 01:04:56,620
这真的很大，

1228
01:04:57,120 --> 01:05:00,070
所以我仍然会存储它们是 8 位，

1229
01:05:00,570 --> 01:05:03,190
但是我会有一个特殊的标记值，

1230
01:05:03,810 --> 01:05:05,470
假设所有位都设置为 1 ，

1231
01:05:06,180 --> 01:05:08,330
然后我有一个单独的表，

1232
01:05:08,330 --> 01:05:12,220
表示给定的偏移量，以下是原始值应该是什么，

1233
01:05:12,450 --> 01:05:15,250
所以现在，当我沿着这一列扫描时，

1234
01:05:15,510 --> 01:05:17,260
如果我看到我的特殊标记值，

1235
01:05:17,550 --> 01:05:20,320
我知道我应该查看这个偏移表，

1236
01:05:20,550 --> 01:05:22,150
找出实际值应该是什么。

1237
01:05:34,790 --> 01:05:35,590
是的，所以他的说法是，

1238
01:05:35,590 --> 01:05:37,330
你能不能对三元组做一些事情，

1239
01:05:37,330 --> 01:05:40,045
不是说所有的东西都是 8 位的，

1240
01:05:40,045 --> 01:05:44,755
你能说，我有一千个连续的值，

1241
01:05:44,755 --> 01:05:46,020
你存储为 4 位，

1242
01:05:46,280 --> 01:05:49,410
然后我可以把它们存储在 12 位或其他什么地方。

1243
01:05:49,700 --> 01:05:51,480
回到打包的事情，

1244
01:05:51,860 --> 01:05:53,430
因为它们将其分解为行组，

1245
01:05:53,660 --> 01:05:55,740
每个行组可以有自己的压缩方案，

1246
01:05:56,600 --> 01:05:57,840
所以你可以做这样的事情。

1247
01:06:00,000 --> 01:06:01,150
我认为 Parquet 更加，

1248
01:06:01,260 --> 01:06:04,270
Parquet 压缩比 Orc 更激进，更复杂，

1249
01:06:05,160 --> 01:06:05,990
也许是另一种方式，

1250
01:06:05,990 --> 01:06:07,700
其中的一个非常简单，

1251
01:06:07,700 --> 01:06:10,150
另一个确实有一堆你所说的各种技巧。

1252
01:06:12,340 --> 01:06:13,485
好的，所以在这个例子中，

1253
01:06:13,485 --> 01:06:14,930
原始大小是 256 位，

1254
01:06:15,310 --> 01:06:16,910
但如果我进行主要编码，

1255
01:06:18,250 --> 01:06:22,430
我必须为主要的 8 列存储 8x8 位，

1256
01:06:22,780 --> 01:06:25,580
然后假设我只需要 16 位用于 offset ，

1257
01:06:25,780 --> 01:06:27,020
32 位用于 value ，

1258
01:06:27,790 --> 01:06:28,640
对于这个查找表，

1259
01:06:28,750 --> 01:06:29,445
这不是真的，

1260
01:06:29,445 --> 01:06:33,375
因为显然分配更多的用于附加元数据，

1261
01:06:33,375 --> 01:06:35,660
但假设你得到它，它是 112 位，

1262
01:06:37,585 --> 01:06:38,130
所以这是非常好的。

1263
01:06:41,320 --> 01:06:43,100
你使用的另一个技巧称为位图编码，

1264
01:06:43,840 --> 01:06:44,920
这里的想法是，

1265
01:06:44,920 --> 01:06:48,175
如果你有一个基数很低的属性，

1266
01:06:48,175 --> 01:06:50,550
意味着它有少量的唯一值，

1267
01:06:51,640 --> 01:06:57,060
现在，不是存储列中的每个 tuple ，

1268
01:06:57,060 --> 01:06:58,010
而是实际的值，

1269
01:06:58,810 --> 01:07:01,100
我要做的是维护位图，

1270
01:07:01,630 --> 01:07:04,640
其中我为列中可能具有的每个值都有一个位图，

1271
01:07:05,140 --> 01:07:06,410
将位设置为 1 ，

1272
01:07:06,550 --> 01:07:14,190
基于列或属性，偏移量处的 tuple 是否具有该特定值。

1273
01:07:15,390 --> 01:07:18,260
所以有一些数据库系统，

1274
01:07:18,260 --> 01:07:20,930
它们提供的位图索引，基本上提供了相同的东西，

1275
01:07:20,930 --> 01:07:21,970
你仍然保留原来的列，

1276
01:07:22,170 --> 01:07:23,885
但它们维护的位图索引，

1277
01:07:23,885 --> 01:07:25,510
将执行与我们在这里看到的相同的技术，

1278
01:07:26,580 --> 01:07:28,535
有一个系统，有一家公司，

1279
01:07:28,535 --> 01:07:33,170
会在本学期晚些时候来谈论他们的数据库，

1280
01:07:33,170 --> 01:07:34,690
它是 FeatureBase 或 FeatureForm ，

1281
01:07:35,670 --> 01:07:38,800
两个不同的数据库都有 Feature 在名字中，

1282
01:07:39,060 --> 01:07:40,985
其中一个只存储位图索引，

1283
01:07:40,985 --> 01:07:43,600
你不能存储真正的数据库数据。

1284
01:07:45,210 --> 01:07:45,950
所以，这个想法是，

1285
01:07:45,950 --> 01:07:48,580
所以回到我们的 isDead 列，

1286
01:07:49,110 --> 01:07:50,855
这里只有两种可能的值，

1287
01:07:50,855 --> 01:07:51,910
要么死了，要么没死，

1288
01:07:54,060 --> 01:07:56,500
不是存储实际的单个值本身，

1289
01:07:56,790 --> 01:07:57,820
我有两个位图，

1290
01:07:58,410 --> 01:08:01,390
一个人说是，一个人说不，

1291
01:08:02,670 --> 01:08:05,470
然后在每个位图中设置一个位，

1292
01:08:05,490 --> 01:08:09,370
它对应于原始值是否有那个位图，

1293
01:08:09,540 --> 01:08:12,690
它是否有那个特定值，

1294
01:08:12,890 --> 01:08:18,540
所以我现在只需要 2 个 8 位， 16 位来存储是或否，

1295
01:08:18,950 --> 01:08:22,950
然后现在我的位图只有 18 位，

1296
01:08:23,940 --> 01:08:26,050
因为我有 9 个值，每个值需要 2 位，

1297
01:08:26,370 --> 01:08:28,240
所以我现在可以把它降到 34 位。

1298
01:08:32,240 --> 01:08:33,340
这种方法最明显的问题是什么？

1299
01:08:35,400 --> 01:08:35,800
是的。

1300
01:08:38,310 --> 01:08:40,070
他说，如果你的数据是高基数的，

1301
01:08:40,070 --> 01:08:41,555
这确实是一个糟糕的想法，

1302
01:08:41,555 --> 01:08:42,280
是的，它是。

1303
01:08:42,660 --> 01:08:43,450
让我们来看一个例子，

1304
01:08:44,300 --> 01:08:45,870
假设我们有一个这样的 customer 表，

1305
01:08:46,310 --> 01:08:48,790
我们有一个 zip_code 列，

1306
01:08:49,170 --> 01:08:51,490
美国有多少个邮政编码，猜猜看。

1307
01:08:53,210 --> 01:08:56,310
我听到 10000 ，更多， 100000 ，更少，

1308
01:08:56,900 --> 01:08:57,930
我们在做的是二分查找，

1309
01:08:59,510 --> 01:09:01,500
是 43000 ，

1310
01:09:02,180 --> 01:09:02,830
假设我们有一个表，

1311
01:09:02,830 --> 01:09:04,020
我们有 1000 万行，

1312
01:09:04,400 --> 01:09:08,970
我将为我拥有的每个唯一的邮政编码构建一个位图，

1313
01:09:09,470 --> 01:09:11,010
好的，我需要，

1314
01:09:13,410 --> 01:09:14,360
只是为了存储数据，

1315
01:09:14,360 --> 01:09:17,680
假设原始数据，邮政编码是 32 位，

1316
01:09:18,090 --> 01:09:19,810
原始数据是 40 兆字节，

1317
01:09:19,890 --> 01:09:26,200
但如果我必须为每个邮政编码都有一个 1000 万大小的位图，

1318
01:09:26,550 --> 01:09:28,420
现在我们是 53G ，

1319
01:09:29,520 --> 01:09:31,270
所以很明显，这不是一个好主意。

1320
01:09:31,890 --> 01:09:36,520
此外，每次有人添加新的 tuple 时，

1321
01:09:37,260 --> 01:09:38,860
我都必须扩展该位图，

1322
01:09:39,150 --> 01:09:40,955
因为它也必须匹配，

1323
01:09:40,955 --> 01:09:42,500
我一直在添加更多在上面，

1324
01:09:42,500 --> 01:09:45,120
所以，我必须为每一种可能的位图这样做。

1325
01:09:46,160 --> 01:09:47,910
所以，位图索引可以产生巨大的差异，

1326
01:09:47,960 --> 01:09:49,200
但这实际上是为了，

1327
01:09:49,460 --> 01:09:51,355
当你有一个非常小的基数时，

1328
01:09:51,355 --> 01:09:52,500
比如，可能小于 10 ，

1329
01:09:53,680 --> 01:09:54,560
你想这么做，

1330
01:09:56,210 --> 01:09:58,740
大多数系统在默认情况下不会这样做。

1331
01:10:01,100 --> 01:10:02,440
好的，增量编码，

1332
01:10:02,440 --> 01:10:03,505
这里的想法是，

1333
01:10:03,505 --> 01:10:06,450
如果值从一个属性到下一个，

1334
01:10:06,770 --> 01:10:07,980
抱歉，从一个 tuple 到下一个，

1335
01:10:09,050 --> 01:10:10,620
如果它们彼此足够接近，

1336
01:10:11,320 --> 01:10:16,110
也许，我不需要存储 tuple 的整个值，

1337
01:10:16,190 --> 01:10:19,650
我只需要存储前一个值之间的差值。

1338
01:10:20,670 --> 01:10:23,060
假设，我有一种传感器读数，

1339
01:10:23,060 --> 01:10:24,700
我在记录房间里的温度，

1340
01:10:24,930 --> 01:10:29,050
每一分钟我都在存储温度，

1341
01:10:29,670 --> 01:10:31,240
所以这里是时间戳列，

1342
01:10:31,530 --> 01:10:33,130
假设我们存储的是 64 位，

1343
01:10:35,470 --> 01:10:38,840
我们知道时间总是以 1 递增，

1344
01:10:39,100 --> 01:10:42,800
而且，假设我能跟踪室内或室外的温度，

1345
01:10:43,450 --> 01:10:47,885
从一分钟到下一分钟，不会有戏剧性的温度波动，

1346
01:10:47,885 --> 01:10:52,480
我们不会在一分钟内从华氏 99 度降到 0 度，

1347
01:10:53,670 --> 01:10:55,265
所以我现在能做的就是，

1348
01:10:55,265 --> 01:10:58,090
存储从一个 tuple 到下一个，

1349
01:10:58,320 --> 01:11:02,290
与前一次有什么不同，

1350
01:11:02,730 --> 01:11:04,655
所以，在时间戳的情况下，

1351
01:11:04,655 --> 01:11:06,280
它只是加 1 ，加一分钟，

1352
01:11:07,350 --> 01:11:08,770
在温度的情况下，

1353
01:11:08,770 --> 01:11:11,980
它是与前一个的微小分数差异，

1354
01:11:14,210 --> 01:11:15,660
我现在进一步压缩它，

1355
01:11:15,710 --> 01:11:18,570
因为我在时间戳的第一列中有什么，

1356
01:11:18,650 --> 01:11:19,350
我有什么，

1357
01:11:20,300 --> 01:11:21,270
一大堆加 1 ，

1358
01:11:21,800 --> 01:11:22,740
我们怎么才能压缩它，

1359
01:11:25,360 --> 01:11:26,780
游程编码，

1360
01:11:27,660 --> 01:11:29,120
所以我们现在可以进一步压缩它，

1361
01:11:29,380 --> 01:11:30,560
并将其转换为，

1362
01:11:33,230 --> 01:11:38,305
使用游程编码转换增量编码，

1363
01:11:38,305 --> 01:11:40,290
来告诉你之后有多少加 1 。

1364
01:11:43,480 --> 01:11:44,445
所以这是一个很好的例子，

1365
01:11:44,445 --> 01:11:45,855
我们可以有乘数效应，

1366
01:11:45,855 --> 01:11:48,290
我们可以进一步压缩压缩的数据，

1367
01:11:48,700 --> 01:11:53,060
因为我们把它变成了一种可以利用它的形式。

1368
01:11:54,200 --> 01:11:56,310
所以，如果回到我们的原始数据大小，

1369
01:11:58,140 --> 01:11:59,870
仅就时间戳列本身而言，

1370
01:12:00,160 --> 01:12:01,430
我们需要 320 位，

1371
01:12:01,900 --> 01:12:05,390
但如果我们先进行增量编码，然后进行游程编码，

1372
01:12:05,470 --> 01:12:06,860
我们可以将其降至 96 位，

1373
01:12:08,020 --> 01:12:10,020
我在这里展示了六、七个 tuple ，

1374
01:12:10,020 --> 01:12:10,710
它没那么大，

1375
01:12:10,710 --> 01:12:11,990
但是，把它想象成极端的，

1376
01:12:12,400 --> 01:12:14,270
想象一下，大概有 10 亿个记录，

1377
01:12:15,160 --> 01:12:16,280
这将是一笔巨大的节省。

1378
01:12:19,940 --> 01:12:21,840
最后讨论的是字典压缩，

1379
01:12:21,860 --> 01:12:23,695
因为我说过，这是最常见的一种，

1380
01:12:23,695 --> 01:12:24,690
这就是我们将如何获得，

1381
01:12:25,160 --> 01:12:27,360
这是大多数系统将如何表达数据，

1382
01:12:27,770 --> 01:12:30,150
即使是对于不是字符串的东西，

1383
01:12:31,100 --> 01:12:35,580
在某些情况下，有一些列式系统会将整数数据或流数据压缩，

1384
01:12:35,750 --> 01:12:37,980
将它们放到字典编码中。

1385
01:12:39,590 --> 01:12:40,510
这里的想法是，

1386
01:12:40,510 --> 01:12:44,010
如果我们有反复看到的值，

1387
01:12:44,630 --> 01:12:47,340
不是在一列中重复存储该值，

1388
01:12:47,660 --> 01:12:51,300
我们将把它转换为一些 32 位整数，

1389
01:12:52,040 --> 01:12:55,410
然后我们维护一个映射数据结构，即字典，

1390
01:12:55,580 --> 01:12:59,100
它知道如何获取它的字典编码，32 位整数，

1391
01:12:59,210 --> 01:13:01,770
并将其转换回原始值，

1392
01:13:03,420 --> 01:13:05,470
通常我们会有一对一的对应，

1393
01:13:05,490 --> 01:13:07,660
对于一个值，我们将有一个字典编码，

1394
01:13:07,950 --> 01:13:09,490
这里有一些技巧，

1395
01:13:09,720 --> 01:13:11,290
我认为任何商业系统都不会这样做，

1396
01:13:11,400 --> 01:13:16,090
你可以说，如果我看到多个属性，模式在一起，

1397
01:13:16,260 --> 01:13:20,200
我会将它们两个或三个的组合转换为单个字典代码，

1398
01:13:20,460 --> 01:13:21,520
以获得进一步的压缩，

1399
01:13:21,990 --> 01:13:25,420
但是，我只在学术文献中看到过这个。

1400
01:13:27,030 --> 01:13:30,460
然后，我们需要一种快速编码和解码的方法，

1401
01:13:30,870 --> 01:13:34,060
允许同时进行范围和点查询，

1402
01:13:34,530 --> 01:13:35,705
所以点查询是显而易见的，

1403
01:13:35,705 --> 01:13:36,880
就像我想要说的那样，

1404
01:13:36,960 --> 01:13:40,600
字符串 Andy 映射到的编码 101 ，

1405
01:13:40,860 --> 01:13:42,400
我知道如何做精确的查找，

1406
01:13:43,050 --> 01:13:47,140
但理想情况下，我希望能够对压缩数据进行范围查询，

1407
01:13:47,720 --> 01:13:53,770
所以，我希望我的字典编码也具有与原始值相同的顺序，

1408
01:13:54,640 --> 01:13:56,000
我们稍后会看看如何做到这个。

1409
01:13:57,600 --> 01:13:58,720
假设这是我的原始数据，

1410
01:13:58,980 --> 01:14:00,730
一堆我以前学生的名字，

1411
01:14:02,640 --> 01:14:05,200
然后，它的压缩版本可以是，

1412
01:14:05,580 --> 01:14:10,480
我有我的原始列，把它们转换成 32 位整数，

1413
01:14:10,680 --> 01:14:12,820
然后我就有了这个映射表，

1414
01:14:13,140 --> 01:14:15,840
转换，允许我查找，

1415
01:14:15,840 --> 01:14:17,600
对于给定的编码，原始值是什么，

1416
01:14:17,770 --> 01:14:20,540
或者给定的原始值，代码是什么，

1417
01:14:20,710 --> 01:14:21,710
这就是字典。

1418
01:14:22,820 --> 01:14:24,010
所以现在我们可以回到我的例子，

1419
01:14:24,010 --> 01:14:26,400
我在最开始的例子，有我和 DJ 2PL ，

1420
01:14:26,660 --> 01:14:29,040
SELECT * FROM user WHERE name = "Andy" ，

1421
01:14:29,690 --> 01:14:34,855
我可以把字符串 Andy 转换成字典编码，

1422
01:14:34,855 --> 01:14:36,780
通过首先在字典中进行查找，

1423
01:14:37,660 --> 01:14:40,380
然后，现在我浏览我的列，

1424
01:14:40,380 --> 01:14:44,300
只进行查找或基于整数进行比较，

1425
01:14:44,590 --> 01:14:46,730
所以我不需要在扫描的时候通过，

1426
01:14:47,200 --> 01:14:49,610
如果我不压缩我的常量，

1427
01:14:49,900 --> 01:14:50,880
那么当我扫描的时候，

1428
01:14:50,880 --> 01:14:53,240
我必须一个一个地解压缩，

1429
01:14:53,410 --> 01:14:54,360
然后再进行查找，

1430
01:14:54,360 --> 01:14:58,270
我基本上失去了所有压缩的好处，

1431
01:14:58,380 --> 01:14:59,720
这就是 MySQL 必须做的，

1432
01:14:59,720 --> 01:15:01,805
因为它们无法解释词典中的实际内容，

1433
01:15:01,805 --> 01:15:04,030
它们无法解释压缩字节的实际含义，

1434
01:15:04,410 --> 01:15:05,320
但在这里，

1435
01:15:05,580 --> 01:15:06,760
因为我们是数据库系统，

1436
01:15:07,020 --> 01:15:09,110
我们建立了字典，我们控制它，

1437
01:15:09,110 --> 01:15:10,660
我们知道如何阅读和解释它，

1438
01:15:10,860 --> 01:15:13,690
我们可以，我的意思是，在 SQL 中，我们知道查询想要做什么，

1439
01:15:13,800 --> 01:15:15,130
我们知道如何获取该常量，

1440
01:15:15,750 --> 01:15:16,870
将其转换为字典代码，

1441
01:15:17,280 --> 01:15:19,510
然后直接对压缩数据进行扫描。

1442
01:15:23,790 --> 01:15:26,240
所以，我们到底要怎么做，

1443
01:15:26,500 --> 01:15:28,460
做编码和解码，

1444
01:15:28,870 --> 01:15:31,635
对于一个给定的未压缩的值，

1445
01:15:31,635 --> 01:15:34,550
我们知道有一种方法到压缩形式，然后反转它，

1446
01:15:35,020 --> 01:15:36,180
所以你要指出的关键是，

1447
01:15:36,180 --> 01:15:39,690
不会有一个神奇的散列函数可以为我们做到这个，

1448
01:15:42,700 --> 01:15:46,365
任何可逆的散列函数都会，

1449
01:15:46,365 --> 01:15:49,005
产生比原始值大得多的东西，

1450
01:15:49,005 --> 01:15:52,550
所以不会把它缩小到 32 位整数，

1451
01:15:53,080 --> 01:15:54,620
所以，我们必须建立一个数据结构，

1452
01:15:54,640 --> 01:15:57,500
我们维护，使我们能够做到这个。

1453
01:15:58,600 --> 01:15:59,430
就像我说的，

1454
01:15:59,430 --> 01:16:02,630
我们想要的是保持原始值的顺序，

1455
01:16:02,980 --> 01:16:05,810
这样压缩数据，压缩的字典代码，

1456
01:16:06,280 --> 01:16:12,680
这些东西在词法上或与原始数据具有相同的顺序。

1457
01:16:14,350 --> 01:16:15,825
所以回到这里，

1458
01:16:15,825 --> 01:16:18,020
我有一堆名字，

1459
01:16:18,610 --> 01:16:26,655
我想要我生成的字典到代码有，

1460
01:16:26,655 --> 01:16:32,370
如果一个原始值在另一个原始值的顺序之前，

1461
01:16:32,510 --> 01:16:34,800
字典编码也应该在它之前，

1462
01:16:36,280 --> 01:16:39,110
所以我会让我的字典是排序的，

1463
01:16:39,430 --> 01:16:41,480
现在，这允许我执行这样的查询，

1464
01:16:42,010 --> 01:16:44,150
SELECT * FROM users WHERE name LIKE "And%" ，

1465
01:16:44,290 --> 01:16:46,490
And ，后面跟着通配符，

1466
01:16:47,120 --> 01:16:49,720
所以，如果我们直接对压缩数据进行操作，

1467
01:16:50,340 --> 01:16:54,460
我们可以将这个 LIKE 子句转换为 BETWEEN 子句，

1468
01:16:55,630 --> 01:16:57,260
因为我们可以在字典中查找，

1469
01:16:58,120 --> 01:17:01,670
运行 LIKE 部分，在字典值上，

1470
01:17:02,230 --> 01:17:08,570
找到匹配的，找到匹配值的最小值和最大值，

1471
01:17:08,920 --> 01:17:11,330
然后将 LIKE 重写为 BETWEEN 调用，

1472
01:17:12,100 --> 01:17:15,180
然后在我的列还在压缩的时候，运行它。

1473
01:17:17,160 --> 01:17:18,125
我们可以这样做，

1474
01:17:18,125 --> 01:17:19,430
因为它是 SQL ，

1475
01:17:19,430 --> 01:17:21,370
我们知道在 WHERE 子句中，

1476
01:17:21,540 --> 01:17:23,710
它不是任意的 Python 代码或 C 代码，

1477
01:17:23,760 --> 01:17:26,380
我们确切地知道 WHERE 子句想要做什么，

1478
01:17:26,730 --> 01:17:28,870
我们可以聪明地转换这个，

1479
01:17:31,050 --> 01:17:32,170
为我们做重写。

1480
01:17:33,050 --> 01:17:36,715
你们作为应用程序程序员，

1481
01:17:36,715 --> 01:17:38,190
或者不是你们，是一些 Javascript 程序员，

1482
01:17:38,540 --> 01:17:41,590
他们不需要知道下面到底发生了什么，

1483
01:17:41,590 --> 01:17:42,745
他们只需编写 LIKE 子句，

1484
01:17:42,745 --> 01:17:45,820
数据库系统就可以变得智能，并为你重写它，

1485
01:17:45,820 --> 01:17:46,650
从而获得更好的性能。

1486
01:17:49,970 --> 01:17:53,005
所以，在这里的某些情况下，

1487
01:17:53,005 --> 01:17:53,910
你仍然必须做，

1488
01:17:55,790 --> 01:17:56,320
问题是，

1489
01:17:56,320 --> 01:17:58,560
你是否仍然执行原始列，

1490
01:17:58,700 --> 01:17:59,470
在这种情况下，

1491
01:17:59,470 --> 01:18:06,210
因为我需要 name 属性的输出，

1492
01:18:06,530 --> 01:18:08,310
我仍然要去[撕开]列，

1493
01:18:08,990 --> 01:18:10,470
真正地查看它们，

1494
01:18:10,880 --> 01:18:13,950
在某些情况下，数据库系统甚至可以更智能，

1495
01:18:14,420 --> 01:18:17,940
它可以在查看压缩数据的情况下回答查询，

1496
01:18:18,670 --> 01:18:21,800
而是直接在字典上操作。

1497
01:18:23,100 --> 01:18:25,300
所以，如果不是 SELECT name FROM users ，

1498
01:18:25,590 --> 01:18:27,250
而是 DISTINCT name FROM users ，

1499
01:18:28,220 --> 01:18:30,185
我不需要获得实际的 tuple 本身，

1500
01:18:30,185 --> 01:18:32,620
我只需要获得唯一的实际值，

1501
01:18:33,360 --> 01:18:35,230
然后，对于这里的查询，

1502
01:18:35,430 --> 01:18:40,660
在转换为 BETWEEN 之后，

1503
01:18:40,830 --> 01:18:45,520
或者将这里的通配符转换为字典值之后，

1504
01:18:45,990 --> 01:18:49,180
我只需要知道字典中实际存在什么值，

1505
01:18:49,760 --> 01:18:51,430
我不需要查看实际列，

1506
01:18:51,750 --> 01:18:54,130
对于使用 DISTINCT 的查询，

1507
01:18:54,690 --> 01:18:58,450
假设我的表中只有四个名字，

1508
01:18:58,590 --> 01:18:59,980
但我有十亿行，

1509
01:19:00,210 --> 01:19:03,820
我只需要查看字典中的四行就可以回答它，

1510
01:19:05,000 --> 01:19:06,900
再次，我已经说过很多次了，

1511
01:19:06,950 --> 01:19:07,860
我们可以这样做，

1512
01:19:07,880 --> 01:19:12,420
因为数据库负责压缩这个，

1513
01:19:14,040 --> 01:19:17,110
现在， Parquet 或者说，他们有一个很大的限制是，

1514
01:19:17,250 --> 01:19:19,450
他们不会向你公开字典，

1515
01:19:20,190 --> 01:19:21,940
当你使用他们的库和程序时，

1516
01:19:22,500 --> 01:19:24,170
所以你不能做我在这里说的这个技巧，

1517
01:19:24,170 --> 01:19:25,930
如果你使用 Parquet 或 Orc ，

1518
01:19:26,190 --> 01:19:27,755
Parquet 和 Orc 解压缩数据，

1519
01:19:27,755 --> 01:19:28,640
当它把数据还给你时，

1520
01:19:28,640 --> 01:19:30,460
你不能直接对压缩数据进行操作，

1521
01:19:31,200 --> 01:19:34,960
在我看来，这是这两种格式最大的限制之一，

1522
01:19:36,330 --> 01:19:40,600
但是，其他不使用 Parquet Orc ，执行简单压缩的系统可以做这个技巧。

1523
01:19:42,430 --> 01:19:44,120
好的，那么这个数据结构是什么，

1524
01:19:45,840 --> 01:19:47,630
我们的字典将使用什么数据结构，

1525
01:19:48,160 --> 01:19:50,540
最常见的方法将是非常简单的数组，

1526
01:19:51,200 --> 01:19:53,120
如果文件是不变的，这很有效，

1527
01:19:53,120 --> 01:19:54,125
因为我只构建一次数组，

1528
01:19:54,125 --> 01:19:55,450
并且我永远不会调整某些东西的大小，

1529
01:19:55,710 --> 01:19:57,605
原地插入某些东西，

1530
01:19:57,605 --> 01:19:59,470
也不会移动东西，

1531
01:19:59,610 --> 01:20:01,180
我只需在完成后就可以构建它，

1532
01:20:01,830 --> 01:20:04,060
如果你需要一些动态的，并且可以支持更新的东西，

1533
01:20:04,290 --> 01:20:06,490
你需要一个哈希表或者一个 B+ 树，

1534
01:20:06,720 --> 01:20:09,790
这些是，哈希表，我想不那么常见，

1535
01:20:10,700 --> 01:20:12,190
事实上，这些东西不太常见，

1536
01:20:12,190 --> 01:20:13,230
大多数人做数组，

1537
01:20:13,340 --> 01:20:18,060
并假设压缩的块将是不可变的，

1538
01:20:18,080 --> 01:20:21,160
并且只有当我需要重建它的时候，

1539
01:20:21,160 --> 01:20:22,170
然后我会重建数组。

1540
01:20:22,190 --> 01:20:23,095
我注意到我超时了，

1541
01:20:23,095 --> 01:20:24,990
让我粗略地展示一下它的样子，

1542
01:20:25,790 --> 01:20:27,720
所以，基本上你的列中有原始数据，

1543
01:20:28,400 --> 01:20:30,990
所以你需要做的第一件事就是建立你的字典，

1544
01:20:31,160 --> 01:20:36,180
然后，所有要做的就是你拥有的值的排序列表，

1545
01:20:36,620 --> 01:20:42,960
然后，你存储字符串的长度，

1546
01:20:43,430 --> 01:20:49,020
然后，字典代码就是这个数组的偏移量，

1547
01:20:49,640 --> 01:20:51,700
所以我的压缩数据是这样的，

1548
01:20:52,320 --> 01:20:55,600
这些只是数组中的字节偏移量，

1549
01:20:56,420 --> 01:20:58,120
所以现在当我做扫描时，

1550
01:20:58,170 --> 01:21:05,590
我想说，如果我有第二个条目，它是 17 ，

1551
01:21:06,000 --> 01:21:08,290
我跳跃到 17 的偏移量，

1552
01:21:08,760 --> 01:21:10,030
我可以在里面查看，

1553
01:21:11,100 --> 01:21:11,565
在这里，

1554
01:21:11,565 --> 01:21:14,480
我可以查看 header ，然后知道后面的字符串有多大。

1555
01:21:15,500 --> 01:21:18,300
所以，字典本身就是一个由这样的字节组成的数组。

1556
01:21:20,150 --> 01:21:20,550
好的？

1557
01:21:22,220 --> 01:21:23,910
好的，最后，

1558
01:21:26,150 --> 01:21:28,180
这个行存储、列存储非常重要，

1559
01:21:28,180 --> 01:21:31,230
我们会在讨论查询执行和其他事情时看到这个，

1560
01:21:32,930 --> 01:21:34,440
大多数在期中之前，

1561
01:21:34,610 --> 01:21:38,340
因为行存储和列存储系统之间的区别

1562
01:21:39,140 --> 01:21:42,450
影响到数据库系统的所有其他部分，

1563
01:21:42,770 --> 01:21:43,600
如何进行恢复，

1564
01:21:43,600 --> 01:21:44,490
如何进行查询执行，

1565
01:21:44,540 --> 01:21:45,690
如何运行事务，

1566
01:21:46,670 --> 01:21:48,060
如何优化查询，

1567
01:21:48,290 --> 01:21:49,740
所以现在理解这一点非常重要，

1568
01:21:50,060 --> 01:21:52,860
我们将看到这两种方法之间的权衡，

1569
01:21:52,940 --> 01:21:55,020
在整个学期中一次又一次地看到。

1570
01:21:55,820 --> 01:21:58,285
然后大多数数据系统要获得最佳的压缩比，

1571
01:21:58,285 --> 01:21:59,970
你想要对自己进行简单压缩，

1572
01:22:00,710 --> 01:22:02,490
而字典编码是常见的一种。

1573
01:22:03,230 --> 01:22:06,370
我在过去三节课中展示了这个，

1574
01:22:06,870 --> 01:22:08,320
在数据库存储中存在两个问题，

1575
01:22:08,550 --> 01:22:10,420
首先，我们如何在磁盘上表示数据，

1576
01:22:10,470 --> 01:22:11,710
到目前为止，我们已经介绍了这个，

1577
01:22:12,390 --> 01:22:14,230
所以，从下周开始，

1578
01:22:14,640 --> 01:22:17,020
当我们将事物带入内存时，

1579
01:22:17,610 --> 01:22:18,470
我们将如何处理它，

1580
01:22:18,470 --> 01:22:19,325
我们如何存储它，

1581
01:22:19,325 --> 01:22:20,800
如何安全地写回内容，

1582
01:22:21,300 --> 01:22:22,420
当我们进行更改时。

1583
01:22:24,220 --> 01:22:44,475
好的，开始。

