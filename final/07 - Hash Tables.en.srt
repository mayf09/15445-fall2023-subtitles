1
00:00:42,640 --> 00:00:44,750
She was trying to find you where.

2
00:00:50,800 --> 00:00:51,480
So real quick,

3
00:00:51,480 --> 00:00:54,450
actually, the facilities people actually just came to me

4
00:00:54,450 --> 00:00:58,610
and they said that the governor is here, Shapiro,

5
00:00:59,800 --> 00:01:01,070
I support him, he's not a Trump supporter,

6
00:01:01,900 --> 00:01:03,860
but the main entrance is blocked off,

7
00:01:04,000 --> 00:01:04,995
so when the class is over,

8
00:01:04,995 --> 00:01:06,180
you can't go through that entrance,

9
00:01:06,180 --> 00:01:07,700
you got to take the elevator and go upstairs,

10
00:01:09,090 --> 00:01:11,230
that's why all the fedies and the cops are out there.

11
00:01:11,430 --> 00:01:16,480
All right, so again for the class,

12
00:01:16,920 --> 00:01:20,050
again, we had the recitation on Monday,

13
00:01:20,160 --> 00:01:21,850
that's been posted on piazza as a video,

14
00:01:23,040 --> 00:01:26,470
project #1 is still due on October 7th, October 2nd,

15
00:01:26,850 --> 00:01:29,650
and then we will have again the special office hours on Saturday on October 1st,

16
00:01:29,910 --> 00:01:33,245
and then homework #1 has been bumped to be due on October 4th,

17
00:01:33,245 --> 00:01:34,540
and that's a Wednesday, not a Sunday.

18
00:01:37,210 --> 00:01:39,620
Any questions about the homework and the projects?

19
00:01:44,180 --> 00:01:47,730
All right, so it's, it's due on the, it's due on the Sunday

20
00:01:48,260 --> 00:01:50,760
and then the office hours are on the Saturday,

21
00:01:50,870 --> 00:01:53,430
so whatever those real dates are?

22
00:01:53,840 --> 00:01:54,240
Yes.

23
00:01:56,160 --> 00:01:59,550
The website and GradeScope should be correct, I'm not.

24
00:02:00,370 --> 00:02:00,950
Other questions?

25
00:02:02,850 --> 00:02:03,520
All right, cool.

26
00:02:03,600 --> 00:02:07,910
So then two sort of, one sort of fun thing to bring up,

27
00:02:07,910 --> 00:02:10,930
someone said hey, what about internships, are these companies hiring,

28
00:02:11,250 --> 00:02:12,070
the answer is yes,

29
00:02:12,780 --> 00:02:15,940
and actually somebody posted out on Twitter,

30
00:02:17,250 --> 00:02:20,950
that you know if you take my class they're hiring,

31
00:02:22,230 --> 00:02:26,315
spacetime_db is a I I think it's time, another time series database system,

32
00:02:26,315 --> 00:02:27,310
I think it's at Europe,

33
00:02:27,900 --> 00:02:28,780
I don't know this dude,

34
00:02:29,100 --> 00:02:30,700
you can contact him if you want,

35
00:02:31,170 --> 00:02:32,860
but we'll post on Piazza,

36
00:02:33,810 --> 00:02:37,505
how you know how, how you can give me your your CV

37
00:02:37,505 --> 00:02:40,340
and then we can send it to the various database companies that we know

38
00:02:40,340 --> 00:02:41,140
and that are friends with us,

39
00:02:41,250 --> 00:02:43,630
and again if you haven't yet, please apply to SingleStore

40
00:02:44,010 --> 00:02:46,330
and there's that special email address that's just for CMU students

41
00:02:47,070 --> 00:02:49,960
and that'll go directly to the hiring people and not the recruiters.

42
00:02:51,210 --> 00:02:52,660
So if you don't want to do an internship,

43
00:02:53,250 --> 00:02:55,420
another way to make money through databases is that,

44
00:02:55,710 --> 00:02:58,330
somebody actually posted an upwork and this is real,

45
00:02:58,620 --> 00:03:02,620
that they're looking for someone to basically design database projects,

46
00:03:02,790 --> 00:03:06,130
that are basically ButTub and the class projects,

47
00:03:07,260 --> 00:03:08,470
so if you like the stuff you're doing,

48
00:03:08,640 --> 00:03:10,450
you can get paid a hundred dollars by this guy

49
00:03:12,120 --> 00:03:13,840
to go re-implement that.

50
00:03:15,060 --> 00:03:16,355
And the way we found this was

51
00:03:16,355 --> 00:03:19,265
somebody actually emailed Chi the TA and me,

52
00:03:19,265 --> 00:03:20,960
like, hey, I can do this job for you

53
00:03:20,960 --> 00:03:22,210
and he's like what are you talking about,

54
00:03:22,470 --> 00:03:23,890
because they thought we posted this,

55
00:03:24,000 --> 00:03:25,450
this is not us, this is [],

56
00:03:27,150 --> 00:03:28,450
hundred dollars not enough,

57
00:03:31,500 --> 00:03:33,735
hundred dollars an hour, in databases,

58
00:03:33,735 --> 00:03:34,430
if not more.

59
00:03:36,620 --> 00:03:39,750
All right, so where we at in the class,

60
00:03:40,940 --> 00:03:45,510
we, we spent the last week or so talking again about the storage layer

61
00:03:45,740 --> 00:03:47,815
and then putting the buffer pool on top of it

62
00:03:47,815 --> 00:03:51,180
to actually manage memory as we get pages in and out of of disk,

63
00:03:51,380 --> 00:03:53,850
and so now we're continuing up the stack

64
00:03:54,260 --> 00:03:56,790
and are now going to talk about different parts of the system,

65
00:03:57,050 --> 00:04:01,080
that can operate and execute and process those pages,

66
00:04:01,130 --> 00:04:04,020
that we've brought into our buffer pool that we retrieved from disk.

67
00:04:04,440 --> 00:04:06,920
And so we're sort of in this middle layer here, in the Access Methods,

68
00:04:07,300 --> 00:04:08,445
and now we're going start talking about

69
00:04:08,445 --> 00:04:10,340
how do we construct the execution engine,

70
00:04:10,630 --> 00:04:13,760
that's going to be responsible for executing these queries.

71
00:04:14,230 --> 00:04:19,130
And so the access method is going to be the mechanisms for accessing the data,

72
00:04:19,660 --> 00:04:23,270
and it could either be through an index or through the tables themselves

73
00:04:23,530 --> 00:04:25,130
and potentially other mechanisms.

74
00:04:25,870 --> 00:04:27,840
So to do that, we need to talk about

75
00:04:27,840 --> 00:04:32,120
what kind of data structures we would have, this part of the system,

76
00:04:33,520 --> 00:04:35,300
and so this class will be on hash tables,

77
00:04:35,650 --> 00:04:38,210
which is an unordered data structure,

78
00:04:38,530 --> 00:04:42,050
and then we'll spend all of next week talking about tree data structures,

79
00:04:42,460 --> 00:04:45,740
which will give you ordering, give you ordering your own keys.

80
00:04:46,490 --> 00:04:48,240
And so we're just slowly building up,

81
00:04:48,560 --> 00:04:49,450
making our way to the top

82
00:04:49,450 --> 00:04:52,170
to actually produce results for our queries.

83
00:04:54,160 --> 00:04:56,875
So, I mean, it goes with sort out saying,

84
00:04:56,875 --> 00:04:59,370
I'm assuming everyone here is taken a data structure class or algorithms class,

85
00:04:59,600 --> 00:05:02,280
data structures are going to be used all throughout the system,

86
00:05:02,570 --> 00:05:05,635
and we've already covered in some ways and some, some parts of the system so far

87
00:05:05,635 --> 00:05:07,140
are where we're going to use these, these things.

88
00:05:08,330 --> 00:05:10,150
But there be other parts of it,

89
00:05:10,150 --> 00:05:14,420
we need to have high performance, safe and correct data structures

90
00:05:14,590 --> 00:05:17,570
to represent state of the system or the data of the system.

91
00:05:18,410 --> 00:05:20,850
So we've already seen how we can use this for internal metadata,

92
00:05:21,350 --> 00:05:23,610
we talked about the page directory or the page table,

93
00:05:24,560 --> 00:05:25,720
that's more or less a hash table

94
00:05:25,720 --> 00:05:31,050
being used to map page IDs to some location on disk or some location in memory.

95
00:05:31,980 --> 00:05:35,410
We could use the data structures for the core storage of the tables themselves,

96
00:05:36,030 --> 00:05:39,130
remember, we talked about the the index-organized tables,

97
00:05:39,240 --> 00:05:43,390
where the actual tuples themselves would be in the leaf nodes of the B+ tree,

98
00:05:43,770 --> 00:05:47,680
so you could have your tables actually just be represented directly in in a data structure

99
00:05:48,060 --> 00:05:50,200
rather than unordered [] files.

100
00:05:51,280 --> 00:05:54,140
We could also use these data structures for query execution

101
00:05:54,160 --> 00:05:58,370
to generate ephemeral or temporary collections of data,

102
00:05:58,570 --> 00:06:00,470
that allows to execute queries more efficiently,

103
00:06:00,910 --> 00:06:03,615
this is basically how we're going to implement hash join very fast

104
00:06:03,615 --> 00:06:06,260
or how implement joins very quickly using hash joins,

105
00:06:06,340 --> 00:06:07,910
so we build a hash table on the fly,

106
00:06:08,230 --> 00:06:10,910
populate with the data from the tables or scanning,

107
00:06:11,320 --> 00:06:13,760
do the join and then throw the hash table away,

108
00:06:15,250 --> 00:06:16,770
so just because, you know we're building hash [],

109
00:06:16,770 --> 00:06:18,920
it doesn't mean it's going to stick around for a long, a long time,

110
00:06:19,300 --> 00:06:21,330
and then probably the one you're most familiar with is

111
00:06:21,330 --> 00:06:24,080
using these data structures for table indexes,

112
00:06:24,430 --> 00:06:25,520
when you call CREATE INDEX,

113
00:06:25,540 --> 00:06:27,860
that's essentially going to create one of these data structures,

114
00:06:28,600 --> 00:06:30,295
and populate it with the keys

115
00:06:30,295 --> 00:06:31,720
and map them to the tuples,

116
00:06:31,720 --> 00:06:32,850
so you do faster lookups,

117
00:06:32,930 --> 00:06:34,800
like a glossary in a textbook.

118
00:06:36,050 --> 00:06:40,140
So again, we'll see, we'll see these data structures being used

119
00:06:40,400 --> 00:06:41,440
throughout the rest of the semester

120
00:06:41,440 --> 00:06:44,190
in different scenarios that are covered in this list here.

121
00:06:45,820 --> 00:06:48,140
So now what do we care about when we design our data structures,

122
00:06:48,190 --> 00:06:49,940
what are the things we need to be cognitive of,

123
00:06:50,170 --> 00:06:52,940
to make sure that we have an efficient database system,

124
00:06:53,170 --> 00:06:56,860
that actually is also correct, which is very important, right.

125
00:06:57,330 --> 00:06:58,780
So the first thing you've got to worry about is,

126
00:06:59,010 --> 00:07:02,860
how we're actually going to organize the, the, the data structure itself,

127
00:07:03,060 --> 00:07:04,060
in either in memory

128
00:07:04,470 --> 00:07:09,100
or pages that will be in memory but backed by by disk in the buffer pool.

129
00:07:09,680 --> 00:07:10,655
And remember, I said in the beginning,

130
00:07:10,655 --> 00:07:14,560
we want to make design choices in how we implement our system,

131
00:07:15,630 --> 00:07:18,400
if we know it's going to be backed by pages on disk,

132
00:07:18,510 --> 00:07:20,740
that we maximize the amount of sequential I/O,

133
00:07:21,400 --> 00:07:23,635
So maybe we'll lay out the pages in such a way,

134
00:07:23,635 --> 00:07:24,550
the data structure, in such a way,

135
00:07:24,550 --> 00:07:28,110
that we have long strides of data that we can read and write through multiple pages

136
00:07:28,460 --> 00:07:30,510
instead of doing random I/O.

137
00:07:32,170 --> 00:07:35,270
And then we'll talk about how do we actually make our data structures thread safe,

138
00:07:35,740 --> 00:07:37,860
and so for this class, we won't really worry about it,

139
00:07:37,860 --> 00:07:42,380
but we'll spend a whole lecture next week on Thursday or Wednesday next week,

140
00:07:42,610 --> 00:07:47,570
talk about how do we make sure that the data structure is, is correct in [sound],

141
00:07:47,830 --> 00:07:51,405
if we have multiple worker threads or processes coming in

142
00:07:51,405 --> 00:07:56,500
and reading, writing or modifying the data structure at at the same time, right.

143
00:07:57,090 --> 00:07:58,520
And this last one is going to be tricky,

144
00:07:58,520 --> 00:08:03,430
because we're going to care sort of two kinds of correctness in our data structures,

145
00:08:03,840 --> 00:08:05,140
if you want to make them multi threaded,

146
00:08:05,340 --> 00:08:07,660
there's obviously the physical correctness of

147
00:08:08,010 --> 00:08:10,400
making sure we don't have a pointer that goes nowhere

148
00:08:10,400 --> 00:08:13,460
or a page ID that that that doesn't exist, right,

149
00:08:13,460 --> 00:08:16,510
if we have one one thread accessing a page,

150
00:08:16,830 --> 00:08:17,840
another thread is updating it

151
00:08:17,840 --> 00:08:20,620
and that the accessing thread reads something that the guy wrote,

152
00:08:21,220 --> 00:08:25,230
but it hasn't been, you know, it's not, you know, safely committed yet or it's not saved correctly,

153
00:08:25,460 --> 00:08:27,930
then we may end up, you know, falling a pointer to nowhere

154
00:08:28,100 --> 00:08:29,160
and the system would crash,

155
00:08:29,240 --> 00:08:30,450
so we have to avoid that.

156
00:08:31,330 --> 00:08:32,940
But then there's another kind of correctness,

157
00:08:32,940 --> 00:08:34,670
that we'll get to after the midterm,

158
00:08:35,260 --> 00:08:36,620
at sort of the logical level

159
00:08:36,970 --> 00:08:41,510
to make sure that if we make changes to our data structures,

160
00:08:41,890 --> 00:08:44,270
that our own thread can see those changes,

161
00:08:44,890 --> 00:08:46,760
that it looks correct to it,

162
00:08:46,780 --> 00:08:49,700
meaning like if my thread deletes a key from an index,

163
00:08:49,990 --> 00:08:51,525
if I then go back in that same thread

164
00:08:51,525 --> 00:08:53,780
and read, try to read that key in that index,

165
00:08:54,100 --> 00:08:55,310
I shouldn't still see it,

166
00:08:56,020 --> 00:08:58,620
the bits may still physically be there, right,

167
00:08:58,620 --> 00:09:00,210
because we haven't to run garbage collection

168
00:09:00,210 --> 00:09:02,720
and maybe there's a little flag that says this thing's been deleted,

169
00:09:02,800 --> 00:09:04,970
so physically it's still there, but logically it's not,

170
00:09:05,080 --> 00:09:08,550
we need to make sure that we don't see things we shouldn't be seeing, right.

171
00:09:08,960 --> 00:09:12,210
Again, so we won't focus too much on concurrency in this week,

172
00:09:12,410 --> 00:09:15,190
but we'll cover this in more detail next week

173
00:09:15,190 --> 00:09:19,260
and this will be a big issue also too when we talk about concurrency control,

174
00:09:19,400 --> 00:09:21,150
at again at the logical level, having transactions

175
00:09:21,890 --> 00:09:24,510
and making sure we we provide ACID guarantees,

176
00:09:24,620 --> 00:09:26,280
but again, that'll be after the midterm.

177
00:09:28,240 --> 00:09:30,135
All right, so today's class,

178
00:09:30,135 --> 00:09:31,460
we're focusing on hash tables

179
00:09:31,840 --> 00:09:35,055
and again, because it's a low level building blocks

180
00:09:35,055 --> 00:09:37,850
that we can reuse throughout the rest of the system,

181
00:09:38,440 --> 00:09:40,520
and again, this shouldn't be news to anyone here,

182
00:09:40,720 --> 00:09:42,980
a hash table is just going to be an associated array,

183
00:09:43,240 --> 00:09:45,290
that can map keys to values.

184
00:09:46,560 --> 00:09:47,560
You guys, okay, you guys good?

185
00:09:50,190 --> 00:09:50,740
Was that?

186
00:10:11,320 --> 00:10:12,915
The way the hash table is going to work is that,

187
00:10:12,915 --> 00:10:16,280
there's going to be, it's going to be this mapping from keys to values

188
00:10:16,660 --> 00:10:18,720
and we're going to use a hash function,

189
00:10:18,720 --> 00:10:22,790
that's going to allow us to essentially compute some offset within an array

190
00:10:24,250 --> 00:10:30,440
and then it's basically reducing down the, the an arbitrary key to this integer domain,

191
00:10:30,700 --> 00:10:34,230
that we can then jump to some, some location in our hash table

192
00:10:34,230 --> 00:10:36,340
to find the thing that we're looking for, right.

193
00:10:37,120 --> 00:10:40,650
And the, this hash function has to be able to take any possible key,

194
00:10:40,650 --> 00:10:43,490
because again, think of any column type you can define in your database system,

195
00:10:44,110 --> 00:10:47,240
also any, any internal metadata we have in the system itself,

196
00:10:47,530 --> 00:10:48,800
and we need to be able to take that,

197
00:10:49,000 --> 00:10:51,530
you know, hash function needs to reduce that down to an integer.

198
00:10:52,570 --> 00:10:57,140
So in hash table, the space complexity is going to be roughly big n or big O n,

199
00:10:57,490 --> 00:11:04,010
because we're going to have to store a slot for for every possible key we want to have, right,

200
00:11:04,660 --> 00:11:05,910
the time complexity is nice,

201
00:11:05,910 --> 00:11:07,700
because on average we're going to get O(1) lookups,

202
00:11:08,050 --> 00:11:12,470
meaning we hash a key, jump to some location in this hash table array

203
00:11:12,730 --> 00:11:16,220
and then ideally there the thing that still there.

204
00:11:16,920 --> 00:11:17,510
Or just kill it,

205
00:11:19,970 --> 00:11:20,550
there you go.

206
00:11:21,420 --> 00:11:24,770
All right, so that's two kills per this semester, that's not bad.

207
00:11:26,580 --> 00:11:29,170
So again, an average is going to be 1,

208
00:11:29,220 --> 00:11:31,090
because it's going to be like hash,

209
00:11:31,740 --> 00:11:33,305
take your key, hash to some location

210
00:11:33,305 --> 00:11:35,650
and then you land a thing, exactly what you're looking for,

211
00:11:36,320 --> 00:11:38,130
worst case will be big O n,

212
00:11:38,330 --> 00:11:41,340
because, because we'll have to deal with collisions,

213
00:11:41,360 --> 00:11:42,325
it may be the case,

214
00:11:42,325 --> 00:11:45,660
that we hash our key land in some location

215
00:11:45,680 --> 00:11:47,310
and then the thing we're looking for is not there

216
00:11:47,480 --> 00:11:49,350
and we got to scan along on our hash table,

217
00:11:49,820 --> 00:11:51,130
we find the thing we're looking for

218
00:11:51,130 --> 00:11:53,790
and it may be the case that all the slots in our hash table are full

219
00:11:53,900 --> 00:11:57,670
and we have to wrap around basically one above the one

220
00:11:57,670 --> 00:11:58,800
where we landed the hash function,

221
00:11:59,030 --> 00:12:01,470
but we had to loop around to find it, right.

222
00:12:02,360 --> 00:12:03,850
And so the way you sort of handle this,

223
00:12:03,850 --> 00:12:04,660
and we'll see as we go along,

224
00:12:04,660 --> 00:12:08,670
is you size the hash table to be roughly 2n the number of keys you expect.

225
00:12:09,280 --> 00:12:10,360
Now, you may say, okay, and Andy,

226
00:12:10,360 --> 00:12:11,850
how do you know, how do you know what n is,

227
00:12:12,410 --> 00:12:14,425
well, this is, we'll get through that semester,

228
00:12:14,425 --> 00:12:16,165
like the database system is going to try to make a decision

229
00:12:16,165 --> 00:12:18,450
or try to predict how many keys you're going actually going to have

230
00:12:18,740 --> 00:12:20,130
and size it accordingly.

231
00:12:21,420 --> 00:12:22,780
So O(1) sounds great,

232
00:12:23,520 --> 00:12:25,095
And if you're going to take an algorithms class,

233
00:12:25,095 --> 00:12:26,625
this is the holy grail, you want this,

234
00:12:26,625 --> 00:12:28,520
you want O(1), because it's constant time,

235
00:12:29,050 --> 00:12:31,640
but in actuality, again, in a real system,

236
00:12:33,670 --> 00:12:35,420
the constants actually matter a lot,

237
00:12:35,590 --> 00:12:36,770
so even though it's O(1),

238
00:12:37,270 --> 00:12:40,350
you could have one hash function that maybe takes 10 milliseconds to compute,

239
00:12:40,350 --> 00:12:42,470
another hash function takes 1 millisecond to compute,

240
00:12:42,550 --> 00:12:45,350
and obviously the 1 millisecond one is meet a lot faster,

241
00:12:45,370 --> 00:12:48,500
if you think in large scale tables like billions of keys.

242
00:12:49,340 --> 00:12:54,450
So again, just because the algorithm complexity is, is ideal on averages 1,

243
00:12:54,920 --> 00:12:56,400
we had to still care about the implementation

244
00:12:56,450 --> 00:12:58,200
and make sure we're as efficient as possible.

245
00:13:00,260 --> 00:13:03,865
So let's look at the, the toy example,

246
00:13:03,865 --> 00:13:04,890
what a hash table looks like,

247
00:13:05,120 --> 00:13:06,955
and we'll see all the problems that you can have with it,

248
00:13:06,955 --> 00:13:09,810
and then we'll build up this look at more sophisticated schemes,

249
00:13:10,040 --> 00:13:12,270
but are actually used in, in real world database systems.

250
00:13:13,610 --> 00:13:15,900
So the easiest hash table to build is a static hash table,

251
00:13:16,310 --> 00:13:18,840
where you just call malloc a giant array,

252
00:13:19,070 --> 00:13:24,150
where you have one slot in your array for for every key I could possibly have,

253
00:13:24,840 --> 00:13:27,340
and then to find an entry for a given key,

254
00:13:27,660 --> 00:13:30,400
you just take the, you might have the key by the number of elements you have

255
00:13:30,750 --> 00:13:33,885
and you land in some offset in the array, right,

256
00:13:33,885 --> 00:13:34,910
so here's my offsets,

257
00:13:35,260 --> 00:13:38,840
and then any key shows up, I know exactly where to go find it,

258
00:13:39,900 --> 00:13:41,675
and you don't store the keys in this array,

259
00:13:41,675 --> 00:13:45,635
it's essentially just a pointer to some other location,

260
00:13:45,635 --> 00:13:50,340
that's going to have the key and the value together, right.

261
00:13:51,770 --> 00:13:53,575
and the reason why you need to store the original key,

262
00:13:53,575 --> 00:13:58,225
because since the hash may, you could have collisions,

263
00:13:58,225 --> 00:13:58,980
which we'll get in a second,

264
00:13:59,780 --> 00:14:03,145
you need to check whether the key you looking that you land on through your hash table

265
00:14:03,145 --> 00:14:04,770
is actually the key you're trying to find,

266
00:14:05,610 --> 00:14:09,010
and the value here could be a pointer to the tuple to a record ID,

267
00:14:09,330 --> 00:14:11,320
or could there actually be some additional values,

268
00:14:11,820 --> 00:14:13,990
for our purpose today, we don't actually care.

269
00:14:16,560 --> 00:14:18,400
So what are some problems with this approach?

270
00:14:20,430 --> 00:14:20,830
Yes.

271
00:14:26,160 --> 00:14:27,670
He says, what if you have, what do you have,

272
00:14:28,770 --> 00:14:30,095
well, I'm assuming it's static,

273
00:14:30,095 --> 00:14:31,780
so what if you have n+1 keys,

274
00:14:32,460 --> 00:14:33,610
how do you resize this thing,

275
00:14:34,200 --> 00:14:35,540
and you know basically, in this scenario,

276
00:14:35,540 --> 00:14:36,610
you have to rehash everything,

277
00:14:37,110 --> 00:14:38,110
so that sucks.

278
00:14:39,360 --> 00:14:39,995
What other problems?

279
00:14:39,995 --> 00:14:40,330
Yes.

280
00:14:41,690 --> 00:14:43,080
Does it handle collision, What is collision?

281
00:14:46,810 --> 00:14:48,230
Yes, that's correct, yes,

282
00:14:48,610 --> 00:14:49,830
two keys that have the same value,

283
00:14:49,830 --> 00:14:53,210
they're going to land in the same location in, in, in our array,

284
00:14:53,440 --> 00:14:54,620
even though they're not the same,

285
00:14:54,670 --> 00:14:58,610
but I'm assuming that you know that everyone has to be unique and you can't have collisions,

286
00:14:58,840 --> 00:15:00,410
and this example doesn't handle that.

287
00:15:02,190 --> 00:15:02,720
There's one more problem.

288
00:15:07,800 --> 00:15:11,110
I'm assuming that the keys are unique, right,

289
00:15:11,340 --> 00:15:13,150
I can have key value equals one

290
00:15:13,290 --> 00:15:14,860
and key value equals two

291
00:15:15,060 --> 00:15:16,210
same key but different values,

292
00:15:16,620 --> 00:15:21,800
in my, my sort of toy example here, doesn't handle this, right.

293
00:15:22,270 --> 00:15:24,225
So this is unrealistic again for these three assumptions.

294
00:15:24,225 --> 00:15:25,010
So the first one is,

295
00:15:25,030 --> 00:15:26,540
you have to know all the keys ahead of time,

296
00:15:27,220 --> 00:15:29,715
in some cases, you do other cases, you don't,

297
00:15:29,715 --> 00:15:30,950
in the case of the buffer pool,

298
00:15:31,120 --> 00:15:32,460
and then we talked about last class,

299
00:15:32,460 --> 00:15:37,550
that if you assume that the, you know, the size of your buffer pool is fixed,

300
00:15:37,750 --> 00:15:40,160
you're going to have a fixed number of frames in your buffer pool,

301
00:15:40,390 --> 00:15:44,150
therefore, you know the exact number of slots you need in your hash table,

302
00:15:44,930 --> 00:15:48,030
but if I if I build a hash table index

303
00:15:48,290 --> 00:15:49,530
and I keep inserting tuples,

304
00:15:49,640 --> 00:15:52,560
now the number of keys is growing, as I insert new tuples.

305
00:15:54,010 --> 00:15:55,755
Every key is unique in this scenario,

306
00:15:55,755 --> 00:16:01,440
here again, how do we need a way to handle keys that have duplicate keys,

307
00:16:01,440 --> 00:16:02,210
we have different values,

308
00:16:02,320 --> 00:16:03,080
got to handle that.

309
00:16:03,820 --> 00:16:06,340
And then the thing the he brought up is that,

310
00:16:06,340 --> 00:16:08,520
we're assuming here we have what it's called a perfect hash function,

311
00:16:08,960 --> 00:16:10,420
that guarantees no collisions,

312
00:16:10,420 --> 00:16:12,690
which does not exist in the real world

313
00:16:13,820 --> 00:16:14,695
and this in the real world,

314
00:16:14,695 --> 00:16:15,930
but it's basically toy [],

315
00:16:15,950 --> 00:16:17,560
no database system can actually do this,

316
00:16:17,560 --> 00:16:21,120
because again, you need to know the key domain ahead of time, right,

317
00:16:21,260 --> 00:16:23,485
there's no magical hash function that guarantees,

318
00:16:23,485 --> 00:16:27,720
for any given key, you can generate a unique, unique hash value,

319
00:16:29,520 --> 00:16:32,680
the way to actually implement one of those is through a hash table,

320
00:16:32,970 --> 00:16:35,710
so you basically need a hash table for your hash table to do this,

321
00:16:38,230 --> 00:16:40,560
for some systems do do that,

322
00:16:40,560 --> 00:16:41,990
but not for a hash function.

323
00:16:44,820 --> 00:16:45,640
So we've got to be smarter,

324
00:16:45,720 --> 00:16:46,685
and we got to make sure that,

325
00:16:46,685 --> 00:16:49,570
we deal with the environment that we're operating in with databases.

326
00:16:50,680 --> 00:16:52,215
All right, so there's two decisions we have to make

327
00:16:52,215 --> 00:16:53,570
when we run a about a hash table,

328
00:16:53,800 --> 00:16:57,560
so someone says they have a hash table, it's sort of two parts there,

329
00:16:57,580 --> 00:16:58,605
the hash function itself,

330
00:16:58,605 --> 00:17:03,350
that can how to map a large key space down to a finite, smaller domain,

331
00:17:04,380 --> 00:17:07,000
based on the number of slots, I'm going to have in in my array,

332
00:17:07,560 --> 00:17:08,630
and there be this trade off

333
00:17:08,630 --> 00:17:10,565
between how fast we want our hash function to be

334
00:17:10,565 --> 00:17:15,940
versus how likely it is that two keys, two distinct keys will collide.

335
00:17:17,210 --> 00:17:20,070
What's the hash, what's the fastest hash function I could build?

336
00:17:22,270 --> 00:17:22,790
What's that?

337
00:17:23,440 --> 00:17:24,880
So, [], you can go faster than that,

338
00:17:24,880 --> 00:17:25,645
so he basically says,

339
00:17:25,645 --> 00:17:27,000
for a given key, you spit out the same key,

340
00:17:27,140 --> 00:17:29,155
but if you have a string key and I got to make it an integer,

341
00:17:29,155 --> 00:17:29,760
how do I do that?

342
00:17:32,690 --> 00:17:33,720
So take the first bit,

343
00:17:36,190 --> 00:17:37,580
yeah, that would be pretty fast, too,

344
00:17:40,300 --> 00:17:42,940
you just can return 1, right,

345
00:17:43,980 --> 00:17:46,600
that sit in the stack in a register that be super fast.

346
00:17:48,120 --> 00:17:49,715
It's the worst hash function in terms of collision,

347
00:17:49,715 --> 00:17:50,920
because everything is going to map to 1,

348
00:17:51,360 --> 00:17:52,420
but it it'll be fast.

349
00:17:53,160 --> 00:17:54,845
So it's this trade off trying to figure out

350
00:17:54,845 --> 00:17:57,490
and you sort of think the perfect hash function is the other end,

351
00:17:57,660 --> 00:17:59,980
the collision rate is, is, is, is zero,

352
00:18:00,300 --> 00:18:01,235
but it's super slow,

353
00:18:01,235 --> 00:18:02,800
because you have to do this extra lookup.

354
00:18:03,270 --> 00:18:04,220
So you want something in the middle,

355
00:18:04,220 --> 00:18:06,340
that's going to be fast and and have a low collision rate.

356
00:18:07,420 --> 00:18:09,150
And, and then the hashing scheme is going be

357
00:18:09,150 --> 00:18:13,190
the mechanism we're going to use to handle collisions after we done our hashing

358
00:18:13,600 --> 00:18:18,060
and the way the trade off here is going to be again,

359
00:18:18,060 --> 00:18:20,780
sort of the classic storage versus compute in computer science,

360
00:18:21,070 --> 00:18:24,530
like I could, I could allocate a two terabyte hash table

361
00:18:24,820 --> 00:18:28,520
and I'm pretty, pretty unlikely going to have, I'm not likely going to have collisions

362
00:18:28,630 --> 00:18:30,030
for my key set is super small,

363
00:18:30,030 --> 00:18:32,270
but I allocated this massive hash table,

364
00:18:32,720 --> 00:18:33,995
or I can have a smaller one,

365
00:18:33,995 --> 00:18:34,925
but I'll have a lot of collisions

366
00:18:34,925 --> 00:18:37,450
and therefore I have to spend more compute to handle those collisions.

367
00:18:37,710 --> 00:18:40,520
So again, it's trying to figure out how to do the right, get the right trade off

368
00:18:40,520 --> 00:18:42,310
between not over allocating,

369
00:18:42,480 --> 00:18:45,730
but then also not wasting a lot of instructions to deal with with collisions.

370
00:18:48,060 --> 00:18:49,060
All right, so today's talk,

371
00:18:49,110 --> 00:18:50,620
we'll talk a little bit about hash functions,

372
00:18:51,030 --> 00:18:53,830
just to show you what the state of the art is,

373
00:18:53,850 --> 00:18:54,845
I'm not going to say how they work

374
00:18:54,845 --> 00:18:56,350
to tell you that they exist,

375
00:18:56,640 --> 00:18:57,620
again, we're database people,

376
00:18:57,620 --> 00:18:58,880
we're not in the business of writing hash functions,

377
00:18:58,880 --> 00:19:00,160
we'll let other people do that for us.

378
00:19:01,080 --> 00:19:05,110
And then we'll talk about the sort of the classic static hashing schemes,

379
00:19:05,190 --> 00:19:07,000
where, you know, the number of keys ahead of time.

380
00:19:07,320 --> 00:19:09,365
And then we'll talk about dynamic hashing schemes,

381
00:19:09,365 --> 00:19:11,260
where the hash table can actually grow and shrink,

382
00:19:11,840 --> 00:19:12,870
based on the number of keys,

383
00:19:13,430 --> 00:19:13,830
okay?

384
00:19:15,460 --> 00:19:15,980
All right.

385
00:19:17,850 --> 00:19:20,470
So again, we're not in the business of writing hash functions,

386
00:19:20,520 --> 00:19:23,440
other people that are smarter than us in this space have have done it for us,

387
00:19:23,970 --> 00:19:25,030
so we're just going to rely on them.

388
00:19:25,470 --> 00:19:27,020
Again, the basic idea of a hash functions is that,

389
00:19:27,020 --> 00:19:27,970
we have some input key,

390
00:19:28,170 --> 00:19:30,340
any arbitrary number of bytes of any type,

391
00:19:30,570 --> 00:19:33,880
and we need to return an integer that represents that key,

392
00:19:34,140 --> 00:19:35,740
typically is 64 bits,

393
00:19:36,420 --> 00:19:38,795
there are 128 bit hash functions,

394
00:19:38,795 --> 00:19:40,420
but I don't think databases use those,

395
00:19:40,620 --> 00:19:42,520
there are 32 bit hash functions as well,

396
00:19:43,970 --> 00:19:45,420
so to return an integer.

397
00:19:46,220 --> 00:19:49,500
So in this scheme or in, in this, in a database system,

398
00:19:49,580 --> 00:19:55,680
we don't care about any sort of protection privacy mechanisms for a hash function,

399
00:19:56,000 --> 00:19:58,680
meaning we're not going to use anything that has cryptographic guarantees,

400
00:19:59,060 --> 00:20:01,105
so we're not using SHA-256 or whatever,

401
00:20:01,105 --> 00:20:02,620
like we don't, we don't care about those things,

402
00:20:02,620 --> 00:20:04,800
because we're running on the inside of the system,

403
00:20:06,440 --> 00:20:09,725
it's we're not worried about leaking anything,

404
00:20:09,725 --> 00:20:11,860
when we can build a hash table to do a JOIN,

405
00:20:12,150 --> 00:20:15,070
because no one on the outside of the system can see that data structure,

406
00:20:15,890 --> 00:20:17,850
so we don't care about any of those things

407
00:20:18,200 --> 00:20:22,310
and as a result, we can actually run a lot faster, right,

408
00:20:22,310 --> 00:20:23,690
SHA-256 will be really slow

409
00:20:23,690 --> 00:20:27,400
versus something like like MurmurHash or XXHash.

410
00:20:29,270 --> 00:20:30,085
And as I already said before,

411
00:20:30,085 --> 00:20:32,070
we want something that's fast and it has a low collision rate.

412
00:20:32,960 --> 00:20:38,415
So, this is just a quick, quick overview,

413
00:20:38,415 --> 00:20:39,860
what hash function systems are using,

414
00:20:40,870 --> 00:20:43,220
some like Postgres roll their own hash function,

415
00:20:43,900 --> 00:20:45,410
but a lot of the more modern systems,

416
00:20:45,550 --> 00:20:47,180
they're going to use something off the shelf,

417
00:20:47,560 --> 00:20:51,500
like XXHash or MurmurHash or the SpookyHash.

418
00:20:52,700 --> 00:20:55,720
So basically the main takeaway from this is that,

419
00:20:56,400 --> 00:20:58,430
the state of art one is XXHash from Facebook,

420
00:20:59,350 --> 00:21:00,980
with the third version, XXHash-3,

421
00:21:01,210 --> 00:21:07,550
this one is shown to have the fastest performance and also the lowest collision rate,

422
00:21:07,980 --> 00:21:14,360
there are some systems that use CRC-32 or 64 for hashing integers,

423
00:21:14,470 --> 00:21:19,340
because it would actually CPU instructions in x86 that do that in a few number of cycles,

424
00:21:21,220 --> 00:21:22,305
there are some systems that do that,

425
00:21:22,305 --> 00:21:26,120
but in terms of random strings, you typically want to use this,

426
00:21:26,760 --> 00:21:27,905
so MurmurHash is [],

427
00:21:27,905 --> 00:21:30,280
because it was MurmurHash was written by this [random dude] on the Internet,

428
00:21:30,480 --> 00:21:32,320
he had a good, fast general purpose hash function,

429
00:21:32,700 --> 00:21:36,580
Google took that and made CityHash by forking it,

430
00:21:36,630 --> 00:21:37,625
and then they have a newer version,

431
00:21:37,625 --> 00:21:39,520
we call it FarmHash,

432
00:21:40,230 --> 00:21:41,380
that has even better collision rates,

433
00:21:41,400 --> 00:21:43,990
there's, there's a bunch of different hash functions out there,

434
00:21:44,520 --> 00:21:46,720
but XXHash-3 is what you want to use.

435
00:21:47,280 --> 00:21:50,120
And so there's a bunch of these repositories on Github,

436
00:21:51,010 --> 00:21:54,080
or people I've written basically [] or benchmarks to,

437
00:21:54,160 --> 00:21:57,110
to run all possible hash functions that are out there

438
00:21:57,340 --> 00:21:58,590
and see what their collision rate is,

439
00:21:58,590 --> 00:21:59,510
see what the performance is,

440
00:22:00,580 --> 00:22:02,820
so this is this Mhasher SMhasher,

441
00:22:02,820 --> 00:22:04,730
there's another one written by the MurmurHash guy

442
00:22:05,140 --> 00:22:07,460
and there's another one that's a fork at this,

443
00:22:08,380 --> 00:22:11,450
that's only [] all the cryptography stuff,

444
00:22:11,800 --> 00:22:13,605
but for this repository here,

445
00:22:13,605 --> 00:22:15,080
they have this nice summary here,

446
00:22:15,160 --> 00:22:18,920
that says these are the ones that work the best and have good collision rates,

447
00:22:19,380 --> 00:22:23,540
and then the top one here is XXHash-3, the Facebook one.

448
00:22:25,540 --> 00:22:26,990
So again, we don't care,

449
00:22:27,460 --> 00:22:29,840
it's a hash function, keys in, integer out,

450
00:22:30,010 --> 00:22:31,430
we'll just use whatever they have.

451
00:22:32,120 --> 00:22:35,460
And then there's the full list of all the different hash functions,

452
00:22:35,750 --> 00:22:36,775
some are tailored to ARM,

453
00:22:36,775 --> 00:22:38,610
some are tailored x86 or whatever

454
00:22:38,840 --> 00:22:43,520
you can, you can get, you know, more low level details based on the environment,

455
00:22:43,520 --> 00:22:46,450
but XXHash3 is going to be a good default choice.

456
00:22:50,050 --> 00:22:50,810
All right, so now.

457
00:22:52,210 --> 00:22:54,200
Assuming, assuming running XXHash3,

458
00:22:54,400 --> 00:22:57,090
we want to talk about what the hash table is going to look like

459
00:22:57,090 --> 00:22:58,250
and how do we handle collisions.

460
00:22:59,030 --> 00:23:02,550
So for this lecture, I'm going to focus on the two most common ones,

461
00:23:02,840 --> 00:23:06,390
but number one is actually going to be the most common one of all the systems,

462
00:23:06,560 --> 00:23:07,530
linear probe hashing,

463
00:23:08,480 --> 00:23:09,510
it's the simplest,

464
00:23:10,650 --> 00:23:12,610
and it seems kind of brain dead in some ways,

465
00:23:13,830 --> 00:23:17,950
but because it's so simple, it is actually the fastest, right.

466
00:23:18,570 --> 00:23:20,885
And then Cuckoo hashing is a variant of this,

467
00:23:20,885 --> 00:23:22,510
that basically does multiple hash functions.

468
00:23:22,800 --> 00:23:23,980
So there's a bunch of other techniques,

469
00:23:24,060 --> 00:23:25,595
Robin Hood hashing, Hopscotch hashing,

470
00:23:25,595 --> 00:23:27,250
Swiss Tables from Google,

471
00:23:27,510 --> 00:23:28,925
we won't cover that in this semester,

472
00:23:28,925 --> 00:23:30,190
but if you take the advanced class,

473
00:23:30,300 --> 00:23:31,270
we will cover those things.

474
00:23:31,860 --> 00:23:34,490
And I would say that, the current research basically shows that,

475
00:23:34,780 --> 00:23:38,340
the Linear Probe stuff and the Swiss Tables are the fastest ones,

476
00:23:38,340 --> 00:23:42,770
all these extra fancy versions are,

477
00:23:44,620 --> 00:23:47,340
as they try to be, they try to be more performance,

478
00:23:47,340 --> 00:23:51,380
because they avoid having to spend longer time looking for for keys

479
00:23:51,610 --> 00:23:53,840
by moving things around, when when you insert,

480
00:23:54,070 --> 00:23:57,650
but all that work of moving things around is is a performance penalty

481
00:23:57,970 --> 00:24:01,280
and you're better off just doing the naive thing in your hashing.

482
00:24:04,290 --> 00:24:06,640
This question, there's a reason why we're not talking about chain hashing,

483
00:24:06,690 --> 00:24:08,860
because that's dynamic, that'll be next,

484
00:24:10,850 --> 00:24:12,870
chain hashing can grow, this is fixed size,

485
00:24:14,030 --> 00:24:15,780
that we'll cover that later this semester or later this class,

486
00:24:17,970 --> 00:24:19,480
these are all static hashing schemes.

487
00:24:21,870 --> 00:24:25,420
There's variations of linear probing, probing,

488
00:24:26,730 --> 00:24:27,520
let's keep it simple.

489
00:24:28,860 --> 00:24:33,880
All right, so Linear Probe hashing is really simple,

490
00:24:34,520 --> 00:24:37,960
it's a giant array of slots

491
00:24:38,580 --> 00:24:40,900
and we're going to hash into it,

492
00:24:42,670 --> 00:24:44,330
if we insert, we hash into it,

493
00:24:44,380 --> 00:24:45,650
if the slot is free,

494
00:24:45,820 --> 00:24:47,240
we insert the thing we're looking for,

495
00:24:47,410 --> 00:24:48,860
if the slot is not free,

496
00:24:49,150 --> 00:24:53,205
we just look at the next slot and insert there if we can,

497
00:24:53,205 --> 00:24:55,640
or we keep looking until we have a free slot,

498
00:24:55,660 --> 00:24:56,810
potentially wrapping around,

499
00:24:57,950 --> 00:24:59,040
until we find a free location.

500
00:24:59,840 --> 00:25:02,585
And then if we loop back around

501
00:25:02,585 --> 00:25:05,110
and realize we're at the slot where we started that,

502
00:25:05,130 --> 00:25:06,520
then we know the hash table is full

503
00:25:06,780 --> 00:25:10,780
and we have to and you know abort it,

504
00:25:11,070 --> 00:25:12,820
double the size and rehash everything,

505
00:25:13,170 --> 00:25:15,340
it's a simple way to grow it, right.

506
00:25:15,900 --> 00:25:21,010
So the state, our implementation for this or one state is this [] thing from, from Google,

507
00:25:22,500 --> 00:25:27,460
and it's the flat_hash, flat_hash_map type or data structure,

508
00:25:28,160 --> 00:25:30,510
and they have pretty good documentation to describe actually how it works,

509
00:25:30,740 --> 00:25:32,550
and some of the optimizations they do will cover.

510
00:25:33,280 --> 00:25:36,560
So this is sometimes called open addressing, open addressing hashing,

511
00:25:36,730 --> 00:25:39,290
because the idea is that it,

512
00:25:39,830 --> 00:25:41,850
there's no guarantee that for a given key,

513
00:25:42,080 --> 00:25:45,180
it's going to always be in the same address or same location in the slot,

514
00:25:45,500 --> 00:25:47,790
depending on what, what, what got inserted before it,

515
00:25:47,990 --> 00:25:50,830
it may, it may get moved around, right.

516
00:25:51,640 --> 00:25:53,485
If you get a dictionary in Python,

517
00:25:53,485 --> 00:25:55,380
this is essentially what you're getting as well.

518
00:25:57,725 --> 00:25:58,270
So let's see how it works.

519
00:25:58,680 --> 00:26:01,130
So say we want to insert key A, right,

520
00:26:01,130 --> 00:26:01,960
so we hash it,

521
00:26:02,160 --> 00:26:03,790
mod by the number of slots that we have

522
00:26:04,020 --> 00:26:05,710
and then we hit land this a location here,

523
00:26:05,820 --> 00:26:10,520
so that we insert our key along the value together, right,

524
00:26:11,080 --> 00:26:12,810
again, the reason why we need the key is because,

525
00:26:12,810 --> 00:26:15,860
if we go to a lookup again for looking for A,

526
00:26:16,210 --> 00:26:18,260
we need, we hash to the same location,

527
00:26:18,550 --> 00:26:20,025
but now we got to do an equality check

528
00:26:20,025 --> 00:26:23,300
to see whether the key that we're looking for is the key in in a given slot.

529
00:26:25,510 --> 00:26:27,000
Same thing, so if you want to hash B,

530
00:26:27,000 --> 00:26:29,970
same thing, hash here mod by the number of slots,

531
00:26:29,970 --> 00:26:31,910
we end up here and we sort at the top.

532
00:26:32,850 --> 00:26:35,080
So now we want to insert C,

533
00:26:36,180 --> 00:26:37,040
so we hash C,

534
00:26:37,040 --> 00:26:38,860
it lands to the same location where A is,

535
00:26:39,300 --> 00:26:41,470
but that slot is occupied,

536
00:26:41,700 --> 00:26:42,970
so we can't insert it there,

537
00:26:43,290 --> 00:26:45,430
so we just follow down to the next slot

538
00:26:45,660 --> 00:26:48,020
and insert our key there, right.

539
00:26:49,930 --> 00:26:50,700
Same thing with D,

540
00:26:50,700 --> 00:26:51,890
D once to go where C is,

541
00:26:52,030 --> 00:26:53,690
it can't, because that slot occupied,

542
00:26:53,980 --> 00:26:56,330
so it just moves down to the next one and inserts it there.

543
00:26:56,860 --> 00:27:00,370
And we just keep going down for all the other keys we want to store, right.

544
00:27:01,250 --> 00:27:02,215
And in this case here,

545
00:27:02,215 --> 00:27:05,550
if, say, if F, this space was occupied,

546
00:27:06,280 --> 00:27:09,140
F kind of wrapped around, start at the beginning and insert at the top,

547
00:27:10,520 --> 00:27:12,240
think of it as a giant circular buffer.

548
00:27:14,450 --> 00:27:15,270
Pretty simple, right?

549
00:27:18,460 --> 00:27:19,730
What are some potential problems with this?

550
00:27:23,870 --> 00:27:24,865
He says deletion sucks,

551
00:27:24,865 --> 00:27:25,675
says you lose the whole chain,

552
00:27:25,675 --> 00:27:26,190
what do you mean?

553
00:27:39,720 --> 00:27:40,660
Right, so he says,

554
00:27:41,010 --> 00:27:42,250
I don't think all the slides ahead of time,

555
00:27:42,420 --> 00:27:44,680
what happens, what happens if you delete C?

556
00:27:46,070 --> 00:27:48,300
So delete C, we hash it,

557
00:27:48,380 --> 00:27:50,690
we land where A is, right,

558
00:27:50,980 --> 00:27:54,170
now we do the equality check to see does A equal C,

559
00:27:54,310 --> 00:27:56,480
no, so we know that's not the key we're looking for

560
00:27:56,650 --> 00:27:57,630
and then we keep going

561
00:27:57,630 --> 00:27:59,720
until we find empty slot or the key we're looking for,

562
00:27:59,830 --> 00:28:00,585
so in this case here,

563
00:28:00,585 --> 00:28:02,030
after jumping down, we find C

564
00:28:02,290 --> 00:28:03,560
and now we need to delete it,

565
00:28:04,470 --> 00:28:06,760
but now we have an empty space, as he said,

566
00:28:07,290 --> 00:28:10,030
so if I try to go do a lookup on something like D,

567
00:28:10,500 --> 00:28:12,190
D is going to hash to this empty spot,

568
00:28:14,080 --> 00:28:16,470
and it's going to say, oh well, nothing is here, right,

569
00:28:16,470 --> 00:28:18,735
but it really is, you know, it's the next slot down,

570
00:28:18,735 --> 00:28:21,140
but because the way the protocol works, the scheme works,

571
00:28:21,430 --> 00:28:25,220
if I see an empty slot, then I know I'm done, right.

572
00:28:26,840 --> 00:28:27,670
So what's one way to handle this?

573
00:28:30,650 --> 00:28:32,610
Gravestone, tombstone, that's one approach, yes,

574
00:28:33,950 --> 00:28:34,690
we'll get it there,

575
00:28:34,690 --> 00:28:36,030
which that is the correct answer.

576
00:28:37,480 --> 00:28:38,560
So you could do this,

577
00:28:38,560 --> 00:28:41,550
you could just rehash, move up, right,

578
00:28:42,490 --> 00:28:43,845
is that a good idea or a bad idea,

579
00:28:43,845 --> 00:28:44,790
well, clearly it's a bad idea,

580
00:28:44,790 --> 00:28:45,800
because I said no one does this,

581
00:28:46,360 --> 00:28:47,450
but why is it a bad idea?

582
00:28:49,900 --> 00:28:50,475
Move everything,

583
00:28:50,475 --> 00:28:52,190
again, think huge, a billion keys,

584
00:28:52,540 --> 00:28:55,220
I going to go rehash everyone that be terrible, right,

585
00:28:55,720 --> 00:28:58,700
so it's super expensive and no one does this.

586
00:29:01,640 --> 00:29:02,610
Skipping through this, right.

587
00:29:02,780 --> 00:29:05,125
So yeah, this is not the sense,

588
00:29:05,125 --> 00:29:05,820
you don't want to do this.

589
00:29:06,800 --> 00:29:10,110
The correct solution is what he was saying is what it's called a tombstone,

590
00:29:10,550 --> 00:29:11,725
and the idea here is that,

591
00:29:11,725 --> 00:29:12,600
I delete C,

592
00:29:13,550 --> 00:29:15,480
but instead, again, instead of setting it as empty,

593
00:29:15,890 --> 00:29:17,400
I'm gonna put a little marker here,

594
00:29:18,270 --> 00:29:19,840
to say this slot,

595
00:29:20,100 --> 00:29:22,210
there was a key here and now it's been deleted.

596
00:29:22,940 --> 00:29:23,650
So that way,

597
00:29:23,650 --> 00:29:26,040
if anybody comes along like doing a lookup in D,

598
00:29:26,420 --> 00:29:28,090
it sees the tombstone

599
00:29:28,090 --> 00:29:29,100
and says, well, it's not empty,

600
00:29:29,420 --> 00:29:30,390
something was here,

601
00:29:31,130 --> 00:29:32,500
but there's nothing here that I'm looking for,

602
00:29:32,500 --> 00:29:35,490
so let me, let me look down and keep scanning along

603
00:29:35,960 --> 00:29:38,500
until I find the thing I'm looking for, right.

604
00:29:40,940 --> 00:29:42,600
So you can reuse these,

605
00:29:44,420 --> 00:29:47,370
you can reuse the slot with the marker of the tombstone for new keys,

606
00:29:48,020 --> 00:29:49,240
you just insert over top of it

607
00:29:49,240 --> 00:29:54,240
and that doesn't break the flow or break anything else in, in the hash table, right.

608
00:29:55,080 --> 00:29:57,305
Now, maybe the case you want to periodically run a garbage collection,

609
00:29:57,305 --> 00:29:59,075
because you can start accumulating much of these tombstones

610
00:29:59,075 --> 00:30:00,130
and it's just wasted space,

611
00:30:00,420 --> 00:30:01,540
if you're not reusing them,

612
00:30:02,790 --> 00:30:05,650
but for our purposes, we can ignore that.

613
00:30:07,530 --> 00:30:09,005
So I want to put say G,

614
00:30:09,005 --> 00:30:09,910
G can go right here

615
00:30:10,020 --> 00:30:10,930
and that's fine.

616
00:30:12,900 --> 00:30:15,650
Now, I'm not going, not going to discuss this too much details,

617
00:30:15,650 --> 00:30:16,985
but there is a challenge though,

618
00:30:16,985 --> 00:30:18,730
how you actually want to represent these tombstones,

619
00:30:19,560 --> 00:30:22,330
and also represent something that is empty,

620
00:30:23,200 --> 00:30:25,590
and potentially also represent that I have a null key,

621
00:30:26,400 --> 00:30:29,620
which you can do in a database system, right.

622
00:30:31,570 --> 00:30:34,755
So we could do the trick we talked about before with slotted pages,

623
00:30:34,755 --> 00:30:39,530
where we could have a bitmap in front of at the top of every head or every page in our hash table,

624
00:30:39,730 --> 00:30:41,955
I'm not showing the division here between pages,

625
00:30:41,955 --> 00:30:42,680
but think of like,

626
00:30:43,150 --> 00:30:45,380
for simplicity, every page is to these slots,

627
00:30:45,730 --> 00:30:46,800
so in the header of that page,

628
00:30:46,800 --> 00:30:47,715
I could keep track of like,

629
00:30:47,715 --> 00:30:49,005
okay, here's the slots that are empty,

630
00:30:49,005 --> 00:30:50,150
here's the slots that are null,

631
00:30:50,320 --> 00:30:54,710
or here's the slots that are, that are, that are marked with the tombstone.

632
00:30:55,630 --> 00:30:58,065
So I need some additional metadata to keep track of these things,

633
00:30:58,065 --> 00:31:00,860
and you obviously don't want to do it on a per key basis,

634
00:31:01,030 --> 00:31:06,900
because, that could mess up with the alignment of things waste space.

635
00:31:10,580 --> 00:31:13,950
So the other thing we got to deal with now is non-unique keys.

636
00:31:15,240 --> 00:31:15,710
All right.

637
00:31:17,235 --> 00:31:18,290
So there's two approaches to do this.

638
00:31:19,040 --> 00:31:19,840
One is that,

639
00:31:20,220 --> 00:31:27,160
instead of storing the value in our giant hash array, array, along with the keys,

640
00:31:28,040 --> 00:31:31,200
instead, the value would just be a pointer, like a page ID,

641
00:31:32,080 --> 00:31:38,970
to some other location that will store my list of keys, sorry list of values, right,

642
00:31:39,320 --> 00:31:41,100
so for the key XYZ,

643
00:31:41,330 --> 00:31:44,400
there's a pointer to some, basically a linked list,

644
00:31:44,540 --> 00:31:45,930
that has all the possible values,

645
00:31:46,280 --> 00:31:48,090
and then for the other key, the same thing.

646
00:31:49,330 --> 00:31:50,580
What's nice about this is because,

647
00:31:50,580 --> 00:31:54,680
as I insert new keys, insert duplic keys over over again,

648
00:31:54,880 --> 00:31:57,270
I'm not really changing the main hash table,

649
00:31:57,270 --> 00:31:59,150
I'm sort of appending to this sort of link list,

650
00:31:59,760 --> 00:32:01,940
it's like the chain hash table that he talked about before

651
00:32:01,960 --> 00:32:02,870
or he asked about before,

652
00:32:02,890 --> 00:32:03,710
we get in a second.

653
00:32:04,870 --> 00:32:09,240
The more common approach is to store redundant keys together, right,

654
00:32:09,960 --> 00:32:12,980
and again, this doesn't break the open addressing of linear probe hashing scheme

655
00:32:13,210 --> 00:32:16,730
is that I'm always hash the key,

656
00:32:16,870 --> 00:32:17,865
I landed on location

657
00:32:17,865 --> 00:32:21,035
and I find a, you know, I just find a free slot

658
00:32:21,035 --> 00:32:22,270
and the thing I'm looking for.

659
00:32:23,290 --> 00:32:24,740
This does make it a little bit more tricky,

660
00:32:24,970 --> 00:32:25,815
when you do lookups,

661
00:32:25,815 --> 00:32:30,570
like give me all, give me all the keys of XYZ, key value pairs,

662
00:32:30,800 --> 00:32:34,345
because now I got, I know, like I got to keep, I got to keep scanning

663
00:32:34,345 --> 00:32:36,415
until I find an empty location, empty slot,

664
00:32:36,415 --> 00:32:38,370
to know that I'm not going to see XYZ ever again,

665
00:32:38,720 --> 00:32:40,980
whereas in the first, the first scenario,

666
00:32:41,880 --> 00:32:43,610
I find XYZ in my hash table,

667
00:32:43,720 --> 00:32:47,570
then I land follow the pointer to the list of values

668
00:32:47,800 --> 00:32:52,540
and I know that's all the possible values I could have for that given key, right.

669
00:32:53,440 --> 00:32:54,555
But for simplicity reasons,

670
00:32:54,555 --> 00:32:58,920
instead I having't maintained the sort of separate linked list for non-unique keys

671
00:32:58,920 --> 00:33:05,620
and and the the non, the the inline version for, for for unique keys,

672
00:33:06,060 --> 00:33:08,620
most systems use the redundant key approach,

673
00:33:09,530 --> 00:33:12,360
because you don't have to multiple implement implementations.

674
00:33:14,790 --> 00:33:15,190
Yes.

675
00:33:21,650 --> 00:33:23,700
Your question is, how would you differentiate between?

676
00:33:32,340 --> 00:33:32,960
So his question is,

677
00:33:32,960 --> 00:33:37,240
how would you differentiate update a value versus an insert of a value,

678
00:33:39,700 --> 00:33:42,360
yeah, for hash tables, you really don't do updates,

679
00:33:42,360 --> 00:33:44,450
it would be a delete followed by an insert, right.

680
00:33:46,620 --> 00:33:47,685
And of course, now the tricky thing is,

681
00:33:47,685 --> 00:33:51,780
if I want to delete key XYZ with value2,

682
00:33:54,660 --> 00:33:58,100
I, if I want to delete one of these,

683
00:33:58,540 --> 00:34:04,160
I have to know the value, make sure that one,

684
00:34:04,540 --> 00:34:05,480
may not be what I want.

685
00:34:13,160 --> 00:34:14,850
The question if you have,

686
00:34:15,920 --> 00:34:17,880
so if you have a key with multiple values,

687
00:34:18,320 --> 00:34:19,465
you just hash the entire tuple,

688
00:34:19,465 --> 00:34:20,130
what do you mean by that?

689
00:34:21,020 --> 00:34:31,250
Yeah, so like,

690
00:34:32,080 --> 00:34:33,050
yeah, so statement is like,

691
00:34:34,070 --> 00:34:36,270
if I'm looking for if I'm looking for exact match,

692
00:34:36,830 --> 00:34:38,185
in that case, like I don't need the hash table,

693
00:34:38,185 --> 00:34:39,150
because if I have,

694
00:34:39,710 --> 00:34:42,070
but like if I'm trying to remove it from the data structure,

695
00:34:42,070 --> 00:34:45,270
like this exact, like key value pair, right,

696
00:34:45,440 --> 00:34:50,910
then you basically, and you, you,

697
00:34:52,660 --> 00:34:54,225
you need to find that exact pair,

698
00:34:54,225 --> 00:34:55,080
you need to have it time

699
00:34:55,080 --> 00:34:56,780
and just maintaining the data structure.

700
00:34:56,980 --> 00:35:05,990
The question is why, Why do this,

701
00:35:06,970 --> 00:35:08,360
so like, if I'm doing a JOIN,

702
00:35:09,370 --> 00:35:11,990
the relationship between the two join tables is that,

703
00:35:12,010 --> 00:35:13,880
one side might not be unique, right,

704
00:35:16,050 --> 00:35:17,170
so I need to have this.

705
00:35:17,430 --> 00:35:18,910
And so I want to get all of the,

706
00:35:19,170 --> 00:35:20,450
you have basically an iterator says,

707
00:35:20,450 --> 00:35:23,140
give me all the values for key equals XYZ

708
00:35:23,460 --> 00:35:27,400
and spitting this things out as I'm as I'm traversing the hash table,

709
00:35:28,415 --> 00:35:29,110
because I'm doing the JOIN.

710
00:35:30,610 --> 00:35:31,010
Yes.

711
00:35:31,180 --> 00:35:34,910
His question is,

712
00:35:34,910 --> 00:35:35,720
would you rehash,

713
00:35:35,720 --> 00:35:37,420
when it's completely for 80% full,

714
00:35:37,590 --> 00:35:39,430
so the different systems have different,

715
00:35:40,940 --> 00:35:42,130
there's like a threshold would say,

716
00:35:42,130 --> 00:35:43,240
if I go above this,

717
00:35:43,240 --> 00:35:45,130
I know I'm gonna overflow or run out of space,

718
00:35:45,130 --> 00:35:46,800
I go ahead and trigger a rehash.

719
00:35:47,150 --> 00:35:54,300
Yeah, point, yes,

720
00:35:54,300 --> 00:35:57,800
like the you get closer to that worst case scenario,

721
00:35:58,060 --> 00:35:59,960
where like if it gets, it starts to get full,

722
00:36:00,190 --> 00:36:02,540
so rather than waiting to it's like 100% full,

723
00:36:02,890 --> 00:36:04,370
maybe go to 80%, because though,

724
00:36:04,600 --> 00:36:07,370
it's better off to pay the penalty to resize the hash table,

725
00:36:07,480 --> 00:36:08,445
which is doubling you,

726
00:36:08,445 --> 00:36:10,400
so resize hash table is double the size of it,

727
00:36:10,810 --> 00:36:11,760
go through all your keys

728
00:36:11,760 --> 00:36:13,170
and rehash them and put them into the new hash table

729
00:36:13,170 --> 00:36:14,090
and then throw away the old one,

730
00:36:14,200 --> 00:36:16,125
that's not, that's expensive,

731
00:36:16,125 --> 00:36:17,090
if it's a large hash table,

732
00:36:17,380 --> 00:36:18,345
so, but there's a tradeoff of,

733
00:36:18,345 --> 00:36:20,090
like, okay, if I'm at 80% full,

734
00:36:20,350 --> 00:36:22,370
I'd rather pay that penalty to double the size

735
00:36:22,660 --> 00:36:25,010
rather than all the additional operations I need to do

736
00:36:25,240 --> 00:36:26,600
spend a long time searching through,

737
00:36:27,900 --> 00:36:29,320
there's no, there's no one answer.

738
00:36:30,390 --> 00:36:34,000
But that's, that's why there's a, there's a, there's usually a tunable threshold,

739
00:36:34,350 --> 00:36:36,430
whether or not they expose, expose that to you

740
00:36:37,080 --> 00:36:38,650
as like a user of the database system,

741
00:36:38,760 --> 00:36:39,670
it depends on the implementation,

742
00:36:40,020 --> 00:36:41,480
but there usually a threshold to say,

743
00:36:41,480 --> 00:36:42,700
when do you want to go ahead and resize.

744
00:36:48,450 --> 00:36:50,680
Okay, so some other optimizations we can do.

745
00:36:51,610 --> 00:36:56,600
That one is you could have different hash table implementations,

746
00:36:57,010 --> 00:37:00,165
that have these different mechanisms or like, you know, decisions

747
00:37:00,165 --> 00:37:02,870
about when to split, how to store things and what not,

748
00:37:03,100 --> 00:37:05,360
based on the data type you're storing.

749
00:37:07,030 --> 00:37:08,570
So an obvious thing would be like,

750
00:37:08,800 --> 00:37:12,620
if I have, I want to be hash tables that support string keys,

751
00:37:13,300 --> 00:37:14,790
if my strings are very small,

752
00:37:14,870 --> 00:37:19,860
like 64 bits 64 bits or 64 bytes or 64 bits or bytes less,

753
00:37:20,120 --> 00:37:21,900
they might store that inline my hash table,

754
00:37:22,040 --> 00:37:23,515
but if it's a really large string,

755
00:37:23,515 --> 00:37:25,050
I don't want to store that in my hash table,

756
00:37:25,490 --> 00:37:28,200
maybe I just want to have a pointer to the actual string itself,

757
00:37:28,610 --> 00:37:30,540
so now I can have a 64 bit pointer,

758
00:37:31,940 --> 00:37:34,045
but now it's going to be expensive to do that lookup

759
00:37:34,045 --> 00:37:35,275
to see whether I have a match,

760
00:37:35,275 --> 00:37:38,310
so maybe I actually want to store the hash of that string

761
00:37:38,720 --> 00:37:40,710
as part of the key in my hash table

762
00:37:40,940 --> 00:37:43,760
to avoid having to do that lookup, right.

763
00:37:46,040 --> 00:37:47,780
We talked about storing the metadata,

764
00:37:47,780 --> 00:37:54,640
like as something a tombstone or something a null value or a or an empty slot,

765
00:37:55,110 --> 00:37:56,710
you could store that in the page header,

766
00:37:56,760 --> 00:37:58,450
because now you have a bunch of packed bits,

767
00:37:58,710 --> 00:38:02,070
you'd actually store that in the entire hash table itself, right.

768
00:38:03,420 --> 00:38:05,270
So the, the Google HashMap does this,

769
00:38:05,270 --> 00:38:07,420
where they have a separate hash table, just for the metadata,

770
00:38:07,500 --> 00:38:09,220
that's much smaller and compact,

771
00:38:09,390 --> 00:38:10,450
you do a lookup on that,

772
00:38:10,650 --> 00:38:14,090
to tell you what is the thing you're about to go look up in the the real hash table,

773
00:38:14,090 --> 00:38:17,320
is that thing you know, null or, or, or empty or not.

774
00:38:19,000 --> 00:38:20,055
And then this one is interesting,

775
00:38:20,055 --> 00:38:21,560
this one comes from Clickhouse,

776
00:38:22,600 --> 00:38:25,310
the OLAP system that came out of the index in Russia,

777
00:38:26,560 --> 00:38:28,220
so they talk about how they want to be,

778
00:38:28,390 --> 00:38:30,620
since it's so expensive to allocate the memory for a hash table,

779
00:38:31,060 --> 00:38:33,710
you don't want to just, you know, allocate a bunch of memory,

780
00:38:33,970 --> 00:38:35,360
use it once and then throw it away,

781
00:38:35,770 --> 00:38:38,390
which you actually want to just reuse that, that memory over and over again,

782
00:38:38,620 --> 00:38:40,310
but you need a fast way to clear it out,

783
00:38:41,240 --> 00:38:44,580
so instead of going through and marking all the slots as deleted,

784
00:38:44,870 --> 00:38:47,640
you just maintain a version counter, a version ID,

785
00:38:48,460 --> 00:38:52,440
and whenever you say I want to delete the contents of this table,

786
00:38:52,790 --> 00:38:54,990
you just increment that version counter on the table,

787
00:38:55,400 --> 00:38:59,380
and then now any lookup you do inside of a slot inside that table,

788
00:38:59,380 --> 00:39:00,750
if the version IDs don't match,

789
00:39:01,190 --> 00:39:05,700
then if the slot version number is less than the table version number,

790
00:39:05,750 --> 00:39:07,030
then you know it's been deleted

791
00:39:07,030 --> 00:39:08,280
and you can ignore everything in there

792
00:39:08,450 --> 00:39:11,790
and that clears it out and then you increment the version ID.

793
00:39:13,260 --> 00:39:15,490
So there a bunch of different tricks you can do in different scenarios

794
00:39:15,510 --> 00:39:16,960
to make these things run more efficiently

795
00:39:17,100 --> 00:39:18,910
and the various systems do different things,

796
00:39:19,350 --> 00:39:20,410
Clickhouse, in my opinion,

797
00:39:20,970 --> 00:39:23,140
there's, that link there will take you to the blog article,

798
00:39:23,460 --> 00:39:26,290
they claim they have 30 different implementations of hash tables,

799
00:39:27,690 --> 00:39:31,210
a lot of it templateized based in C++, based on the data type,

800
00:39:31,350 --> 00:39:33,065
and they do a bunch of compiler tricks

801
00:39:33,065 --> 00:39:35,290
to, to remove code you don't need,

802
00:39:35,790 --> 00:39:37,505
if you know, like something cannot be null

803
00:39:37,505 --> 00:39:39,820
or is a string of certain size,

804
00:39:41,610 --> 00:39:44,110
they probably, in my opinion, of all the source systems I looked at,

805
00:39:44,220 --> 00:39:45,640
they're probably the most sophisticated ones,

806
00:39:45,720 --> 00:39:47,200
that have the most sophisticated hash tables.

807
00:39:50,570 --> 00:39:55,590
All right, so one variant of linear, linear probe hashing is a technique called Cuckoo hashing.

808
00:39:56,590 --> 00:39:57,920
And the idea here is that,

809
00:39:58,450 --> 00:40:00,180
instead of having a single hash function,

810
00:40:00,180 --> 00:40:04,310
to do a look up to one location in my, in my hash table,

811
00:40:05,060 --> 00:40:06,430
what if I had multiple hash functions

812
00:40:07,230 --> 00:40:09,155
and I hash up multiple locations

813
00:40:09,155 --> 00:40:12,790
and I find whatever one has a free slot,

814
00:40:13,400 --> 00:40:14,200
and I use that one,

815
00:40:14,810 --> 00:40:16,230
instead of having to scan through now,

816
00:40:17,180 --> 00:40:19,500
until I find a free slot for my key.

817
00:40:20,670 --> 00:40:25,090
So this is going to guarantee that all my lookup and deletions will be O(1),

818
00:40:25,110 --> 00:40:28,570
because no matter how many hash functions I have,

819
00:40:30,170 --> 00:40:31,375
you know, I don't have to scan through,

820
00:40:31,375 --> 00:40:34,980
I'm going to land at some location in my, in my HashMap or my hash table,

821
00:40:35,000 --> 00:40:36,730
that has the data that I'm looking for,

822
00:40:36,730 --> 00:40:37,650
or it doesn't exist,

823
00:40:39,330 --> 00:40:40,565
insert is going to be more expensive,

824
00:40:40,565 --> 00:40:41,855
because we'll see it in a second,

825
00:40:41,855 --> 00:40:44,170
you may have to start moving things around and reorganizing stuff.

826
00:40:44,770 --> 00:40:47,625
So there's only one system I know that does Cuckoo hashing,

827
00:40:47,625 --> 00:40:48,830
that publicly talks about it,

828
00:40:48,970 --> 00:40:53,930
and that's this overlap accelerator from IBM called BLU B L U,

829
00:40:54,820 --> 00:40:55,470
and in their paper,

830
00:40:55,470 --> 00:40:57,920
they talk about how they make heavy use of of Cuckoo hashing.

831
00:40:58,480 --> 00:40:59,385
And as far as, you know,

832
00:40:59,385 --> 00:41:02,565
the best open-source implementation of a Cuckoo hash table is actually

833
00:41:02,565 --> 00:41:04,700
from David Anderson from CMU,

834
00:41:06,140 --> 00:41:09,420
I think Google, said, David claims Google uses a lot of it.

835
00:41:10,320 --> 00:41:12,550
And so the name has to do, the Cuckoo has to do with.

836
00:41:16,290 --> 00:41:17,500
Okay, it send by Google that, yeah.

837
00:41:21,380 --> 00:41:21,780
So.

838
00:41:23,780 --> 00:41:25,590
So the name Cuckoo has to do with this bird,

839
00:41:25,640 --> 00:41:30,660
where they lay their eggs, they can lay their eggs in another bird's nest, right,

840
00:41:30,980 --> 00:41:31,920
and so the idea is,

841
00:41:31,940 --> 00:41:36,010
my key may end up, may end up [stealing] somebody else's slot in my hash table,

842
00:41:36,010 --> 00:41:38,070
if I try to go there and they're using it.

843
00:41:38,610 --> 00:41:39,380
So let's see examples.

844
00:41:39,380 --> 00:41:41,950
So say we have, again, we have a simple hash table,

845
00:41:42,180 --> 00:41:44,260
but now anytime we do an operation,

846
00:41:44,400 --> 00:41:45,490
we're going to have two hash functions,

847
00:41:45,810 --> 00:41:49,640
so it's going to be the same hash function implementation that we talked about before,

848
00:41:49,640 --> 00:41:52,330
like XXHash, MurmurHash, SpookyHash, it doesn't matter,

849
00:41:52,530 --> 00:41:55,720
but we'll just give it a different seed to the hash function,

850
00:41:55,720 --> 00:41:58,105
that guarantees for a given key, it doesn't guarantee,

851
00:41:58,105 --> 00:42:00,150
but it's very likely that that for a given key,

852
00:42:00,350 --> 00:42:03,270
it's going, it's going to produce two different hash values.

853
00:42:04,600 --> 00:42:05,505
So I hash A,

854
00:42:05,505 --> 00:42:07,050
and I have these two locations here,

855
00:42:07,050 --> 00:42:07,830
so at the very beginning,

856
00:42:07,830 --> 00:42:08,600
my hash table is empty,

857
00:42:09,040 --> 00:42:11,510
so I can either flip a coin or pick the first one, it doesn't matter,

858
00:42:11,740 --> 00:42:14,600
and so I'll decide that for inserting A,

859
00:42:15,130 --> 00:42:16,610
it goes in the first slot here,

860
00:42:17,810 --> 00:42:19,320
now I want to put B in,

861
00:42:20,480 --> 00:42:22,950
and so the first hash function hash to where A is,

862
00:42:23,060 --> 00:42:24,870
the second hash function goes to an empty slot,

863
00:42:25,130 --> 00:42:27,300
so because the other one is occupied,

864
00:42:27,440 --> 00:42:29,065
I'm going to always choose the empty one

865
00:42:29,065 --> 00:42:30,870
and I'll put B at the top like that,

866
00:42:32,500 --> 00:42:33,810
Now where things get tricky is that,

867
00:42:33,810 --> 00:42:39,890
we have multiple, two hash functions or multiple hash functions hash to two locations that both have,

868
00:42:40,960 --> 00:42:42,230
that are both are being occupied,

869
00:42:43,020 --> 00:42:43,930
so in this case here,

870
00:42:44,190 --> 00:42:46,595
for whatever, you know, whatever protocol, whatever scheme we want to use,

871
00:42:46,595 --> 00:42:49,330
say we can flip a coin, we decide we want to evict B,

872
00:42:50,290 --> 00:42:53,330
so we'll go ahead and bash B on the head,

873
00:42:53,410 --> 00:42:55,130
take its location, put C in there,

874
00:42:55,540 --> 00:42:58,670
but now we now we got to put B back in,

875
00:42:59,340 --> 00:43:03,830
so because B landed on this using the second hash function,

876
00:43:04,890 --> 00:43:07,480
after we, after we take it out and put it back in,

877
00:43:07,650 --> 00:43:09,310
we use the first hash function,

878
00:43:10,330 --> 00:43:12,770
but then that takes us to the location where A is located,

879
00:43:13,120 --> 00:43:14,750
so B is allowed to steal from A,

880
00:43:15,040 --> 00:43:17,115
so B goes there, A comes out,

881
00:43:17,115 --> 00:43:18,555
we hash A with a second hash function,

882
00:43:18,555 --> 00:43:20,690
and then we land to another location.

883
00:43:21,890 --> 00:43:24,040
And again, just like before in linear probe hashing,

884
00:43:24,040 --> 00:43:26,700
you need to keep track of, if you're stuck in a loop, right,

885
00:43:26,900 --> 00:43:27,970
so you just got to keep track is,

886
00:43:27,970 --> 00:43:30,535
this is the key, I'm putting in the same key,

887
00:43:30,535 --> 00:43:31,920
I try to first put in the very beginning

888
00:43:32,030 --> 00:43:34,470
and I've I've just loop back around and I'm stuck in an infinite loop

889
00:43:34,700 --> 00:43:36,000
and therefore I need to abort,

890
00:43:36,290 --> 00:43:38,340
double the size of the hash table and rehash everything.

891
00:43:40,620 --> 00:43:42,400
So now when I want to do a lookup on B,

892
00:43:43,820 --> 00:43:45,390
I take B, hash it twice

893
00:43:45,860 --> 00:43:47,695
and I get two different locations

894
00:43:47,695 --> 00:43:49,180
and now I do my check to see,

895
00:43:49,180 --> 00:43:51,960
is the key stored in this slot the key I'm looking for,

896
00:43:52,070 --> 00:43:53,760
if yes, then I have the thing I'm looking for.

897
00:43:54,650 --> 00:43:58,680
Again now, and I don't need to do that linear probe scanning or looking for an empty slot,

898
00:43:58,790 --> 00:43:59,550
key I'm looking for,

899
00:43:59,570 --> 00:44:00,550
because I'm guaranteed,

900
00:44:00,550 --> 00:44:03,180
either the key is going to be there after hashing

901
00:44:03,830 --> 00:44:05,130
or does not exist in the table.

902
00:44:09,050 --> 00:44:10,350
This is a good idea or bad idea?

903
00:44:20,080 --> 00:44:21,590
He said it seems like there would be more collisions.

904
00:44:29,520 --> 00:44:31,030
Well, no, right, because if,

905
00:44:34,450 --> 00:44:35,330
well, so like,

906
00:44:36,220 --> 00:44:36,900
there's trade offs, right,

907
00:44:37,900 --> 00:44:38,910
yes, could be more collisions,

908
00:44:38,910 --> 00:44:40,490
but like, at least in linear probe hashing,

909
00:44:40,660 --> 00:44:43,065
you're guaranteed to always put something in there, right,

910
00:44:43,065 --> 00:44:44,085
it may be in the worst slot,

911
00:44:44,085 --> 00:44:47,415
maybe the slot right above the one you try to go into

912
00:44:47,415 --> 00:44:48,380
and you loop back around,

913
00:44:49,060 --> 00:44:51,080
but at least if there's a free slot, you'll get it.

914
00:44:53,350 --> 00:44:53,750
Yes.

915
00:44:57,150 --> 00:44:58,745
Yeah, he's absolutely right,

916
00:44:58,745 --> 00:44:59,950
so this is doing random IOs,

917
00:45:00,000 --> 00:45:01,000
because I'm jumping around,

918
00:45:01,140 --> 00:45:03,350
the hash table is essentially random IOs,

919
00:45:03,350 --> 00:45:05,860
but once I land somewhere, doing a random lookup,

920
00:45:06,060 --> 00:45:07,120
then a sequential scan,

921
00:45:07,620 --> 00:45:09,950
this is always random, right.

922
00:45:44,900 --> 00:45:51,040
I mean, that's the life.

923
00:45:51,480 --> 00:45:53,830
It's impressive, yeah,

924
00:45:56,040 --> 00:45:58,840
okay. I I mean, you're a good DJ, it's not,

925
00:45:59,760 --> 00:46:01,990
and you do databases, that makes sense, right?

926
00:46:03,900 --> 00:46:06,890
Yeah, all right, congrats.

927
00:46:11,260 --> 00:46:12,110
So your question was?

928
00:46:27,040 --> 00:46:28,010
Yeah, so his question is,

929
00:46:28,360 --> 00:46:31,070
is it possible to parallelize the access to different locations?

930
00:46:33,390 --> 00:46:34,570
Yes, you could do that,

931
00:46:35,460 --> 00:46:37,930
like there's two different ways to parallelize,

932
00:46:38,100 --> 00:46:39,010
we'll eventually get to there,

933
00:46:39,420 --> 00:46:40,930
like you can have multiple threads

934
00:46:41,220 --> 00:46:45,160
or single threads, but do vectorized instructions, SIMD instructions, like,

935
00:46:46,670 --> 00:46:49,350
and for SIMD, this won't,

936
00:46:49,840 --> 00:46:50,755
you could do this,

937
00:46:50,755 --> 00:46:52,840
but it requires you moving data around a bit much,

938
00:46:52,840 --> 00:46:54,000
because you have to make sure things aligned,

939
00:46:55,160 --> 00:46:55,560
but,

940
00:46:56,220 --> 00:46:59,330
so you may introduce with a single thread with vectorized instructions,

941
00:47:00,100 --> 00:47:01,425
I know there's techniques exist,

942
00:47:01,425 --> 00:47:02,570
I don't know about Cuckoo hashing [],

943
00:47:03,400 --> 00:47:05,570
but for, to make this multi threaded,

944
00:47:05,950 --> 00:47:09,350
it'd be so much work or just too much work,

945
00:47:09,550 --> 00:47:12,285
to tell two threads,

946
00:47:12,285 --> 00:47:13,610
okay, we're looking this key,

947
00:47:13,690 --> 00:47:14,490
you hash it this way,

948
00:47:14,490 --> 00:47:15,380
I'll hash it this way,

949
00:47:15,520 --> 00:47:20,570
and then to then synchronize on who produces back result,

950
00:47:20,680 --> 00:47:21,830
that is just not worth it.

951
00:47:22,900 --> 00:47:23,300
Yes.

952
00:47:40,260 --> 00:47:41,260
So your question is,

953
00:47:41,820 --> 00:47:44,950
do you have to guarantee that the hash function.

954
00:47:54,220 --> 00:47:54,620
Yes.

955
00:47:58,310 --> 00:48:01,710
So, so do you have to guarantee your hash function can't do that,

956
00:48:02,900 --> 00:48:03,900
you can't, right,

957
00:48:04,340 --> 00:48:08,125
that's, that's why I'm saying you want to choose a hash function that has a low collision rate,

958
00:48:08,125 --> 00:48:10,560
so the like, you can't guarantee that won't happen,

959
00:48:10,850 --> 00:48:14,750
but the likelihood that that it will happen is low,

960
00:48:16,330 --> 00:48:18,140
the only thing you can get to is a perfect hash function.

961
00:48:21,450 --> 00:48:21,935
His question,

962
00:48:21,935 --> 00:48:23,770
does it default to linear probe hashing, what do you mean?

963
00:48:37,070 --> 00:48:40,830
Oh, if I, if I run out locations for this,

964
00:48:41,480 --> 00:48:43,200
either because all the slots are full

965
00:48:43,340 --> 00:48:46,200
or I get a wrap around when I try to do the cuckoo thing,

966
00:48:47,330 --> 00:48:48,360
you double the size of it.

967
00:49:02,180 --> 00:49:04,385
Yeah, so his question is,

968
00:49:04,385 --> 00:49:05,255
is there a defined order,

969
00:49:05,255 --> 00:49:09,250
such that like you can maybe just always check the first hash,

970
00:49:09,510 --> 00:49:11,105
like I'm showing two lines coming out of it,

971
00:49:11,105 --> 00:49:13,090
but in, in a assume it's not parallel,

972
00:49:13,350 --> 00:49:15,160
it is executing sequentially,

973
00:49:15,330 --> 00:49:19,800
like could I, is there some protocol, say, check this,

974
00:49:20,700 --> 00:49:23,145
and then only fetch the page for the second one,

975
00:49:23,145 --> 00:49:25,010
if I if I know it's not going to be there,

976
00:49:25,240 --> 00:49:26,295
I mean, you can do a bunch of everything,

977
00:49:26,295 --> 00:49:29,010
so you could prefetch the second page, right,

978
00:49:29,240 --> 00:49:30,625
because the hashing is actually cheap,

979
00:49:30,625 --> 00:49:32,370
it's the lookup is expensive, right,

980
00:49:32,870 --> 00:49:36,600
so maybe I I could choose one that,

981
00:49:37,610 --> 00:49:39,810
I have two page IDs I went into look up,

982
00:49:40,220 --> 00:49:42,655
so if I have a way to go which one actually exists first,

983
00:49:42,655 --> 00:49:44,670
maybe go check that one, prefetch the other one,

984
00:49:44,990 --> 00:49:46,800
again, it depends on the implementation.

985
00:49:58,310 --> 00:50:01,510
But, but it depending on what got inserted and how you think, got moving around,

986
00:50:01,510 --> 00:50:06,360
like, you know, but the fact that we're coming up with sort of so many.

987
00:50:10,020 --> 00:50:10,870
Is there another woman,

988
00:50:12,690 --> 00:50:14,195
is going to bathroom,

989
00:50:14,195 --> 00:50:14,770
what is he doing?

990
00:50:18,700 --> 00:50:19,725
Anyway, yeah.

991
00:50:19,725 --> 00:50:21,495
So the fact we coming of different ways to do this,

992
00:50:21,495 --> 00:50:23,505
it shows how complicated is where linear probe hashing,

993
00:50:23,505 --> 00:50:25,610
you just kind of [] through it, right?

994
00:50:31,760 --> 00:50:32,365
This question is,

995
00:50:32,365 --> 00:50:34,165
in a general system, what, what is the default,

996
00:50:34,165 --> 00:50:37,140
I don't know, we can go look up on David's code,

997
00:50:37,400 --> 00:50:38,280
default, it might be three,

998
00:50:39,160 --> 00:50:40,320
I have no idea, yeah.

999
00:50:41,830 --> 00:50:42,230
All Right.

1000
00:50:43,130 --> 00:50:46,660
I want to get through the chain hashing,

1001
00:50:46,660 --> 00:50:48,960
because we'll need this for one of the projects.

1002
00:50:50,710 --> 00:50:53,130
So again, all of these protocols I've showed so far,

1003
00:50:53,130 --> 00:50:54,830
these are all static hashing schemes,

1004
00:50:55,000 --> 00:50:57,800
again, if we run out of space or we loop back around,

1005
00:50:58,060 --> 00:51:01,705
then we need to double the size of the hash table and repopulate it,

1006
00:51:01,705 --> 00:51:02,310
and that's expensive.

1007
00:51:03,080 --> 00:51:07,500
So we will now talk about different techniques to incrementally resize the hash table

1008
00:51:07,760 --> 00:51:09,780
without having to rebuild the entire thing.

1009
00:51:10,610 --> 00:51:12,745
So the most common one going to be chain hashing,

1010
00:51:12,745 --> 00:51:15,150
and again, this is what most people think of when you think about hash tables sometimes

1011
00:51:16,460 --> 00:51:18,360
and then, but we'll look at two more advanced techniques,

1012
00:51:19,010 --> 00:51:21,930
that actually are used in real systems.

1013
00:51:24,310 --> 00:51:26,420
So chain hashing, the basic idea is that,

1014
00:51:27,150 --> 00:51:31,630
instead of having this giant array of all the slots where we actually insert keys,

1015
00:51:33,300 --> 00:51:39,010
array is just going to be pointers to essentially link lists or chains or buckets,

1016
00:51:39,270 --> 00:51:44,060
where all the keys that map to that slot in our hash table

1017
00:51:44,290 --> 00:51:47,550
will be found in that in that linked list, right,

1018
00:51:47,960 --> 00:51:49,740
if you allocate a hashmap in Java,

1019
00:51:49,940 --> 00:51:51,000
this is essentially what you get.

1020
00:51:51,750 --> 00:51:54,730
And so the, the linked list part can essentially grow infinitely,

1021
00:51:55,080 --> 00:51:56,630
because again, in the worst case scenario,

1022
00:51:56,630 --> 00:51:58,240
all my keys hash to the same slot,

1023
00:51:58,440 --> 00:52:00,400
I'm, I'm just appending to this giant list

1024
00:52:00,810 --> 00:52:04,480
and I'm, I'm falling down or basically end up with a sequential scan,

1025
00:52:04,860 --> 00:52:07,450
but again, ideally if I have a hash function that's good,

1026
00:52:07,680 --> 00:52:10,660
I won't, I'll have a good distribution of keys.

1027
00:52:11,620 --> 00:52:12,740
The way to think about this is,

1028
00:52:12,820 --> 00:52:15,530
we're essentially partitioning our giant hash table we have before

1029
00:52:15,760 --> 00:52:19,760
into smaller hash tables themselves, or smaller tables,

1030
00:52:20,530 --> 00:52:23,720
we can get unique keys doing the same tricks we did before,

1031
00:52:24,640 --> 00:52:27,410
just keep appending the redundant keys to this giant list,

1032
00:52:28,240 --> 00:52:29,360
we can still use tombstones,

1033
00:52:29,410 --> 00:52:32,510
but oftentimes compaction is just faster in this case.

1034
00:52:34,300 --> 00:52:36,890
All right, so now again we have, here we have our bucket pointers

1035
00:52:37,060 --> 00:52:39,680
and this is where hash functions are going to hash into,

1036
00:52:40,000 --> 00:52:44,120
and then these are just be pointers to the different buckets that exist,

1037
00:52:44,800 --> 00:52:46,160
so if we want to put A in,

1038
00:52:46,240 --> 00:52:49,010
we hash it, mod it by the number of bucket pointers we have

1039
00:52:49,240 --> 00:52:50,390
and then we land in that bucket,

1040
00:52:50,440 --> 00:52:54,750
we find the first free slot and we just insert it, right.

1041
00:52:55,520 --> 00:52:56,525
Same thing, going to put B,

1042
00:52:56,525 --> 00:52:58,600
B goes to the top here just as before,

1043
00:52:59,840 --> 00:53:01,470
and then now, in case C,

1044
00:53:01,790 --> 00:53:05,670
C hashes to the same bucket where A is located,

1045
00:53:05,930 --> 00:53:07,375
we just scan through sequentially

1046
00:53:07,375 --> 00:53:09,390
until we find the first free slot.

1047
00:53:12,730 --> 00:53:13,310
Take a call?

1048
00:53:17,760 --> 00:53:20,930
You put D and d goes where where A is,

1049
00:53:21,100 --> 00:53:23,210
it scans through all the slots are empty,

1050
00:53:23,380 --> 00:53:24,585
you can put something in the page header

1051
00:53:24,585 --> 00:53:25,820
and say I have no more free slots,

1052
00:53:25,900 --> 00:53:28,490
therefore, always expand me, when you get to me, it doesn't matter,

1053
00:53:28,930 --> 00:53:33,170
and then basically this page here will then point to another page,

1054
00:53:33,520 --> 00:53:34,490
where you can find D.

1055
00:53:35,010 --> 00:53:36,040
And then we want to put E

1056
00:53:36,450 --> 00:53:39,400
and follow through until we find E here, right.

1057
00:53:40,430 --> 00:53:41,640
And f can go here.

1058
00:53:43,540 --> 00:53:45,210
So again, the nice thing about this is that,

1059
00:53:45,210 --> 00:53:49,430
I can grow the the key list within a bucket

1060
00:53:49,690 --> 00:53:53,560
without affecting other parts of the, of the table,

1061
00:53:54,260 --> 00:53:57,980
but, you can have a sort of like two level, two level hash tables,

1062
00:53:57,980 --> 00:53:59,450
where like this is a hash table,

1063
00:53:59,450 --> 00:54:01,990
that takes you into these buckets another hash table,

1064
00:54:02,790 --> 00:54:05,290
but simplicity, we're just showing it as as a linked list like this.

1065
00:54:06,890 --> 00:54:07,615
But doing that,

1066
00:54:07,615 --> 00:54:08,220
yes, question.

1067
00:54:12,240 --> 00:54:12,770
His question is,

1068
00:54:12,770 --> 00:54:13,580
when you create a new bucket,

1069
00:54:13,580 --> 00:54:14,920
how do you determine the size of it?

1070
00:54:17,780 --> 00:54:18,750
We're not talking about whether,

1071
00:54:19,040 --> 00:54:20,500
for this lecture, we haven't talked about whether something,

1072
00:54:20,500 --> 00:54:23,580
like backed by pages from on disk or in memory,

1073
00:54:24,020 --> 00:54:25,140
but like if it is a,

1074
00:54:25,400 --> 00:54:29,460
assume it's backed by pages that are on disk and on buffer pool,

1075
00:54:29,750 --> 00:54:31,830
so if whatever the page size in the database,

1076
00:54:32,000 --> 00:54:33,450
that'll be the page size of a bucket,

1077
00:54:34,620 --> 00:54:36,190
so in Postgres is 8 kilobytes,

1078
00:54:36,750 --> 00:54:38,530
MySQL is 16 kilobytes, right.

1079
00:54:41,050 --> 00:54:43,365
I'm showing within one page two, two keys,

1080
00:54:43,365 --> 00:54:44,540
because it's powerpoint.

1081
00:54:46,750 --> 00:54:50,000
So again, if I have a lot of keys, hash in the same location,

1082
00:54:50,320 --> 00:54:52,820
this linear scan here can be expensive,

1083
00:54:53,350 --> 00:54:55,820
so actually a really simple optimization you can do is,

1084
00:54:56,290 --> 00:54:57,650
in your bucket pointer list,

1085
00:54:57,970 --> 00:54:59,600
you also store a Bloom filter,

1086
00:55:00,840 --> 00:55:03,910
that just tells you whether a key exists in in my linked list,

1087
00:55:04,170 --> 00:55:05,710
so if I want to look up now G,

1088
00:55:06,060 --> 00:55:07,415
I first check the Bloom filter,

1089
00:55:07,415 --> 00:55:08,830
I ask it whether it exists or not,

1090
00:55:09,400 --> 00:55:12,010
if yes, then then I'll keep following the pointer

1091
00:55:12,010 --> 00:55:14,310
and go, then scan along to I find the thing I'm looking for,

1092
00:55:14,570 --> 00:55:16,715
if not, if it says no,

1093
00:55:16,715 --> 00:55:17,740
then I don't do that scan,

1094
00:55:19,220 --> 00:55:21,480
so that avoids that, having to do that extra traversal.

1095
00:55:24,260 --> 00:55:25,500
Do everyone know what a Bloom filter is?

1096
00:55:26,730 --> 00:55:27,520
No, okay,

1097
00:55:28,930 --> 00:55:29,870
so I asked, Hold up.

1098
00:55:31,600 --> 00:55:32,550
Bloom filters are awesome

1099
00:55:32,550 --> 00:55:34,610
and be useful for a bunch of things.

1100
00:55:36,000 --> 00:55:36,970
All right, quickly,

1101
00:55:37,170 --> 00:55:39,100
a Bloom filter is a probabilistic data structure,

1102
00:55:39,390 --> 00:55:42,100
that can tell you that can answer set membership queries,

1103
00:55:42,570 --> 00:55:44,830
so a filter is different than an index,

1104
00:55:44,940 --> 00:55:46,250
an index tells you,

1105
00:55:46,250 --> 00:55:48,340
for a given key, where is it,

1106
00:55:48,690 --> 00:55:50,680
in this record ID, in this page,

1107
00:55:50,940 --> 00:55:52,300
a filter can only say,

1108
00:55:52,560 --> 00:55:54,310
does the key exist, yes or no,

1109
00:55:54,450 --> 00:55:55,570
can't tell you where it is,

1110
00:55:55,590 --> 00:55:57,990
it just tells you whether it exists, right,

1111
00:55:58,610 --> 00:56:01,680
so a, a Bloom filter,

1112
00:56:02,480 --> 00:56:04,950
the guy was named Bloom, I think from the 70s,

1113
00:56:05,990 --> 00:56:09,480
so the the bloom filterter is a probabilistic data structure,

1114
00:56:09,770 --> 00:56:13,590
meaning like, it can tell you with with hundred percent percent correctness,

1115
00:56:13,730 --> 00:56:15,150
that a key does not exist,

1116
00:56:15,830 --> 00:56:18,190
but if you say, it can tell you that a key does exist

1117
00:56:18,190 --> 00:56:19,350
and it might actually be wrong

1118
00:56:20,120 --> 00:56:21,510
and give you false positives,

1119
00:56:22,710 --> 00:56:25,450
and you can only do two operations on the basic Bloom filter,

1120
00:56:25,500 --> 00:56:27,530
you can do an insert and you can do a lookup,

1121
00:56:27,530 --> 00:56:28,600
you can't do deletes,

1122
00:56:29,320 --> 00:56:29,990
we'll see why.

1123
00:56:31,200 --> 00:56:34,830
So it basically just think of like most of all it just a bitmap, right,

1124
00:56:35,560 --> 00:56:40,710
and a bit will be set based on the keys that get inserted,

1125
00:56:41,360 --> 00:56:44,130
so say I start inserting members of the Wu-Tang Clan,

1126
00:56:44,960 --> 00:56:45,990
so I insert RZA

1127
00:56:46,130 --> 00:56:49,050
and so I'll have the hash functions,

1128
00:56:49,790 --> 00:56:50,880
I'll hash it again,

1129
00:56:50,990 --> 00:56:52,950
same hash implementation, just a different seed,

1130
00:56:53,330 --> 00:56:54,630
I get some hash value out

1131
00:56:54,920 --> 00:56:59,220
and then I mod it by the number of bits that I have in my my Bloom filter,

1132
00:56:59,690 --> 00:57:02,850
and then whatever that number is, I set those bits to 1,

1133
00:57:04,210 --> 00:57:05,270
flip it from 0 to 1,

1134
00:57:06,030 --> 00:57:06,970
I insert GZA,

1135
00:57:07,230 --> 00:57:08,650
same thing, hash it,

1136
00:57:09,090 --> 00:57:11,710
mod by the number of bits and set those bits to 1,

1137
00:57:12,610 --> 00:57:14,660
now if I want to do a lookup on RZA,

1138
00:57:15,790 --> 00:57:17,480
same thing, I just do a,

1139
00:57:18,100 --> 00:57:20,910
hash the key I'm looking for, mod the number

1140
00:57:20,910 --> 00:57:22,245
and then I go check to see,

1141
00:57:22,245 --> 00:57:26,720
whether all the bits, bit locations that I I've hashed to,

1142
00:57:26,980 --> 00:57:28,070
if they're set to 1,

1143
00:57:28,330 --> 00:57:29,390
if they're set to 1,

1144
00:57:29,440 --> 00:57:32,390
then I know this was set,

1145
00:57:34,960 --> 00:57:35,680
if it's set to 1,

1146
00:57:35,680 --> 00:57:38,170
then I I I think it could exist,

1147
00:57:38,170 --> 00:57:39,025
but I could be wrong,

1148
00:57:39,025 --> 00:57:42,780
because something else might have set those bits, right,

1149
00:57:42,830 --> 00:57:44,040
so I'll get back TRUE for this,

1150
00:57:44,300 --> 00:57:46,170
if I do lookup Raekwon, the Chef,

1151
00:57:46,920 --> 00:57:48,120
again, when I do a look up,

1152
00:57:48,120 --> 00:57:49,580
one of the bits is set to 0,

1153
00:57:49,930 --> 00:57:51,645
so I know that cannot have been inserted,

1154
00:57:51,645 --> 00:57:54,290
because otherwise one of those bits, all those bits would have been set,

1155
00:57:54,400 --> 00:57:55,160
so I get FALSE,

1156
00:57:55,540 --> 00:57:57,530
but I lookup ODB, rest in peace,

1157
00:57:57,970 --> 00:57:59,505
again, now I can get a false positive,

1158
00:57:59,505 --> 00:58:00,720
because I never inserted it,

1159
00:58:00,720 --> 00:58:02,750
but his bits were set to 1,

1160
00:58:03,310 --> 00:58:07,080
therefore it's TRUE, but it's actually wrong.

1161
00:58:08,280 --> 00:58:11,650
So you can put that Bloom filter in front of your bucket chain,

1162
00:58:12,400 --> 00:58:16,890
and that, and it'll be populated with the bits set for the keys that are actually inserted into it,

1163
00:58:16,940 --> 00:58:18,370
and I can maintain it incrementally,

1164
00:58:18,370 --> 00:58:22,380
because every time I insert a new key into that bucket list,

1165
00:58:22,460 --> 00:58:24,860
I update my Bloom filter, right.

1166
00:58:25,720 --> 00:58:27,240
There's different variations of Bloom filters,

1167
00:58:27,240 --> 00:58:28,245
you can have different levels of them,

1168
00:58:28,245 --> 00:58:29,240
you have decaying ones,

1169
00:58:29,650 --> 00:58:32,300
the size of the Bloom filters can, can vary them hash functions,

1170
00:58:32,380 --> 00:58:33,270
there's a whole bunch of different things,

1171
00:58:33,270 --> 00:58:35,070
but like this data structure is super useful,

1172
00:58:35,070 --> 00:58:36,330
as we use all throughout the system.

1173
00:58:36,330 --> 00:58:40,360
Yes. His question is,

1174
00:58:40,360 --> 00:58:41,860
how does the rate false, rate false positive change,

1175
00:58:41,860 --> 00:58:42,625
as you [set] the Bloom filter,

1176
00:58:42,625 --> 00:58:43,990
there's some formula that says,

1177
00:58:43,990 --> 00:58:46,890
like for if you want like a 1% false positive rate,

1178
00:58:46,970 --> 00:58:48,540
you need to have a Bloom filter of this size

1179
00:58:48,590 --> 00:58:50,395
and, and, but this [] hash function.

1180
00:58:50,395 --> 00:58:53,910
His question is exponential or linear,

1181
00:58:53,990 --> 00:58:55,180
I don't know, I don't remember,

1182
00:58:55,180 --> 00:58:55,830
but there,

1183
00:58:57,060 --> 00:58:59,230
this website here, the Bloom Filter Calculator,

1184
00:58:59,250 --> 00:59:00,790
you say what false positive rate you want,

1185
00:59:00,870 --> 00:59:01,655
how many keys you have

1186
00:59:01,655 --> 00:59:04,060
and it'll tell you the size of the Bloom you want

1187
00:59:04,290 --> 00:59:06,790
and then the number of hash functions.

1188
00:59:07,690 --> 00:59:08,090
Yes.

1189
00:59:10,920 --> 00:59:12,670
The question is, how does Bloom filter deletion,

1190
00:59:13,010 --> 00:59:14,950
they don't, right,

1191
00:59:16,850 --> 00:59:20,100
there are variations of them with multi levels you can do it,

1192
00:59:20,270 --> 00:59:21,870
for basic one, they don't.

1193
00:59:23,500 --> 00:59:24,860
And Bloom filter are super useful,

1194
00:59:24,970 --> 00:59:27,585
we'll use throughout the system in a bunch of ways,

1195
00:59:27,585 --> 00:59:28,850
we, we'll use it for hash Joins.

1196
00:59:29,880 --> 00:59:31,580
Because again, it's a lot cheaper to go look up,

1197
00:59:31,580 --> 00:59:33,010
to see is it in my Bloom filter,

1198
00:59:33,330 --> 00:59:36,380
than go actually follow a page and look on disk

1199
00:59:36,380 --> 00:59:37,570
and see whether something exists or not.

1200
00:59:44,240 --> 00:59:47,040
So a more sophisticated scheme is called extendible hashing,

1201
00:59:47,640 --> 00:59:49,155
and this is going to be like chain hashing,

1202
00:59:49,155 --> 00:59:53,060
but, we're gonna allow the, the,

1203
00:59:53,290 --> 00:59:54,495
we're gonna be able to split the buckets

1204
00:59:54,495 --> 00:59:57,500
to avoid these infinitely long bucket lists

1205
00:59:57,790 --> 00:59:58,875
and we're going to split it in such a way,

1206
00:59:58,875 --> 01:00:03,645
that we only, we only need to do it incrementally in a small part of the hash table

1207
01:00:03,645 --> 01:00:05,630
rather than having to re-rehash everything.

1208
01:00:06,560 --> 01:00:08,515
And the key idea of this is going to work is that,

1209
01:00:08,515 --> 01:00:11,100
we're going to expand the number of bits we have to look at,

1210
01:00:11,450 --> 01:00:14,400
when we do lookups in our in our bucket list or bucket hash table

1211
01:00:14,750 --> 01:00:17,310
to go find the, the bucket chain that we're looking for,

1212
01:00:17,660 --> 01:00:24,915
and we can vary this per, sort of per value, per key type,

1213
01:00:24,915 --> 01:00:30,560
not key type, we can vary this based on what bucket list we're looking at,

1214
01:00:30,940 --> 01:00:32,390
so it may be the case that,

1215
01:00:32,980 --> 01:00:36,990
two different locations, multiple locations in our in our bucket array will point to the same bucket list,

1216
01:00:37,220 --> 01:00:38,695
but then that can expand and break up

1217
01:00:38,695 --> 01:00:40,050
as we need it as we go along.

1218
01:00:40,810 --> 01:00:42,960
So I I didn't actually think,

1219
01:00:42,960 --> 01:00:44,300
this is, this is a bit complicated

1220
01:00:44,350 --> 01:00:46,370
and I didn't think any system actually uses it,

1221
01:00:46,600 --> 01:00:50,820
but it turns out GDBM, which is the GNU database manager,

1222
01:00:50,820 --> 01:00:53,600
think of like you know a key value store,

1223
01:00:54,040 --> 01:00:56,355
that like sort of like RocksDB or SQLite,

1224
01:00:56,355 --> 01:00:57,830
you can run this and embedded in your system,

1225
01:00:58,720 --> 01:01:00,650
that database tie you on like extendible hash tables

1226
01:01:01,030 --> 01:01:05,060
and then AsteriskDB is a, is a big data project at a UC Irvine

1227
01:01:05,650 --> 01:01:09,100
and they have and they're using extendible hashing in their implementation, right.

1228
01:01:10,240 --> 01:01:10,860
So let's see how this works.

1229
01:01:12,090 --> 01:01:13,565
All right, so the first thing we're going to have is that,

1230
01:01:13,565 --> 01:01:14,855
we have our slot array

1231
01:01:14,855 --> 01:01:16,000
and it's going to point to our bucket list,

1232
01:01:16,740 --> 01:01:19,670
and then we're going to have this global identifier,

1233
01:01:19,900 --> 01:01:23,120
that tells us how many bits we need to look at for our hash values

1234
01:01:23,410 --> 01:01:27,020
to determine how we do our lookups in our bucket array,

1235
01:01:27,860 --> 01:01:29,940
and then for sort of bookkeeping reasons,

1236
01:01:30,230 --> 01:01:34,590
every bucket list as well will also have what our local bit bit size is,

1237
01:01:34,610 --> 01:01:35,760
number bits they need to look at.

1238
01:01:36,260 --> 01:01:37,390
So you can see in the case here,

1239
01:01:38,370 --> 01:01:39,760
these first two slots here,

1240
01:01:39,840 --> 01:01:42,010
they're both going to be pointing to the same bucket list,

1241
01:01:42,150 --> 01:01:43,480
whereas these two ones at the bottom,

1242
01:01:43,740 --> 01:01:45,250
they're going to be pointing to different locations,

1243
01:01:46,770 --> 01:01:48,160
and this is because the,

1244
01:01:50,540 --> 01:01:51,420
we need to look at,

1245
01:01:51,680 --> 01:01:53,160
globally, we're going to look at two bits,

1246
01:01:53,540 --> 01:01:56,730
but for the, the first two entries,

1247
01:01:57,050 --> 01:02:00,060
when the when the bit is 0,

1248
01:02:00,740 --> 01:02:02,850
they're going to reuse the same, the same bucket list,

1249
01:02:03,140 --> 01:02:05,760
identified by the the local identifier up here.

1250
01:02:07,800 --> 01:02:11,050
So let's say now I want to do a look up on on this key here,

1251
01:02:11,280 --> 01:02:12,160
I hash it,

1252
01:02:12,420 --> 01:02:14,315
and then look at the top two bits,

1253
01:02:14,315 --> 01:02:17,440
because that's what set my my global identifier, global counter,

1254
01:02:18,570 --> 01:02:19,865
and then I hash to this location,

1255
01:02:19,865 --> 01:02:20,920
I just follow the pointer

1256
01:02:20,970 --> 01:02:22,145
and I land in that bucket

1257
01:02:22,145 --> 01:02:23,885
and I can just do the linear search

1258
01:02:23,885 --> 01:02:25,060
to find the thing I'm looking for.

1259
01:02:27,360 --> 01:02:28,450
Say now I want to put B,

1260
01:02:30,170 --> 01:02:33,390
B, again, globally, I know I need to look at the top two bits,

1261
01:02:33,770 --> 01:02:37,050
I do a look up at my, my [], my bucket, my bucket list,

1262
01:02:37,640 --> 01:02:38,520
based on those two bits,

1263
01:02:38,900 --> 01:02:40,345
then I land into this location here,

1264
01:02:40,345 --> 01:02:41,910
I go ahead and insert it.

1265
01:02:42,850 --> 01:02:44,150
But now I want to put C in,

1266
01:02:44,840 --> 01:02:46,990
and if I look at the last two bits,

1267
01:02:47,250 --> 01:02:49,270
it lands in the same location when I inserted B,

1268
01:02:49,530 --> 01:02:51,160
but now this bucket is full,

1269
01:02:51,600 --> 01:02:52,990
I can't put any more entries in,

1270
01:02:53,490 --> 01:02:56,045
so I need to expand the number of bits I'm looking at

1271
01:02:56,045 --> 01:02:58,840
to now expand the number of options that I have,

1272
01:02:59,220 --> 01:03:02,320
so I'm going to increment the global counter to from 2 to 3,

1273
01:03:03,060 --> 01:03:07,940
I'm going to double the size of the number of pointers I have in my bucket array,

1274
01:03:08,690 --> 01:03:11,340
but then create the new entry,

1275
01:03:11,510 --> 01:03:15,060
but then the when the bit is set to 0,

1276
01:03:15,320 --> 01:03:17,790
they're all still going to point to the first bucket here,

1277
01:03:17,990 --> 01:03:19,470
because I haven't that one yet,

1278
01:03:19,610 --> 01:03:21,390
so I I need to look at one bit for that,

1279
01:03:22,100 --> 01:03:25,620
for the next, when the bits are 111,

1280
01:03:26,000 --> 01:03:28,020
that points to this other bucket down here

1281
01:03:28,250 --> 01:03:31,000
and the same thing for these other ones, right.

1282
01:03:31,690 --> 01:03:33,800
So now when I want to do a lookup to put C in,

1283
01:03:33,880 --> 01:03:35,090
I need to look at three bits,

1284
01:03:35,380 --> 01:03:36,735
I followed that pointer here,

1285
01:03:36,735 --> 01:03:38,360
that then takes me to this bucket location.

1286
01:03:41,270 --> 01:03:42,010
So going back here,

1287
01:03:42,010 --> 01:03:42,960
when I did my split,

1288
01:03:43,750 --> 01:03:44,900
I had to resize,

1289
01:03:45,490 --> 01:03:47,310
these guys just slid down

1290
01:03:47,310 --> 01:03:50,060
and I only had to insert one new bucket,

1291
01:03:51,070 --> 01:03:52,460
but I took what was here,

1292
01:03:52,600 --> 01:03:53,510
because this one was full

1293
01:03:53,590 --> 01:03:54,690
and I just split that one

1294
01:03:54,690 --> 01:03:55,820
and created the new bucket for it,

1295
01:03:55,930 --> 01:03:57,285
I didn't have to touch the one at the bottom

1296
01:03:57,285 --> 01:03:58,490
and didn't have to touch the one at the top,

1297
01:04:00,450 --> 01:04:01,940
I do have to double the size of this,

1298
01:04:01,940 --> 01:04:03,010
but that,

1299
01:04:05,230 --> 01:04:06,765
you have to take a latch on it when you do it,

1300
01:04:06,765 --> 01:04:09,620
because you have to make a copy and resize it,

1301
01:04:09,670 --> 01:04:11,450
but it's not It's not that big of a deal,

1302
01:04:11,770 --> 01:04:12,710
you can do that pretty quickly.

1303
01:04:18,220 --> 01:04:19,070
Any questions about this?

1304
01:04:21,760 --> 01:04:22,430
Is this good?

1305
01:04:25,580 --> 01:04:28,660
So, resizing this slot array is relatively cheap,

1306
01:04:30,570 --> 01:04:31,870
it's clever, it's a good idea,

1307
01:04:33,180 --> 01:04:34,205
it's a clever idea,

1308
01:04:34,205 --> 01:04:35,230
whether or not it's good or not,

1309
01:04:36,740 --> 01:04:39,255
it, engineering wise,

1310
01:04:39,255 --> 01:04:41,460
it's a bit tricky to keep track of all the metadata,

1311
01:04:41,460 --> 01:04:47,130
where like you know what, what bits I need to be looking at as a hash into it,

1312
01:04:48,470 --> 01:04:50,610
but it's basically just chain hashing,

1313
01:04:50,660 --> 01:04:54,070
so all the benefits I get from chain hashing are applicable here,

1314
01:04:54,070 --> 01:04:56,245
I have an extra mechanism now to split things up,

1315
01:04:56,245 --> 01:05:00,770
so I don't have this infinitely growing, you know, linked list,

1316
01:05:02,380 --> 01:05:05,330
so it's just a way to handle incremental resizing

1317
01:05:05,740 --> 01:05:08,720
in a way you couldn't do in regular chain hashing.

1318
01:05:10,530 --> 01:05:10,930
Yes.

1319
01:05:13,830 --> 01:05:14,380
I think.

1320
01:05:14,970 --> 01:05:18,040
Yeah, again, linear probing is probably the easiest thing to do

1321
01:05:18,360 --> 01:05:21,280
and lock the whole table and double the size of it,

1322
01:05:22,620 --> 01:05:23,180
sometimes it's good enough.

1323
01:05:27,640 --> 01:05:31,190
All right, so the last one is linear hashing,

1324
01:05:32,110 --> 01:05:33,320
and this is actually what Postgres does,

1325
01:05:34,300 --> 01:05:35,750
something very close to this.

1326
01:05:36,440 --> 01:05:37,660
And the reason why Postgres,

1327
01:05:38,610 --> 01:05:40,990
well, there another about Berkeley DB that also does this,

1328
01:05:42,810 --> 01:05:46,150
the the company that built Berkeley DB was a company called Sleepycat Software,

1329
01:05:46,800 --> 01:05:49,030
so the people that built WiredTiger,

1330
01:05:49,260 --> 01:05:53,495
they, they, they, they originally started Sleepycat, that got sold to Oracle,

1331
01:05:53,495 --> 01:05:55,300
so Oracle own Berkeley DB

1332
01:05:55,560 --> 01:05:56,980
and then they went out and started the new company

1333
01:05:57,120 --> 01:05:58,955
instead of calling a Sleepycat called a WiredTiger,

1334
01:05:58,955 --> 01:06:02,650
like a like you know, tiger on [] or whatever,

1335
01:06:03,600 --> 01:06:04,895
it was trying to be the opposite,

1336
01:06:04,895 --> 01:06:09,940
but, but the the woman that wrote the linear hashing implementation in Postgres in the early 90s,

1337
01:06:10,260 --> 01:06:11,740
was the founder of Berkeley DB,

1338
01:06:11,790 --> 01:06:12,860
so she wrote it for Postgres

1339
01:06:12,860 --> 01:06:13,900
and then wrote it for Berkeley DB,

1340
01:06:14,360 --> 01:06:20,140
all right, and she was one of Stonebraker [] students, at Berkeley.

1341
01:06:20,490 --> 01:06:25,900
So the way, linear hashing can be more complicated than extendible hashing, potentially,

1342
01:06:25,920 --> 01:06:28,535
but, the basic idea is that,

1343
01:06:28,535 --> 01:06:36,060
we're going to keep track of the next, next bucket list we want to split,

1344
01:06:36,740 --> 01:06:42,035
and that when any time there's an overflow in our, in our bucket list chain

1345
01:06:42,035 --> 01:06:43,210
and anywhere in our hash table,

1346
01:06:43,740 --> 01:06:45,670
whatever we're pointing at with our split pointer,

1347
01:06:45,720 --> 01:06:47,470
that's the one we're going to split.

1348
01:06:48,100 --> 01:06:49,165
And the idea here is that,

1349
01:06:49,165 --> 01:06:50,760
again, we want to do this incrementally

1350
01:06:51,110 --> 01:06:53,280
and not have to lock the whole table while we resize,

1351
01:06:53,570 --> 01:06:55,920
so we can make small changes as we go along.

1352
01:06:56,520 --> 01:06:57,330
And the idea here is,

1353
01:06:57,330 --> 01:06:59,060
again, you're amortizing the cost of resizing,

1354
01:06:59,110 --> 01:07:01,500
so like, it's sort of shared across multiple workers,

1355
01:07:01,500 --> 01:07:04,490
so there's only one worker who's the unlucky one that shows up,

1356
01:07:04,600 --> 01:07:05,570
tries to insert something,

1357
01:07:05,950 --> 01:07:08,175
and then, you know, they draw the short straw

1358
01:07:08,175 --> 01:07:09,800
and they're responsible for resizing the whole thing,

1359
01:07:09,910 --> 01:07:12,075
you do it incrementally as you go along,

1360
01:07:12,075 --> 01:07:13,370
and that sort of smooths out performance.

1361
01:07:14,460 --> 01:07:16,190
So again, the idea here is that,

1362
01:07:16,295 --> 01:07:20,190
we're going to, we're gonna, we're gonna be able to split a,

1363
01:07:21,590 --> 01:07:23,370
split whatever the next one we need is split,

1364
01:07:23,450 --> 01:07:24,940
which may not be the one that overflowed,

1365
01:07:24,940 --> 01:07:28,320
it should be whatever the next one is in, in our incremental order,

1366
01:07:28,910 --> 01:07:31,540
and then we'll have maintain multiple hash functions,

1367
01:07:31,540 --> 01:07:33,060
that that are gonna help us determine,

1368
01:07:35,380 --> 01:07:39,560
which location within our bucket list we should be looking at.

1369
01:07:40,720 --> 01:07:42,380
Let me share the diagram and this makes more sense.

1370
01:07:43,850 --> 01:07:44,740
So again, just look before,

1371
01:07:44,740 --> 01:07:46,470
we have, we have our bucket list here,

1372
01:07:46,640 --> 01:07:49,770
and that's going to map to bucket chains,

1373
01:07:50,500 --> 01:07:51,760
and then we're going have a split pointer,

1374
01:07:51,760 --> 01:07:52,240
that's going to say,

1375
01:07:52,240 --> 01:07:54,030
here's the next thing we want to split anytime,

1376
01:07:54,500 --> 01:07:56,490
anything overflows in our hash, hash table,

1377
01:07:57,290 --> 01:07:58,800
and then we have, at the very beginning,

1378
01:07:59,000 --> 01:08:00,250
we assume we have one hash function,

1379
01:08:00,250 --> 01:08:07,440
that's just the key, the key mod by n, for simplicity reasons,

1380
01:08:07,670 --> 01:08:11,575
but again, assuming it's taking any arbitrary string or any arbitrary byte sequence

1381
01:08:11,575 --> 01:08:13,080
and spitting out integer.

1382
01:08:14,160 --> 01:08:16,180
So say I want to, I want to get 6,

1383
01:08:17,280 --> 01:08:19,940
I do my lookup and at 2

1384
01:08:19,940 --> 01:08:21,790
and I follow along and I find the key I'm looking for,

1385
01:08:22,650 --> 01:08:24,430
that looks just like before, nothing special,

1386
01:08:25,020 --> 01:08:26,080
but now I want to put 17

1387
01:08:26,760 --> 01:08:29,440
and it should go into this bucket here,

1388
01:08:29,730 --> 01:08:30,850
but that thing is full,

1389
01:08:31,400 --> 01:08:33,040
so we're just going to do an overflow,

1390
01:08:33,600 --> 01:08:36,820
just like chain hashing to extend it with another, another bucket

1391
01:08:37,080 --> 01:08:41,530
and insert it into the, insert it into that new page,

1392
01:08:42,150 --> 01:08:44,290
but now, because we overflowed,

1393
01:08:45,030 --> 01:08:48,010
we need to split whatever the split pointer I was pointing at,

1394
01:08:48,640 --> 01:08:49,345
so in this case here,

1395
01:08:49,345 --> 01:08:53,160
it's pointing to bucket 0, bucket list 0,

1396
01:08:53,660 --> 01:08:55,050
even though that didn't overflow,

1397
01:08:56,080 --> 01:08:57,150
so what we need to do now is

1398
01:08:57,150 --> 01:08:59,660
look at all the entries inside this bucket list,

1399
01:09:00,010 --> 01:09:04,460
and we're going to rehash them based on the 2n,

1400
01:09:05,160 --> 01:09:10,210
because we're going to incrementally grow the size of the bucket list by one each time,

1401
01:09:10,650 --> 01:09:12,040
so we had four entries,

1402
01:09:12,330 --> 01:09:14,560
now, after we got to split, now we'll have five,

1403
01:09:15,880 --> 01:09:18,380
so we go through and this points there,

1404
01:09:18,430 --> 01:09:19,335
for every single key,

1405
01:09:19,335 --> 01:09:20,190
we're going to rehash it,

1406
01:09:20,190 --> 01:09:22,940
based on, on, instead of mod n, but mod two n,

1407
01:09:23,410 --> 01:09:25,200
so 8 mod 8 is 0,

1408
01:09:25,200 --> 01:09:26,150
so that stays where it was,

1409
01:09:26,650 --> 01:09:28,370
20 mod 8 is now 4,

1410
01:09:28,570 --> 01:09:32,530
so that's going to get moved down to this new page down here, right,

1411
01:09:33,820 --> 01:09:36,230
and then now the split pointer just moves down by one,

1412
01:09:37,180 --> 01:09:38,490
and we continue doing whatever,

1413
01:09:38,490 --> 01:09:42,070
you know, continue operating on the hash table, right.

1414
01:09:43,940 --> 01:09:45,000
so now I do get 20,

1415
01:09:45,760 --> 01:09:49,470
I first, when I first hash it, I would get 0,

1416
01:09:49,940 --> 01:09:54,750
but then I know that, that location in my, my bucket list here

1417
01:09:54,950 --> 01:09:57,750
is above where the split pointer is pointing at,

1418
01:09:57,860 --> 01:10:00,240
so I know I've already splitted everything up above it,

1419
01:10:00,640 --> 01:10:02,760
so after I mod by 4,

1420
01:10:02,960 --> 01:10:04,390
I got a mod by 8 now,

1421
01:10:04,390 --> 01:10:05,730
to figure out where it really is,

1422
01:10:06,170 --> 01:10:08,310
and then that's how I can find it down here at the bottom.

1423
01:10:10,610 --> 01:10:11,520
Say I want to get 9,

1424
01:10:11,930 --> 01:10:12,805
in this case here,

1425
01:10:12,805 --> 01:10:15,930
it's pointing to exactly where the bucket the split pointer is pointing at,

1426
01:10:16,010 --> 01:10:17,400
so I know I haven't split it yet,

1427
01:10:17,570 --> 01:10:19,170
so I can just only hash it once

1428
01:10:19,400 --> 01:10:21,130
and I scan along the link list

1429
01:10:21,130 --> 01:10:22,380
until I find the thing I'm looking for.

1430
01:10:27,350 --> 01:10:30,030
And at some point, the split pointer will get to the bottom,

1431
01:10:30,650 --> 01:10:32,370
and I'll have, I'll have 8 slots

1432
01:10:32,690 --> 01:10:34,740
and I just loop back around and start all over again.

1433
01:10:37,430 --> 01:10:39,190
So this seems kind of counterintuitive that,

1434
01:10:39,190 --> 01:10:40,980
like I'm not splitting the thing that overflowed,

1435
01:10:41,180 --> 01:10:43,110
I'm splitting to whatever the split pointer points at,

1436
01:10:43,960 --> 01:10:45,380
but the idea is again that,

1437
01:10:45,490 --> 01:10:49,460
if you say this location 1,

1438
01:10:49,660 --> 01:10:50,775
this thing is super hot

1439
01:10:50,775 --> 01:10:52,250
and I keep overflowing, overflowing,

1440
01:10:53,710 --> 01:10:55,640
I'm eventually going to split it, right,

1441
01:10:56,730 --> 01:11:00,760
so eventually, everything gets split out and resize correctly.

1442
01:11:04,490 --> 01:11:04,890
Yes.

1443
01:11:19,220 --> 01:11:20,500
So the question is when,

1444
01:11:20,500 --> 01:11:22,920
when would it actually wrap around, because [] 1,

1445
01:11:23,030 --> 01:11:24,180
so you would get to the pointer like,

1446
01:11:25,230 --> 01:11:27,190
so it'd be 5, 6, 7,

1447
01:11:27,360 --> 01:11:28,630
and then you'll be 7,

1448
01:11:29,100 --> 01:11:32,345
and then you have to you loop back around to 0,

1449
01:11:32,345 --> 01:11:33,100
because you know that,

1450
01:11:34,320 --> 01:11:36,190
from when it's here,

1451
01:11:36,720 --> 01:11:38,200
when it's only from 0 to 3,

1452
01:11:40,580 --> 01:11:42,305
yeah, once you get past 7,

1453
01:11:42,305 --> 01:11:44,060
you know where you starting point,

1454
01:11:44,060 --> 01:11:46,660
that's two n from where you started at,

1455
01:11:46,740 --> 01:11:47,830
so then you loop back around.

1456
01:11:53,450 --> 01:11:54,460
You add a new page,

1457
01:11:54,460 --> 01:11:56,155
but like, I know that I should wrap around,

1458
01:11:56,155 --> 01:11:59,840
when I get, when I go to 8, at position 8,

1459
01:12:00,070 --> 01:12:02,270
because when I started I had 4,

1460
01:12:02,500 --> 01:12:03,915
so 2 times 4 is 8,

1461
01:12:03,915 --> 01:12:05,540
so when I get past 8, I loop back around,

1462
01:12:05,890 --> 01:12:07,370
then you do that until you get 16

1463
01:12:07,450 --> 01:12:08,360
and loop back around.

1464
01:12:15,280 --> 01:12:16,400
Good idea or bad idea?

1465
01:12:22,920 --> 01:12:23,920
It's clever, right,

1466
01:12:24,030 --> 01:12:27,220
again, it's, it's a nice technique to do, do this incrementally,

1467
01:12:28,440 --> 01:12:31,000
but again, there's a lot more bookkeeping, a lot more machinery,

1468
01:12:31,660 --> 01:12:32,790
in order to actually implement those.

1469
01:12:33,690 --> 01:12:34,090
Yes.

1470
01:12:38,610 --> 01:12:39,800
And this question is,

1471
01:12:39,800 --> 01:12:40,850
if you only do when you do look up,

1472
01:12:40,850 --> 01:12:42,280
you only had hash it most twice,

1473
01:12:42,450 --> 01:12:43,450
in this scenario, yes.

1474
01:12:45,480 --> 01:12:47,045
Like if, like, if this thing is massive,

1475
01:12:47,045 --> 01:12:47,920
I could have like,

1476
01:12:48,240 --> 01:12:49,450
yeah, so actually what happens is,

1477
01:12:50,310 --> 01:12:53,650
once I get to say I got to 8 and I wrap back around,

1478
01:12:53,820 --> 01:12:55,330
I can drop the first hash function,

1479
01:12:55,650 --> 01:12:56,780
yeah, so in this case,

1480
01:12:56,780 --> 01:12:57,670
are you going to hash most two.

1481
01:13:02,430 --> 01:13:02,830
Yes.

1482
01:13:19,830 --> 01:13:20,590
His question is,

1483
01:13:22,390 --> 01:13:25,070
and here I'm trying to mod by 8,

1484
01:13:25,900 --> 01:13:28,070
but what if I get 7 and I don't have it,

1485
01:13:28,270 --> 01:13:30,630
but again, you, you wouldn't be able to get 7,

1486
01:13:30,630 --> 01:13:35,160
because, you'd be below the split pointer

1487
01:13:35,160 --> 01:13:39,495
and you'd only hash by 4, not 8, right,

1488
01:13:39,495 --> 01:13:41,550
so, so this demarcation line says,

1489
01:13:41,550 --> 01:13:44,090
I've split everything above and nothing below,

1490
01:13:44,560 --> 01:13:45,380
avoids that problem,

1491
01:13:45,790 --> 01:13:47,070
that you don't land here

1492
01:13:47,070 --> 01:13:48,680
and you really like, you don't,

1493
01:13:49,310 --> 01:13:50,815
you don't hash first and land here,

1494
01:13:50,815 --> 01:13:52,255
but if you hash by 2n,

1495
01:13:52,255 --> 01:13:54,000
you land something here, you haven't split yet,

1496
01:13:54,870 --> 01:13:57,040
the split pointer waterline avoids that problem.

1497
01:14:04,330 --> 01:14:05,000
All right, so,

1498
01:14:06,500 --> 01:14:07,830
splitting buckets based on the split pointer,

1499
01:14:08,270 --> 01:14:10,200
eventually gets you all overflow buckets,

1500
01:14:11,780 --> 01:14:13,940
again, said this, when you reach the bottom,

1501
01:14:13,940 --> 01:14:16,270
you just drop the first hash function and loop back around,

1502
01:14:17,880 --> 01:14:21,490
the, in this technique,

1503
01:14:21,720 --> 01:14:25,840
also allows you to do, contraction or coalescing as well,

1504
01:14:26,280 --> 01:14:30,250
because you could identify that a bucket list is empty,

1505
01:14:31,130 --> 01:14:32,620
and you could do the reverse,

1506
01:14:32,620 --> 01:14:34,410
you could, could could throw it away,

1507
01:14:34,820 --> 01:14:37,510
consolidate the the one bucket empty,

1508
01:14:37,510 --> 01:14:39,000
so like you could just throw it away

1509
01:14:39,110 --> 01:14:40,530
and you move the split pointer back up,

1510
01:14:41,410 --> 01:14:44,860
and at last, you actually shrink the size of the hash table, right.

1511
01:14:44,860 --> 01:14:45,720
So going back here,

1512
01:14:46,010 --> 01:14:46,980
say I delete 20,

1513
01:14:48,680 --> 01:14:50,010
I mod it by 4,

1514
01:14:50,450 --> 01:14:52,495
but then I realize that's below the split pointer

1515
01:14:52,495 --> 01:14:54,240
and I got to get down to the bottom here

1516
01:14:54,560 --> 01:14:55,680
and I go ahead and delete it,

1517
01:14:55,940 --> 01:14:57,150
but now this page is empty,

1518
01:14:58,070 --> 01:14:59,035
so if I wanted to,

1519
01:14:59,035 --> 01:15:00,930
I could just move the pointer back up

1520
01:15:01,310 --> 01:15:02,800
and then drop that last entry

1521
01:15:02,800 --> 01:15:05,380
and drop the last hash table, right.

1522
01:15:05,940 --> 01:15:07,370
And obviously you need to be clever

1523
01:15:07,370 --> 01:15:09,680
and make sure that like I don't oscillate,

1524
01:15:09,680 --> 01:15:11,315
like insert 20, delete 20, insert 20,

1525
01:15:11,315 --> 01:15:13,030
I keep splitting it and coalescing,

1526
01:15:13,110 --> 01:15:13,870
that would be bad.

1527
01:15:15,540 --> 01:15:19,520
But you could contract the data structure based on this, right.

1528
01:15:19,780 --> 01:15:21,510
You don't want to insert 21, then overflow

1529
01:15:21,510 --> 01:15:22,760
and split all over again.

1530
01:15:26,220 --> 01:15:30,250
I don't think Postgres supports shrinking the size of the hash table,

1531
01:15:31,470 --> 01:15:32,075
as far as I know,

1532
01:15:32,075 --> 01:15:33,340
without rebuilding the whole thing.

1533
01:15:38,580 --> 01:15:41,260
All right, so hash tables, again, super useful,

1534
01:15:41,820 --> 01:15:44,260
most systems are going to just implement the linear probe hashing,

1535
01:15:44,370 --> 01:15:45,785
but again you can still specialize it

1536
01:15:45,785 --> 01:15:47,435
based on the data type and other aspects

1537
01:15:47,435 --> 01:15:48,790
of how it's going to be used,

1538
01:15:49,140 --> 01:15:51,580
and ClickHouse is probably the best example of this,

1539
01:15:52,830 --> 01:15:53,900
for a lot of the commercial systems,

1540
01:15:53,900 --> 01:15:56,170
it's very hard to know what hash table they're actually using,

1541
01:15:56,880 --> 01:15:58,600
unless there's a paper talking about it

1542
01:15:58,770 --> 01:16:00,640
or we know people that work there that can tell us,

1543
01:16:01,800 --> 01:16:03,800
you know, this is not something you as,

1544
01:16:03,800 --> 01:16:06,430
you know someone using SQL, an application developer,

1545
01:16:06,690 --> 01:16:07,690
you should know or care,

1546
01:16:08,670 --> 01:16:11,440
but it's nice to know what sometimes how these [assessments] are implemented.

1547
01:16:14,030 --> 01:16:15,720
Nice thing about hash functions again, it'll be fast,

1548
01:16:15,980 --> 01:16:17,790
they'll support O(1) lookup in the best case scenario,

1549
01:16:18,260 --> 01:16:20,850
but again, we need to be able to make sure that we can,

1550
01:16:22,330 --> 01:16:24,930
you know, we, we may need to grow efficiently,

1551
01:16:24,930 --> 01:16:27,680
if we estimate the size incorrectly,

1552
01:16:27,760 --> 01:16:30,290
and we'll see how we do those estimations later on.

1553
01:16:31,800 --> 01:16:35,650
So some systems will give you hash table when you call create index,

1554
01:16:36,150 --> 01:16:37,235
Postgres will let you do this,

1555
01:16:37,235 --> 01:16:38,390
Postgres be called create index,

1556
01:16:38,390 --> 01:16:39,520
you can say using hash

1557
01:16:39,630 --> 01:16:40,610
and you'll get a hash table,

1558
01:16:40,610 --> 01:16:44,500
You'll get their their linear hash table implemention, right,

1559
01:16:45,120 --> 01:16:46,160
but this is not the default,

1560
01:16:46,910 --> 01:16:48,460
for, for almost all systems,

1561
01:16:48,460 --> 01:16:49,860
when you call create index,

1562
01:16:51,160 --> 01:16:51,980
is there know why?

1563
01:16:55,540 --> 01:16:56,900
No range cans, yes,

1564
01:16:56,980 --> 01:16:59,960
the only thing you can do with the hash table is the quality lookups

1565
01:17:00,160 --> 01:17:02,980
and you need to have the entire key, right,

1566
01:17:03,000 --> 01:17:07,145
if my key is on column A and column B,

1567
01:17:07,145 --> 01:17:08,290
I can do composite keys,

1568
01:17:08,610 --> 01:17:10,820
if I don't have A, I don't have B,

1569
01:17:10,820 --> 01:17:11,830
I can't do a lookup.

1570
01:17:13,130 --> 01:17:14,110
In a B+ tree,

1571
01:17:14,110 --> 01:17:15,510
which we'll discuss next class,

1572
01:17:16,610 --> 01:17:18,240
you can do these prefix lookups

1573
01:17:18,320 --> 01:17:21,630
and it is the best data structure of all time, for databases,

1574
01:17:22,490 --> 01:17:23,790
Tries are actually pretty good too,

1575
01:17:24,740 --> 01:17:28,290
but you can put Tries of B+ trees,

1576
01:17:30,800 --> 01:17:33,990
the default choice those systems are going be a B+ tree,

1577
01:17:34,500 --> 01:17:36,480
and that's what we'll discuss next week,

1578
01:17:36,480 --> 01:17:39,730
but again, we'll assume it's single threaded, on Monday

1579
01:17:39,810 --> 01:17:42,550
and then on Wednesday, we'll see how to make it multi threaded.

1580
01:17:44,330 --> 01:17:45,240
All right, hit it.

